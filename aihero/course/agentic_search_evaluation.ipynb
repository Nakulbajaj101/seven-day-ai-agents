{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf24cb3-7c14-4482-810a-d7d9dad24b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index, VectorSearch\n",
    "from openai import OpenAI, APIConnectionError, RateLimitError, APIStatusError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "import pickle\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e998bea4-398a-4c0d-8625-42684d4de6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for qa purpose in english\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22f2a6-7a74-4e1a-824a-8a206c86a09f",
   "metadata": {},
   "source": [
    "# Lets implement hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b27bf38-1e1c-4161-8e10-3b2af8c8a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully unpickled from vector_search_data.pkl\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"vector_search_data.pkl\"\n",
    "loaded_data = None\n",
    "try:\n",
    "    with open(FILE_PATH, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        print(f\"Data successfully unpickled from {FILE_PATH}\")\n",
    "\n",
    "    # Access the loaded data\n",
    "    loaded_embeddings = loaded_data['embeddings']\n",
    "    loaded_docs = loaded_data['documents']\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {FILE_PATH} was not found.\")\n",
    "except pickle.UnpicklingError as e:\n",
    "    print(f\"An error occurred during unpickling: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2473f13e-791b-4625-bd72-10b8743725aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch, Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844c0d96-c42a-49d6-8cd7-88de471a5778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x319636c60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(text_fields=['title', 'description', 'filename', 'section'],\n",
    "              keyword_fields=[])\n",
    "index.fit(loaded_docs)\n",
    "\n",
    "vs = VectorSearch()\n",
    "vs.fit(loaded_embeddings, loaded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3bc24a-9b20-46f8-b361-9bec0df68501",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I evaluate classification model results, and ensure numerical data is not drifted?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637c60f3-d165-404d-be99-7ef7de753d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import List, Any, Callable\n",
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e7b85aa-2b1e-4416-bf3c-e9e649306987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query: str) -> List[Any]:\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query: str) -> List[Any]:\n",
    "    q = embedding_model.encode(query)\n",
    "    return vs.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        text_to_hash = result['filename'] + ' ' + result['section'][0:250]\n",
    "        encoded_string = text_to_hash.encode('utf-8')\n",
    "        hash_object = hashlib.sha256(encoded_string)\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4612f98e-fa04-4dba-b740-d6e76036a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant to read and understand technical docs for evidently ai. \n",
    "\n",
    "Use the search tool to find relevant information from the data before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c17a472-0302-4fa6-8ac1-3c11ebcc65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b3871d-82a7-4db0-9756-389cb5759e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate classification model results and ensure numerical data is not drifted, you can follow these comprehensive steps outlined in the Evidently AI documentation:\n",
      "\n",
      "### Evaluating Classification Model Results\n",
      "1. **Use the Classification Quality Preset**: Utilize the `ClassificationPreset` to evaluate and visualize the performance of your classification tasks. This can be done for a single dataset or by comparing it against a reference dataset.\n",
      "   \n",
      "2. **Metrics and Visualizations**:\n",
      "   - **Metrics**: You can measure various metrics including Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss.\n",
      "   - **Visualizations**: Generate visual aids like Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, and PR Curve.\n",
      "   - If you include feature columns in the report, it will also show performance by column to assess different data segments.\n",
      "\n",
      "3. **Testing and Diagnostics**:\n",
      "   - Enable tests to automatically run checks that assess if model performance metrics are within acceptable bounds.\n",
      "   - Test conditions can be derived from a reference dataset, or heuristically using a dummy classification model as a baseline for comparison.\n",
      "   - Utilize reports generated to explore model errors and underperforming segments when there's a drop in performance.\n",
      "\n",
      "4. **Customization**: You can customize the report by modifying test conditions or adding additional metrics like correlation, missing values, or data drift (for concept drift evaluation).\n",
      "\n",
      "### Ensuring Numerical Data is Not Drifted\n",
      "1. **Data Drift Detection**: You should utilize methods for detecting distribution drift in numerical columns. Evidently provides different algorithms based on the size of the dataset:\n",
      "   - For smaller datasets (<= 1000 observations), use tests like the Kolmogorov-Smirnov test for numerical columns with many unique values, and Chi-squared tests for categorical columns or numerical columns with limited unique values.\n",
      "   - For larger datasets, methods such as Wasserstein Distance and Jensen-Shannon divergence can be applied.\n",
      "\n",
      "2. **Configuration of Drift Detection**: \n",
      "   - The default methods and thresholds can be adjusted, allowing for a custom statistic test for drift detection.\n",
      "   - Metrics like `ValueDrift`, `DriftedColumnsCount`, and `DataDriftPreset` can be employed to track drift over specific individual columns or overall datasets.\n",
      "\n",
      "3. **Monitoring**: Regularly run these drift detection evaluations whenever new data is obtained in production. Continuous monitoring helps in promptly addressing potential drifts.\n",
      "\n",
      "Following these guidelines will help ensure that your classification models are evaluated effectively and that any numerical data drift is detected and managed appropriately.\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(user_prompt=query)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cfd68ae-f93c-49a3-88a7-2ffa5bf8f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1d24fb-240a-4fb2-b9d4-c47cc9e03619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_entry(agent, messages, source: str=\"user\"):\n",
    "    tools = []\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "    \n",
    "    messages = json.loads(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9b7fe7f-0ad5-4f64-a9f4-bbb644577223",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = log_entry(\n",
    "    agent=agent,\n",
    "    messages=result.new_messages_json()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fc55e51-d759-4b84-ab14-6f29811301c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-12-28T02:00:17.110796Z'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry[\"messages\"][-1]['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54452246-c2fc-4870-96fd-eeb7bd5a4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6941dde1-9913-45cd-8140-99da58a3c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f867c6-bff5-4c2d-b6c2-7280f3c85685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdbbada1-ca01-4126-a5dc-efacf3e47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_interaction(agent, messages, source: str=\"user\"):\n",
    "\n",
    "    entry = log_entry(\n",
    "        agent=agent,\n",
    "        messages=messages,\n",
    "        source=source\n",
    "    )\n",
    "\n",
    "    ts = entry[\"messages\"][-1]['timestamp']\n",
    "    ts_obj = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    ts_str = ts_obj.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(nbytes=3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with open(filepath, mode='w', encoding='utf-8') as f_out:\n",
    "        json.dump(obj=entry, fp=f_out, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ee1148a-befd-4008-a5d4-4704f4cbe027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20251228_020017_a78065.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create the first log file\n",
    "log_interaction(agent=agent, messages=result.new_messages_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "642cd2c4-1231-4d14-8e4b-d174b55b0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets ask another question\n",
    "user_question = \"What evaluation metrics are relevant for image data?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27956d7f-a85a-4482-80a6-0d662e89f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When evaluating image data, particularly in the context of classification tasks, several key evaluation metrics are commonly used to assess the performance of models. Here are the relevant metrics:\n",
      "\n",
      "1. **Accuracy**: Measures the overall correctness of the model across all classes.\n",
      "   \n",
      "2. **Precision**: Indicates the proportion of true positive results in relation to the total predicted positives. It helps understand how many of the predicted classes were actually correct.\n",
      "\n",
      "3. **Recall (Sensitivity)**: Measures the proportion of true positives identified by the model compared to the actual positives. It shows how well the model can identify actual instances of a class.\n",
      "\n",
      "4. **F1 Score**: The harmonic mean of precision and recall, providing a single metric that balances both concerns, which is particularly useful in scenarios where class distribution is uneven.\n",
      "\n",
      "5. **ROC Curve**: The Receiver Operating Characteristic curve visualizes the true positive rate against the false positive rate at various threshold settings, aiding in understanding model performance across different discrimination thresholds.\n",
      "\n",
      "6. **AUC (Area Under the Curve)**: The area under the ROC curve which summarizes the performance of the model across all thresholds. A value closer to 1 indicates better model performance.\n",
      "\n",
      "7. **Log Loss**: Measures the uncertainty of the model's predictions, penalizing false classifications, and is used to evaluate probabilistic outputs.\n",
      "\n",
      "8. **Confusion Matrix**: A visual representation that allows you to see the true positive, false positive, true negative, and false negative counts for each class, helping to diagnose specific areas where the model performs poorly.\n",
      "\n",
      "9. **Precision-Recall Curve**: Similar to the ROC curve, this visualization helps in understanding the trade-off between precision and recall for various thresholds. \n",
      "\n",
      "10. **Quality Metrics by Class**: These metrics show individual class performance, which is especially important in multi-class problems where some classes may be harder to classify than others.\n",
      "\n",
      "These metrics provide a comprehensive view of an image classification model's performance and can help guide improvements and adjustments.\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(user_prompt=user_question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60ada7ae-0731-4979-9217-943cc807752d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20251228_025937_ee3694.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_interaction(agent=agent, messages=result.new_messages_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb0c33-fe22-4bd3-809f-b8823f1fb2d0",
   "metadata": {},
   "source": [
    "# Lets add references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93099e27-2396-44fd-9bc9-09c62a17217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the document materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e444fb2a-6757-4f77-8084-363202b2187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb30d0fc-e254-4d72-a830-47ecc8aaa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets ask another question\n",
    "user_question = \"What evaluation metrics are relevant for image data?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "751ce5e3-5203-4a93-91e4-5feba28a0b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For evaluating image data, particularly in the context of classification tasks, several metrics are commonly used to measure the performance of models. Here are some of the most relevant evaluation metrics for image classification:\n",
      "\n",
      "1. **Accuracy**: The ratio of correctly predicted instances to the total instances. It provides a basic measure of performance.\n",
      "\n",
      "2. **Precision**: Indicates the accuracy of the positive predictions. It is defined as the number of true positives divided by the sum of true positives and false positives.\n",
      "\n",
      "3. **Recall (Sensitivity)**: Measures the ability to find all relevant instances, defined as the number of true positives divided by the sum of true positives and false negatives.\n",
      "\n",
      "4. **F1 Score**: The harmonic mean of precision and recall, useful for dealing with class imbalances.\n",
      "\n",
      "5. **ROC AUC (Receiver Operating Characteristic - Area Under Curve)**: This metric evaluates the trade-off between the true positive rate and false positive rate across different thresholds.\n",
      "\n",
      "6. **Confusion Matrix**: A table that summarizes the performance of a classification algorithm by showing the correct and incorrect predictions across the different classes.\n",
      "\n",
      "7. **Log Loss**: Evaluates the probability outputs of the model by measuring the uncertainty of the predictions.\n",
      "\n",
      "8. **Precision-Recall Curve**: Displays the trade-off between precision and recall at different thresholds, providing a more insightful visualization than the ROC curve, particularly for imbalanced datasets.\n",
      "\n",
      "9. **Class Representation**: Additional visualization to understand how well the model performs across different classes.\n",
      "\n",
      "These metrics allow for a comprehensive evaluation of the classification performance on image data and can help in optimizing and tuning machine learning models for better accuracy and reliability.\n",
      "\n",
      "You can find more detailed information about these metrics in the following document: [Classification Metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(user_prompt=user_question)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ee6f36f-41e3-4914-bd8f-3553e2b66c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_v2_20251228_030704_0134b1.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_interaction(agent=agent, messages=result.new_messages_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ecb67-9a71-4276-9833-40ecfcf92615",
   "metadata": {},
   "source": [
    "# Building the evaluation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37910fa5-9b66-4f98-b1b1-14d41c1f843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need the evaluation system prompt to build an eval agent with pydantic\n",
    "# We will need the answer provided by agent, question of the user, log for further analysis\n",
    "system_eval_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- factual: The response was not hallucinated and covers actual facts\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6823c129-8da4-4e9e-be13-3825fb27fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing the structure for output\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checks: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb0de8d4-e9d2-4d76-a78e-7a84a9a07eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "273dc20d-8900-44d7-a39a-c93e4bdad674",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_agent = Agent(\n",
    "    model=\"gpt-5-nano\",#Using a different model to avoid bias and ensure better results\n",
    "    name=\"eval_agent\",\n",
    "    instructions=system_eval_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "53d63a5c-304f-426f-89c7-3956e5848ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "60215fa9-bb47-41dd-bf63-031cedd322d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill the user prompt for evaluation, we need a function to fetch the log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6dffee0e-1c86-4bb0-a54b-a70f7b672525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_data(log_file: str):\n",
    "    try:\n",
    "        with open(log_file, 'r') as f_in:\n",
    "            log_data = json.load(f_in)\n",
    "            log_data['log_file'] = log_file\n",
    "            return log_data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{log_file} is not found in the desired location\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "65c14dab-c540-4ab3-9f98-06c33428d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record_file = \"logs/faq_agent_v2_20251228_030704_0134b1.json\"\n",
    "log_data = load_log_data(log_file=log_record_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8007f69a-beb8-4563-b7c6-dc456a6fd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = log_data[\"system_prompt\"][0]\n",
    "question = log_data['messages'][0]['parts'][0]['content']\n",
    "answer = log_data['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_data['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "442e4045-a3d1-4ec2-991a-bc1d233430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e565226-8504-4f19-8c71-41a178bbdc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will send the user formatted prompt to the eval agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b03b6773-c6a1-486c-a28c-05235761c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await evaluation_agent.run(user_prompt=user_prompt, output_type=EvaluationChecklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "69a00d39-f958-4ace-af94-c2533fd08bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = result.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e432816-88ed-47ce-9840-ec28dc083120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provides a comprehensive, accurate list of image classification evaluation metrics with a sourced reference. It demonstrates alignment with the instruction to use search results and cite sources.\n"
     ]
    }
   ],
   "source": [
    "print(checklist.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "857280b5-6425-47e0-b72e-31b161c19c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationChecklist(checks=[EvaluationCheck(check_name='instructions_follow', justification='The answer used information from search results and provided a source citation to a Classification Metrics document, aligning with the instruction to search and cite sources.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='No prohibited content or actions; the answer stays within educational guidance for evaluation metrics.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification='The response directly lists common image classification metrics (accuracy, precision, recall, F1, ROC AUC, confusion matrix, log loss, PR curve, class representation).', check_pass=True), EvaluationCheck(check_name='answer_clear', justification='Metrics are listed clearly with brief explanations for some of them, making the answer easy to follow.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='Includes a citation link to the source: [Classification Metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).', check_pass=True), EvaluationCheck(check_name='completeness', justification='Covers the core metrics for image classification evaluation; includes both threshold-based metrics and visualization tools.', check_pass=True), EvaluationCheck(check_name='factual', justification='Facts align with standard ML evaluation metrics widely used for image classification tasks.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The provided log shows the use of a search tool to retrieve information about evaluation metrics, indicating tool usage.', check_pass=True)], summary='The answer provides a comprehensive, accurate list of image classification evaluation metrics with a sourced reference. It demonstrates alignment with the instruction to use search results and cite sources.')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5a235eb0-8fcb-455b-970f-db4b38b68b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_name='instructions_follow' justification='The answer used information from search results and provided a source citation to a Classification Metrics document, aligning with the instruction to search and cite sources.' check_pass=True\n",
      "\n",
      "check_name='instructions_avoid' justification='No prohibited content or actions; the answer stays within educational guidance for evaluation metrics.' check_pass=True\n",
      "\n",
      "check_name='answer_relevant' justification='The response directly lists common image classification metrics (accuracy, precision, recall, F1, ROC AUC, confusion matrix, log loss, PR curve, class representation).' check_pass=True\n",
      "\n",
      "check_name='answer_clear' justification='Metrics are listed clearly with brief explanations for some of them, making the answer easy to follow.' check_pass=True\n",
      "\n",
      "check_name='answer_citations' justification='Includes a citation link to the source: [Classification Metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).' check_pass=True\n",
      "\n",
      "check_name='completeness' justification='Covers the core metrics for image classification evaluation; includes both threshold-based metrics and visualization tools.' check_pass=True\n",
      "\n",
      "check_name='factual' justification='Facts align with standard ML evaluation metrics widely used for image classification tasks.' check_pass=True\n",
      "\n",
      "check_name='tool_call_search' justification='The provided log shows the use of a search tool to retrieve information about evaluation metrics, indicating tool usage.' check_pass=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for check in checklist.checks:\n",
    "    print(check)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a4c9b456-6be5-4f42-be60-0fc64e3b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reduce the tokens by simplifying the logs, and send less unnecessary info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d7779ed3-faee-460d-b379-b1598b541fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "900dcdf2-b645-40d7-a1ba-185cbe18aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_data[\"system_prompt\"][0]\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log)\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "daf28f85-c321-4359-b32b-5a757b15bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record_file = \"logs/faq_agent_20251228_025937_ee3694.json\"\n",
    "log_record = load_log_data(log_file=log_record_file)\n",
    "eval1 =  await evaluate_log_record(eval_agent=evaluation_agent,\n",
    "                    log_record=log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d0c4bc80-2475-426b-be96-4e23f761e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provides a solid list of common image classification metrics and is clear and factual, but it lacks citations and misses some important metrics (e.g., top-5 accuracy, per-class metrics for multi-class, and mAP in detection). The user’s instruction to cite source materials was not fulfilled.\n",
      "\n",
      "check_name='instructions_follow' justification='User asked to use the search tool and cite sources in the answer; the provided answer did not include citations from source files.' check_pass=False\n",
      "\n",
      "check_name='instructions_avoid' justification='No disallowed content; the response adheres to allowed content.' check_pass=True\n",
      "\n",
      "check_name='answer_relevant' justification='The answer lists common evaluation metrics relevant to image data and classification tasks.' check_pass=True\n",
      "\n",
      "check_name='answer_clear' justification=\"The list is organized with bullet points and explanations for each metric; it's clear.\" check_pass=True\n",
      "\n",
      "check_name='answer_citations' justification='No citations were provided in the answer to accompany the metrics; thus not satisfied.' check_pass=False\n",
      "\n",
      "check_name='completeness' justification='Covers many common metrics but misses some important ones for image tasks (e.g., top-5 accuracy, per-class metrics, mAP for multi-label or object detection); thus incomplete.' check_pass=False\n",
      "\n",
      "check_name='factual' justification='All listed metrics are standard and factual for image classification tasks.' check_pass=True\n",
      "\n",
      "check_name='tool_call_search' justification='The log shows a search tool was invoked in the prior interaction (hybrid_search); thus the search action occurred.' check_pass=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval1.summary)\n",
    "print()\n",
    "for check in eval1.checks:\n",
    "    print(check)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3fa28b08-8054-47ec-b88c-51988586313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are question generation assistant agent, whose goal is to generate questions associated \n",
    "with MLOPs and LLMOPs part of evaluation and monitoring, while leveraging evidently ai. \n",
    "These questions should be strictly based on how to and why evaluate\n",
    "and monitor machine learning models for text, image, and structure data, or how to and why evaluate llms in production, \n",
    "so students and practioners of evidently ai, will be leveraging these questions to build monitoring and evaluation pipelines.\n",
    "Feel free to use common questions about evidently ai asked in stack overflow, reddit or github.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "- There should be no number preceding the question\n",
    "\n",
    "Maximum number of questions should not be more than 20.\n",
    "There will be some sample docs that I will be providing with <Example></Example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "491da021-01a6-4953-afa5-1b99aa2f60ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully unpickled from vector_search_data.pkl\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"vector_search_data.pkl\"\n",
    "loaded_data = None\n",
    "try:\n",
    "    with open(FILE_PATH, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        print(f\"Data successfully unpickled from {FILE_PATH}\")\n",
    "\n",
    "    # Access the loaded data\n",
    "    loaded_docs = loaded_data['documents']\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {FILE_PATH} was not found.\")\n",
    "except pickle.UnpicklingError as e:\n",
    "    print(f\"An error occurred during unpickling: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "00ded78c-5033-45f3-b70d-496d87de6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "sample = random.sample(loaded_docs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2b6819a0-fce3-42d6-a4bb-0352e5da4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [f\"<Example>{data}</Example>\" for data in sample]\n",
    "user_prompt = \"\"\"Based on the data below can you generate maximum of 20 questions\n",
    "Makes sure list doesnt have any numbering.\n",
    "<EXAMPLES>{examples}</EXAMPLES>\n",
    "\"\"\".format(examples=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4a62aa7b-3b73-4788-83ed-db70cb39f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2171eca7-783e-4205-84d0-7c6126e1e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await question_generator.run(user_prompt, output_type=QuestionsList)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b80ed5c-bcac-4bcc-a3cc-19a9b7cc6390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is it essential to evaluate machine learning models in production environments?',\n",
       " 'What challenges can arise during the monitoring of large language models (LLMs)?',\n",
       " 'How can data drift affect the performance of machine learning models?',\n",
       " 'What are the best practices for setting up an evaluation pipeline for text data models?',\n",
       " 'In what ways can evaluation metrics differ between image and structured data models?',\n",
       " 'How does synthetic data contribute to the evaluation of machine learning systems?',\n",
       " 'What factors should be considered when determining the evaluation metrics for a specific machine learning model?',\n",
       " 'How can the performance of an LLM be continuously monitored over time?',\n",
       " 'What is the role of confusion matrices in evaluating machine learning models?',\n",
       " 'How can visualizations enhance the understanding of model performance during evaluations?',\n",
       " 'What specific metrics can be used to evaluate the accuracy of image classification models?',\n",
       " 'Why is regression testing necessary when monitoring machine learning models in production?',\n",
       " 'How does the concept of precision-recall differ in the context of model evaluation?',\n",
       " \"What steps should be taken to establish a custom metric for evaluating a specific model's performance?\",\n",
       " 'How can you effectively use Evidently AI to evaluate and monitor LLMs in real-time?',\n",
       " 'What are the implications of using biased training datasets on the evaluation of machine learning models?',\n",
       " 'How can continuous integration/continuous deployment (CI/CD) practices affect model evaluation processes?',\n",
       " 'Why is feature importance analysis a critical part of model monitoring?',\n",
       " 'How can benchmarking against previous model versions assist in evaluation and monitoring?',\n",
       " 'What role does user feedback play in the ongoing evaluation of machine learning models?']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d4959d9d-b06f-44df-aa07-0600d0aac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will iterate over each question and generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6dce2614-d7b3-481d-b1bf-2c07671f3871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7892c9fbf3554fefa406902480801e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is it essential to evaluate machine learning models in production environments?\n",
      "Evaluating machine learning models in production environments is essential for several reasons:\n",
      "\n",
      "1. **Performance Monitoring**: Continuous monitoring of model performance is crucial to ensure that models maintain their accuracy and effectiveness over time. Models may degrade due to changes in input data patterns, a phenomenon known as data drift. Regular evaluations help identify these issues early [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "2. **Quality Assurance**: Evaluation processes help in ensuring the quality of predictions. This includes checking for accuracy in classification or regression outputs and assessing the quality of input data, such as the presence of missing values or out-of-range features [source](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_ml.mdx).\n",
      "\n",
      "3. **Regression Testing**: When models are updated or modified, it is vital to conduct regression testing to verify that the model's performance has not degraded due to these changes. This testing helps ensure stability and reliability in environments where models frequently undergo iterations [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "4. **Safety and Adversarial Testing**: Evaluations also encompass testing how models respond to edge cases and adversarial inputs. This aspect is critical to ensuring that the model behaves as expected in challenging or unforeseen scenarios [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "5. **Decision-Making for Retraining**: Evaluating model performance can guide decisions on whether retraining is necessary. If evaluations indicate data drift, it might indicate that the model needs to be updated with new data to maintain accuracy [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n",
      "\n",
      "In conclusion, regular evaluation of machine learning models in production is vital for maintaining their performance, ensuring quality, and making informed decisions regarding model updates and retraining strategies.\n",
      "\n",
      "What challenges can arise during the monitoring of large language models (LLMs)?\n",
      "Monitoring large language models (LLMs) can present several challenges, including:\n",
      "\n",
      "1. **Dynamic Outputs**: LLMs often generate outputs that can vary significantly with slight changes in input or model parameters. This variability can complicate evaluation and monitoring efforts, especially in ensuring that the model maintains consistency over time.\n",
      "\n",
      "2. **Evaluation Complexity**: Different evaluation methods such as reference-based (e.g., exact match, semantic similarity) and reference-free evaluations (e.g., analyzing text statistics, using regex) add complexity. Selecting the appropriate metrics to capture the model’s performance accurately can be challenging, as the best method may depend on the specific use case and objectives.\n",
      "\n",
      "3. **Regulatory and Ethical Concerns**: Monitoring LLMs for bias and ensuring fairness in their outputs is crucial. These models may unintentionally propagate biases present in the training data, necessitating systematic checks and balances over their output.\n",
      "\n",
      "4. **System Updates**: Any updates or changes to the model can lead to unexpected changes in output behavior. Regression testing is often necessary to identify significant changes after an update, which can require substantial resources and expertise.\n",
      "\n",
      "5. **Long-Term Tracking**: Implementing a consistent evaluation pipeline to track model performance over time can be complicated. It requires scheduling regular evaluations and possibly integrating these efforts with larger machine learning workflows.\n",
      "\n",
      "6. **Data Privacy**: When monitoring outputs, especially in production environments, ensuring user privacy while evaluating model performance poses a significant challenge. Aggregated data approaches may help, but they also require careful planning and implementation.\n",
      "\n",
      "For a successful monitoring strategy, it’s important to establish clear evaluation goals, choose appropriate metrics, and set up robust evaluation pipelines to assess model performance consistently. These efforts can also benefit from community-supported frameworks and tools that streamline the process [LLM Evaluation Methods](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_evals.mdx).\n",
      "\n",
      "How can data drift affect the performance of machine learning models?\n",
      "Data drift occurs when the statistical properties of the input data used by a machine learning model change over time, which can significantly impact the model's performance. Here’s how data drift can affect machine learning models:\n",
      "\n",
      "1. **Model Performance Degradation**: When the input features that the model was trained on change, the model might no longer make accurate predictions. This can lead to a drop in the accuracy of the model since it is no longer operating in a familiar environment.\n",
      "\n",
      "2. **Feature and Prediction Drift**: Data drift can manifest as shifts in features (feature drift) or in the model's predictions (prediction drift). Monitoring these shifts is essential; otherwise, performance could deteriorate unnoticed [Data Drift](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n",
      "\n",
      "3. **Debugging Model Quality**: If a model's performance drops, analyzing data drift helps identify which features have changed and how they may be impacting model predictions. This analysis is critical for debugging and understanding the underlying causes of model performance decay [Data Drift](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n",
      "\n",
      "4. **Need for Retraining**: Detecting data drift may indicate that the model needs to be retrained. If substantial drift is observed, it suggests that the model's assumptions no longer hold, indicating that retraining could help adapt the model to the new data distribution [Data Drift](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n",
      "\n",
      "5. **Monitoring Systems**: Establishing a robust monitoring system to regularly assess data drift is essential. This can involve setting thresholds for acceptable drift levels and determining when retraining should occur based on these metrics [Data Drift](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n",
      "\n",
      "By actively monitoring and addressing data drift, you can help maintain the relevance and accuracy of your machine learning models over time.\n",
      "\n",
      "What are the best practices for setting up an evaluation pipeline for text data models?\n",
      "Setting up an evaluation pipeline for text data models involves several best practices to ensure effective assessments and improvements. Here are some key strategies:\n",
      "\n",
      "1. **Define the Evaluation Objectives**: Clearly specify what aspects of your model you want to evaluate, such as accuracy, consistency, and robustness. This includes identifying the evaluation criteria relevant to your specific use case.\n",
      "\n",
      "2. **Create Diverse Test Datasets**: Utilize a mix of real, synthetic, and historical datasets to ensure comprehensive coverage of potential inputs. Synthetic datasets are particularly useful for generating edge cases and addressing any situational deficits in real data. They can be created using templates or algorithms to simulate various scenarios that the model may encounter in practice ([docs-main/synthetic-data/why_synthetic.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/synthetic-data/why_synthetic.mdx)).\n",
      "\n",
      "3. **Utilize Evaluation Metrics**: Implement various metrics tailored to text evaluation. These can include machine learning model-based metrics like accuracy and precision, as well as natural language processing (NLP)-specific measures such as BLEU scores, ROUGE, or custom scoring methods based on semantic similarity ([docs-main/metrics/customize_hf_descriptor.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_hf_descriptor.mdx)). \n",
      "\n",
      "4. **Incorporate Human Feedback**: In addition to automated evaluation, consider integrating human judgments to provide insights into model performance, particularly in cases where subjective criteria are important.\n",
      "\n",
      "5. **Debugging and Continuous Improvement**: Post-evaluation, analyze the results to identify failures or weaknesses in the model. Use insights from the evaluation to iteratively improve the model by refining data inputs, training techniques, or feature engineering ([docs-main/docs/platform/datasets_overview.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/datasets_overview.mdx)).\n",
      "\n",
      "6. **Implement Evaluation Pipelines**: Automated evaluation pipelines streamline the process of executing and reporting evaluations. Ensure that the pipeline can collect logs, run checks, and generate reports efficiently, possibly using frameworks that allow for CI/CD integrations.\n",
      "\n",
      "7. **Visualize Results**: Utilize visualization tools to explore the evaluation results, which help in understanding distributions and identifying trends in performance metrics ([docs-main/metrics/preset_text_evals.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_text_evals.mdx)). \n",
      "\n",
      "8. **Integration of LLMs**: Consider using Language Models (LLMs) as evaluators in your pipeline to provide nuanced insights into text performance, which can facilitate richer evaluations through customized prompts and criteria ([docs-main/metrics/customize_llm_judge.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_llm_judge.mdx)).\n",
      "\n",
      "By following these best practices, you can establish a robust evaluation pipeline that not only assesses model performance effectively but also equips you with insights for continuous refinement and enhancement of your text data models.\n",
      "\n",
      "In what ways can evaluation metrics differ between image and structured data models?\n",
      "Evaluation metrics can differ significantly between models processing image data and those handling structured data due to the inherent characteristics and requirements of the two types of data. Here are some key ways in which these metrics can differ:\n",
      "\n",
      "1. **Nature of Data**:\n",
      "   - **Image Data**: Metrics often focus on aspects pertinent to image characteristics such as accuracy, precision, recall, F1 score, Intersection over Union (IoU) for segmentation tasks, and Mean Average Precision (mAP) for object detection tasks. These metrics evaluate how well a model performs concerning visual features and spatial arrangements.\n",
      "   - **Structured Data**: Metrics for structured data might include accuracy, ROC AUC (Area Under the Receiver Operating Characteristic Curve), precision, recall, F1 score, and regression metrics like RMSE (Root Mean Square Error) or MAE (Mean Absolute Error). These evaluations are often based on numerical values or categorical classes and look more at the correctness of data representation and classification.\n",
      "\n",
      "2. **Complexity of Interpretation**:\n",
      "   - **Image Data**: Evaluating image models involves subjective assessments of visual outputs in many cases. For example, visual quality evaluations could incorporate human judgments about whether the generated images meet certain aesthetic criteria.\n",
      "   - **Structured Data**: Evaluations are more straightforward as they typically involve comparing predicted vs. actual values. The interpretation of output is often quantitative.\n",
      "\n",
      "3. **Error Analysis**:\n",
      "   - **Image Models**: Error analysis in image classification can entail examining specific images to understand misclassifications, often requiring visual inspection to determine why a model may have failed.\n",
      "   - **Structured Models**: Error analysis tends to be more statistical, focusing on classification reports, confusion matrices, and metrics that quantify the model's errors, making the faults easier to quantify and address.\n",
      "\n",
      "4. **Use of Synthetic Data**:\n",
      "   - While both domains can utilize synthetic data for testing and training, the methods and implications may vary. In image data evaluation, synthetic data generation can help create various scenarios for the model to experience diverse conditions (e.g., different lighting, orientations). For structured data, synthetic datasets might focus on generating edge cases or augmenting datasets to explore decision boundaries without direct visual interpretation.\n",
      "\n",
      "5. **Temporal Considerations**:\n",
      "   - Models working with structured data may often involve temporal sequences (like time series data), requiring evaluation metrics that account for the time aspect (e.g., forecasting accuracy, seasonality adjustments). In contrast, image data typically doesn't involve time-related metrics unless evaluating frames in a video.\n",
      "\n",
      "For a detailed examination of these metrics across various ML applications, especially relating to their specific characteristics, you can refer to the [Evidently documentation](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/introduction.mdx).\n",
      "\n",
      "How does synthetic data contribute to the evaluation of machine learning systems?\n",
      "Synthetic data plays a crucial role in the evaluation of machine learning systems by providing several benefits:\n",
      "\n",
      "1. **Scaling Test Data**: It allows researchers and developers to quickly generate large volumes of structured test cases. This is particularly beneficial when starting from scratch without access to real data, or when there is a need to scale a manually designed dataset with more variation.\n",
      "\n",
      "2. **Testing Edge Cases and Robustness**: Synthetic data can be created to specifically test edge cases, adversarial inputs, or other scenarios that may be difficult to capture with real-world data. This helps in assessing the robustness of AI systems under diverse conditions.\n",
      "\n",
      "3. **Controlled Variations**: By adding controlled variations in datasets, synthetic data allows for the evaluation of specific weaknesses or behaviors of machine learning models. This focus on specific attributes enables a more thorough assessment.\n",
      "\n",
      "4. **Application in Complex Systems**: In the context of complex AI systems, such as Retrieval-Augmented Generation (RAG) or AI agents, synthetic data helps in generating input-output datasets from knowledge bases and facilitates multi-turn interactions across various scenarios that are hard to design manually.\n",
      "\n",
      "5. **Efficient Evaluation**: It serves as a practical approach to expand the evaluation dataset efficiently, while expert human oversight can be directed towards higher-value areas of testing, ensuring a balanced evaluation strategy.\n",
      "\n",
      "Overall, synthetic data is not a substitute for real data or well-designed tests; instead, it enhances the evaluation process, allowing teams to cover a broader range of scenarios more efficiently ([source](https://github.com/evidentlyai/docs/tree/main/docs-main/synthetic-data/why_synthetic.mdx)).\n",
      "\n",
      "What factors should be considered when determining the evaluation metrics for a specific machine learning model?\n",
      "When determining the evaluation metrics for a specific machine learning model, several important factors should be considered:\n",
      "\n",
      "1. **Type of Problem**: The choice of metrics can vary significantly between classification and regression problems. For classification, metrics such as accuracy, precision, recall, F1-score, and ROC AUC are crucial. For regression problems, metrics like Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) are typically used.\n",
      "\n",
      "2. **Model Goal**: Understanding what you are trying to achieve with the model is vital. For example, if the priority is to minimize false positives, precision may be more critical. Conversely, if minimizing false negatives is essential, recall should be emphasized.\n",
      "\n",
      "3. **Class Imbalance**: In classification tasks where classes are imbalanced, accuracy may not be a sufficient measure. Metrics such as F1-score or area under the ROC curve (AUC-ROC) can provide a more comprehensive evaluation.\n",
      "\n",
      "4. **Stakeholder Requirements**: Consider what metrics stakeholders are most interested in. Different stakeholders may prioritize different outcomes, which affects which metrics should be reported.\n",
      "\n",
      "5. **Robustness of Metrics**: It’s important to consider the stability and interpretability of the metrics you choose. Metrics should effectively convey the model's performance and provide actionable insights, such as identifying model weaknesses or sources of error.\n",
      "\n",
      "6. **Visualization Support**: Utilizing metrics that can be complemented with visualizations to analyze model performance can provide deeper insights into where the model is making mistakes and how it can be improved.\n",
      "\n",
      "Evidently calculates standard evaluation metrics and provides interactive visualizations to assist in analyzing model performance. For classification, common metrics include accuracy, precision, recall, F1-score, and ROC AUC. For regression, standard metrics include Mean Error, MAE, and MAPE, along with insights into stability through the standard deviation of these metrics ([Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx), [Regression metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_regression.mdx)). \n",
      "\n",
      "Overall, these factors help ensure that the evaluation reflects the model's performance appropriately for the specific context of its use.\n",
      "\n",
      "How can the performance of an LLM be continuously monitored over time?\n",
      "To continuously monitor the performance of a large language model (LLM) over time, the following structured approach can be implemented:\n",
      "\n",
      "1. **Batch Monitoring with Evaluation API**:\n",
      "   - Utilize the core evaluation API to set up batch monitoring for your LLM. This involves defining metrics and conditions for evaluating your model's performance. The process begins by configuring an Evidently Report with optional tests to specify what evaluations are needed ([Batch monitoring](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx)).\n",
      "\n",
      "2. **Workflow Steps**:\n",
      "   - **Configure Metrics**: Define the metrics and tests to evaluate the LLM.\n",
      "   - **Run Evaluations**: Execute evaluations at scheduled intervals using tools like Apache Airflow. This can include checks for data quality, drift, and model predictions.\n",
      "   - **Upload Results**: Decide whether to store raw predictions or just upload metric summaries; this ensures that relevant data is readily available for analysis.\n",
      "   - **Setup Dashboards**: Create a dashboard that tracks results over time, allowing for a visual representation of the performance metrics.\n",
      "   - **Configure Alerts**: Set alerts for significant changes or failures in performance metrics to take timely corrective action.\n",
      "\n",
      "3. **Regression Testing**:\n",
      "   - Implement regression testing where new outputs from the model are compared against reference outputs. This involves creating a dataset of expected outputs, retrieving new outputs, and running tests to evaluate the performance over time ([LLM regression testing](https://github.com/evidentlyai/docs/tree/main/docs/examples/llm_regression_testing.mdx)).\n",
      "\n",
      "4. **Scheduled Evaluations**:\n",
      "   - Set up scheduled evaluations that automatically run at specified times. This automated approach allows for continuous monitoring without manual intervention, ensuring consistent oversight of the model's performance ([Overview](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_overview.mdx)).\n",
      "\n",
      "5. **Using Monitoring Dashboards**:\n",
      "   - Utilize dashboards to visualize and track the results over time, providing insights into performance trends. This helps in identifying anomalies or declines in output quality ([LLM Evaluation](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_llm.mdx)).\n",
      "\n",
      "In conclusion, continuous monitoring of LLM performance involves setting up automated evaluations, regression testing, and visual dashboards to effectively track and respond to changes in model behavior over time. Each component contributes to a comprehensive monitoring ecosystem designed to maintain and enhance model performance.\n",
      "\n",
      "What is the role of confusion matrices in evaluating machine learning models?\n",
      "Confusion matrices play an important role in evaluating machine learning models, particularly in classification tasks. They provide a visualization of the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions. Below are the key aspects and benefits of using confusion matrices:\n",
      "\n",
      "1. **Error Visualization**: A confusion matrix enables the visualization of classification errors and their types. This helps in understanding not just the overall accuracy of the model but also the specific areas where it performs poorly. For instance, it reveals whether a model is better at identifying one class over another.\n",
      "\n",
      "2. **Multiple Metrics Derivation**: From a confusion matrix, various evaluation metrics can be easily derived, such as accuracy, precision, recall, and F1 score. This helps in assessing different aspects of model performance beyond accuracy alone. \n",
      "\n",
      "3. **Class Imbalance Understanding**: In cases of class imbalance, where the number of instances in various classes differs significantly, a confusion matrix helps clarify how the model performs across these classes, allowing for better assessment and adjustment.\n",
      "\n",
      "4. **Model Comparison**: By comparing the confusion matrices of different models, one can identify which model has better performance characteristics and make more informed decisions based on those insights.\n",
      "\n",
      "Overall, confusion matrices provide critical insights that assist in model diagnostics, allowing practitioners to fine-tune models for better performance.\n",
      "\n",
      "For additional information, you can refer to the following source: [Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n",
      "\n",
      "How can visualizations enhance the understanding of model performance during evaluations?\n",
      "Visualizations play a crucial role in enhancing the understanding of model performance during evaluations in several ways:\n",
      "\n",
      "1. **Interactive Analysis**: Visualizations can help users interactively explore model evaluation metrics. For instance, they allow stakeholders to see where a model makes mistakes and can suggest areas for improvement. This is particularly useful for analyzing classification metrics such as accuracy, precision, and recall, where interactive visualizations from frameworks like Evidently enable a deeper dive into model performance [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n",
      "\n",
      "2. **Visualizing Predictions vs Actual Outcomes**: In regression evaluations, visualizations help display the predicted versus actual outcomes. By plotting these predictions, one can easily identify regions where the model underestimates or overestimates values, assisting in pinpointing specific areas that may need model adjustments [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_regression.mdx).\n",
      "\n",
      "3. **Clarity with Metrics**: Visual aids in the metrics documentation clarify complex data and performance indicators. Such representations can facilitate understanding, making it easier for stakeholders to grasp the evaluation results at a glance [source](https://github.com/evidentlyai/docs/tree/main/docs-main/images/metrics/readme.md).\n",
      "\n",
      "Overall, by employing effective visualizations, one can significantly improve comprehension and communication regarding model performance, leading to informed decision-making and more effective model tuning.\n",
      "\n",
      "What specific metrics can be used to evaluate the accuracy of image classification models?\n",
      "To evaluate the accuracy of image classification models, several key metrics can be employed:\n",
      "\n",
      "1. **Accuracy**: This is the ratio of correctly predicted instances to the total instances. It provides a quick overview of the model's performance.\n",
      "\n",
      "2. **Precision**: Precision measures the ratio of true positive predictions to the total positive predictions made by the model. It reflects the quality of the positive class predictions.\n",
      "\n",
      "3. **Recall (Sensitivity)**: Recall is the ratio of true positive predictions to the total actual positives. It indicates how well the model identifies positive instances.\n",
      "\n",
      "4. **F1-score**: The F1-score is the harmonic mean of precision and recall. It provides a balance between the two metrics, especially in cases of class imbalance.\n",
      "\n",
      "5. **ROC AUC (Receiver Operating Characteristic Area Under the Curve)**: This metric evaluates the model's ability to distinguish between classes and plots the trade-off between true positive rate and false positive rate.\n",
      "\n",
      "6. **Log Loss (Cross-Entropy Loss)**: Log Loss provides a measure of how close the predicted probabilities are to the actual classes, penalizing incorrect predictions based on their confidence.\n",
      "\n",
      "7. **Confusion Matrix**: This visual tool helps understand the performance of a model by depicting true positives, false positives, true negatives, and false negatives.\n",
      "\n",
      "These metrics can help to analyze different aspects of the model's performance and identify areas for improvement. They are essential for ensuring that the model is not only accurate but also reliable and effective in making predictions across potentially skewed classes or distributions [Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n",
      "\n",
      "Why is regression testing necessary when monitoring machine learning models in production?\n",
      "Regression testing is a crucial step in monitoring machine learning models in production for several reasons:\n",
      "\n",
      "1. **Performance Consistency**: When models are updated, whether through new training data or algorithm changes, regression testing ensures that the performance of the model does not degrade. This is particularly important as small changes can lead to significant differences in outcomes.\n",
      "\n",
      "2. **Change Detection**: It allows teams to check for changes in model outputs against a known reference (previous model versions or expected outputs). This helps identify unwanted changes due to updates or shifts in the input data distribution.\n",
      "\n",
      "3. **Debugging Capability**: If a model underperforms, regression testing can help identify specific areas where the model's accuracy has dropped. By comparing metrics such as Mean Absolute Error (MAE) and visualizations of errors, data scientists can diagnose issues more effectively ([Regression Quality Preset](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n",
      "\n",
      "4. **Automated Testing**: With tools like the `RegressionPreset`, regression tests can be automatically generated and assessed. This allows for continuous monitoring of model performance metrics against benchmarks and easier decision-making regarding model updates or retraining ([Overview of Regression Quality Preset](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n",
      "\n",
      "5. **Production Monitoring**: Regular evaluations using regression tests can give insights into how well the model is performing in the real world, facilitating early detection of issues and allowing for swift corrective actions when the model's predictions start to deviate from the expected ([Production Monitoring](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n",
      "\n",
      "In summary, regression testing is essential for maintaining the reliability and effectiveness of machine learning models in production environments, helping ensure that updates do not introduce regressions while providing the insights necessary for informed decisions.\n",
      "\n",
      "How does the concept of precision-recall differ in the context of model evaluation?\n",
      "In the context of model evaluation, **precision** and **recall** are two important metrics that measure the effectiveness of a classification model, particularly in binary classification tasks.\n",
      "\n",
      "### Precision\n",
      "Precision refers to the proportion of true positive predictions to the total predicted positives (i.e., the sum of true positives and false positives). High precision indicates that when the model predicts a positive class, it is usually correct. It is mathematically defined as:\n",
      "\n",
      "\\[\n",
      "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
      "\\]\n",
      "\n",
      "### Recall\n",
      "Recall, on the other hand, measures the proportion of true positive predictions relative to the total actual positives (i.e., the sum of true positives and false negatives). High recall indicates that the model successfully identifies a large portion of the actual positive cases. Its formula is:\n",
      "\n",
      "\\[\n",
      "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
      "\\]\n",
      "\n",
      "### Trade-off Between Precision and Recall\n",
      "These two metrics often have an inverse relationship, forming a trade-off when adjusting the classification thresholds. If a model is tuned to maximize precision, it may miss a number of actual positive cases, thus lowering recall, and vice versa. This relationship is often visualized using a **Precision-Recall Curve**, which illustrates how the model's precision and recall change at different thresholds.\n",
      "\n",
      "The choice between optimizing for precision or recall depends on the specific context of the application:\n",
      "- **Precision** is critical in scenarios where false positives are costly or undesirable.\n",
      "- **Recall** is more important in situations where failing to identify a positive case (false negative) is more detrimental.\n",
      "\n",
      "In summary, both metrics are vital for assessing model performance, and understanding their balance is key for developing effective classification models. For further detailed examples and visual representations, refer to the documentation [here](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n",
      "\n",
      "What steps should be taken to establish a custom metric for evaluating a specific model's performance?\n",
      "To establish a custom metric for evaluating a specific model's performance, you can follow these steps based on the documentation from Evidently AI:\n",
      "\n",
      "1. **Implement the Metric Calculation Method**: This is a required step where you define how the custom metric will be calculated. This involves coding the logic that determines the performance based on your criteria.\n",
      "\n",
      "2. **Define Default Test Conditions (Optional)**: You can set up default parameters for the tests that will be applied when using the metric. This step allows you to personalize the evaluations further.\n",
      "\n",
      "3. **Create a Custom Visualization (Optional)**: If needed, you can use visualization tools like Plotly to create graphs that represent the metric visually, assisting in better interpretation of the results.\n",
      "\n",
      "4. **Test Your Custom Metric**: After implementing your metric, it’s important to run it against a dataset to validate its performance. This can involve creating a sample dataset to ensure that the metric behaves as expected.\n",
      "\n",
      "5. **Reference the Metric in Reports**: Post validation, you can include your custom metric in reports for monitoring and tracking model performance over time.\n",
      "\n",
      "6. **Monitor Over Time**: Utilize dashboards to continuously evaluate the performance of the model, enabling adjustments based on the custom metrics you've defined.\n",
      "\n",
      "These steps facilitate the creation and implementation of metrics tailored to specific evaluation needs, which can include business metrics or other weighted scores.\n",
      "\n",
      "For more detailed information on customizing metrics, refer to the documentation on creating a [Custom Metric](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_metric.mdx).\n",
      "\n",
      "How can you effectively use Evidently AI to evaluate and monitor LLMs in real-time?\n",
      "To effectively use Evidently AI for evaluating and monitoring large language models (LLMs) in real-time, you can utilize several key features and approaches:\n",
      "\n",
      "1. **LLM Jury Evaluation**:\n",
      "   - This method involves using multiple LLMs to evaluate the same output. You can aggregate results and only consider an output a \"pass\" if it is approved by all or a majority of the evaluating LLMs. This also allows you to identify discrepancies among different models. You will need to set up evaluator models and pass API keys for the LLMs being used.\n",
      "   - Example setup:\n",
      "     ```python\n",
      "     import os\n",
      "     os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n",
      "     os.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\n",
      "     os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\n",
      "     ```\n",
      "   - Create a project in Evidently Cloud to store evaluation results.\n",
      "   - Additional details can be found in the tutorial: [LLM-as-a-jury](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_jury.mdx).\n",
      "\n",
      "2. **Integration with CI/CD**:\n",
      "   - Use Evidently AI as part of your Continuous Integration/Continuous Deployment (CI/CD) workflow. This allows you to automatically evaluate LLM outputs every time you make changes, ensuring rapid feedback loops and high confidence in production. Check the tutorial in the GitHub actions example: [Evidently CI Example](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx).\n",
      "\n",
      "3. **Observability and Experimentation**:\n",
      "   - Evidently AI provides tools for LLM observability by allowing you to run evaluations on production data and track metrics over time. You can explore evaluation results visually, compare various models and prompts, and conduct regression or adversarial tests. This enhances your ability to quickly iterate and stabilize your LLM deployments.\n",
      "\n",
      "4. **Tracing Capabilities**:\n",
      "   - Set up tracing for your LLM applications to capture inputs and outputs, which can then be evaluated in real-time. This involves using open-source libraries like Tracely and Evidently to collect and analyze the data seamlessly. More details are available in the tracing quickstart guide: [Tracing LLM Inputs and Outputs](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_tracing.mdx).\n",
      "\n",
      "By utilizing these approaches, you can ensure that your LLMs are evaluated comprehensively and monitored effectively in real-time, thereby making informed decisions in their deployment and optimization.\n",
      "\n",
      "For further details, check the respective sections in the Evidently AI documentation and tutorials: \n",
      "- [LLM Jury Evaluation](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_jury.mdx)\n",
      "- [Evidently CI Example](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx)\n",
      "- [Tracing LLM Inputs and Outputs](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_tracing.mdx)\n",
      "\n",
      "What are the implications of using biased training datasets on the evaluation of machine learning models?\n",
      "Using biased training datasets can have several significant implications for the evaluation of machine learning models:\n",
      "\n",
      "1. **Performance Degradation**: Models trained on biased datasets may exhibit poor performance on unseen data, especially if that data reflects a more balanced distribution of features or classes. When evaluated, the model's accuracy and reliability can be significantly lower than expected, leading to misguided conclusions about its effectiveness.\n",
      "\n",
      "2. **Misleading Evaluation Metrics**: Bias in training data can skew evaluation metrics (like precision, recall, and F1 score). For instance, if a certain class is under-represented during training, a model may predict heavily toward the majority class, thus artificially inflating its evaluation metrics when tested on a similar biased dataset.\n",
      "\n",
      "3. **Unequal Treatment of Groups**: Bias in datasets can lead models to perform poorly for under-represented or marginalized groups, resulting in ethical and legal implications. Evaluations may mask these issues if they do not reflect the performance across diverse populations.\n",
      "\n",
      "4. **Reinforcement of Pre-existing Biases**: Machine learning models are vulnerable to perpetuating biases present in training data. If these biases are not recognized and managed during the evaluation phase, they can contribute to systemic issues, particularly in sensitive applications like hiring, law enforcement, and healthcare.\n",
      "\n",
      "5. **Need for Comprehensive Testing**: Relying solely on biased datasets for evaluation limits understanding of model robustness. Evaluations should involve diverse datasets to ensure models can generalize well across various scenarios and groups.\n",
      "\n",
      "6. **Challenges in Data Drift and Model Monitoring**: Bias can also complicate ongoing performance monitoring and data drift evaluations. If the training and evaluation datasets continue to convey biased information, it can hinder the accurate detection of whether a model is still functioning effectively in real-world scenarios ([docs-main/docs/platform/datasets_overview.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/datasets_overview.mdx)).\n",
      "\n",
      "In conclusion, the implications of using biased training datasets require careful consideration to ensure machine learning models are fair, reliable, and effectively evaluated. Addressing bias in datasets is crucial for ethical AI deployment and maintaining model integrity.\n",
      "\n",
      "How can continuous integration/continuous deployment (CI/CD) practices affect model evaluation processes?\n",
      "Continuous Integration/Continuous Deployment (CI/CD) practices can significantly enhance model evaluation processes in several ways:\n",
      "\n",
      "1. **Automated Evaluation**: CI/CD frameworks can automate the evaluation of machine learning models as part of the regular integration and deployment cycles. This allows teams to run evaluations each time changes are made to the codebase, ensuring that model performance is continuously assessed against a fixed set of evaluation criteria. This can lead to faster feedback loops and identification of issues before they reach production.\n",
      "\n",
      "2. **Integration of Evaluation Reports**: By integrating model evaluations with CI/CD tools, teams can generate evaluation reports as part of the deployment pipeline. These reports can include detailed examination of model outputs against test datasets, metrics tracking, and any other performance indicators that are crucial for assessing model quality. This continuous monitoring helps in maintaining model performance over time and aids in decision-making regarding model updates or rollbacks [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx).\n",
      "\n",
      "3. **Custom Evaluation Criteria**: CI/CD setups allow the incorporation of custom evaluation criteria based on the specific needs of the project. For instance, teams can configure how outputs are assessed—using metrics like tone, correctness, and classification quality. This level of customization leads to more relevant and accurate evaluations of model performance under deployment conditions [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx).\n",
      "\n",
      "4. **Error Tracking and Alerts**: With CI/CD, teams can set up alerts for any evaluation failures. This means that if a model fails to meet the defined criteria during evaluation, the CI job can fail, preventing inadequate models from being deployed and ensuring that only validated models make it to production [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_regression_testing.mdx).\n",
      "\n",
      "5. **Version Control of Model Evaluations**: CI/CD supports version control not just of the code but also of the evaluation scripts and configurations, making it easier to track changes over time. This ensures that the stakeholders can revert to previous evaluation setups whenever necessary, promoting more reliable and repeatable evaluations [source](https://github.com/evidentlyai/docs/tree/main/docs-main/faq/why_evidently.mdx).\n",
      "\n",
      "6. **Monitoring Trends**: CI/CD pipelines can include monitoring features that allow teams to observe trends in model performance over time. This can be implemented through dashboards that track key evaluation metrics, helping teams to adjust their models proactively based on real-time data and trends [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/monitoring_overview.mdx).\n",
      "\n",
      "In summary, CI/CD practices facilitate more structured, automated, and effective evaluation processes, resulting in higher confidence in model quality and faster deployment cycles. These improvements are crucial for teams looking to maintain high standards in machine learning model performance.\n",
      "\n",
      "Why is feature importance analysis a critical part of model monitoring?\n",
      "Feature importance analysis is a critical component of model monitoring due to several key reasons:\n",
      "\n",
      "1. **Understanding Model Behavior**: It allows practitioners to comprehend which features significantly impact model predictions. By analyzing feature importance, teams can identify whether the model relies on the expected features or if it is influenced by unexpected ones. This understanding can guide decisions during model deployment and adjustments.\n",
      "\n",
      "2. **Monitoring for Drift**: Feature importance helps in monitoring changes in data distributions over time. If the importance of certain features significantly shifts, it can indicate potential data drift, where the relationship between input features and target outcomes changes. Recognizing this early can assist in maintaining model accuracy and reliability.\n",
      "\n",
      "3. **Debugging and Diagnosing Issues**: When model performance degrades, feature importance analysis can help pinpoint which features are contributing to errors or biases in predictions. It aids in diagnosing issues more effectively by revealing if the model is overfitting or relying too heavily on certain features.\n",
      "\n",
      "4. **Guiding Feature Selection and Engineering**: Understanding which features contribute most to model predictions can inform decisions about feature selection during future iterations of model development. This can lead to better-performing models by focusing on the most impactful variables and potentially reducing complexity by removing less important features.\n",
      "\n",
      "5. **Improving Model Transparency**: In an era where model transparency is increasingly demanded, providing insights into feature importance can enhance trust in AI systems. Stakeholders and end-users are more likely to accept models when they understand why certain decisions are made.\n",
      "\n",
      "Overall, by incorporating feature importance analysis into model monitoring practices, organizations can make informed decisions, enhance model performance, and ensure robustness against unforeseen changes in data.\n",
      "\n",
      "For more information on monitoring practices, you can refer to the documentation: [Overview of Batch Monitoring](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx).\n",
      "\n",
      "How can benchmarking against previous model versions assist in evaluation and monitoring?\n",
      "Benchmarking against previous model versions is critical for effective evaluation and monitoring of machine learning models. This process allows practitioners to:\n",
      "\n",
      "1. **Identify Performance Trends**: By comparing the performance metrics of different model versions, it becomes feasible to track improvements or regressions over time. This is essential to ensure that updates to the model are indeed beneficial.\n",
      "\n",
      "2. **Understand Model Behavior**: Benchmarking provides insights into how changes in the model impact its performance on various tasks or datasets. It can help in identifying which specific changes (e.g., feature adjustments or algorithm updates) lead to performance variations.\n",
      "\n",
      "3. **Facilitate Quality Assurance**: Regularly comparing against established benchmarks serves as a quality control mechanism. If a new model version performs significantly worse than its predecessors, it raises flags for further investigation.\n",
      "\n",
      "4. **Support Explainability and Transparency**: With the ability to visualize differences between model versions, stakeholders can understand why certain models perform better. This transparency can be vital when justifying model choices in production environments.\n",
      "\n",
      "5. **Optimize Parameters and Strategies**: Benchmarking can highlight which configurations yield the best results, allowing for more informed decisions in hyperparameter tuning and model selection strategies.\n",
      "\n",
      "For practical implementation, using tools for batch monitoring and evaluation can create comprehensive reports that compare the performance of various models side by side. For instance, utilizing features from evaluation libraries can help streamline this process, track results over time, and set up alerts based on performance metrics.\n",
      "\n",
      "For more detailed information on model evaluation and monitoring, you can check the relevant resources in the documentation:\n",
      "- [Batch Monitoring Overview](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx)\n",
      "- [Comparing Evaluations](https://github.com/evidentlyai/docs/tree/main/docs/platform/evals_explore.mdx)\n",
      "\n",
      "What role does user feedback play in the ongoing evaluation of machine learning models?\n",
      "User feedback plays a crucial role in the ongoing evaluation of machine learning models in several ways:\n",
      "\n",
      "1. **Performance Monitoring**: User feedback helps track the quality of model predictions, particularly in production environments. Continuous feedback indicates how well the model performs in real-world situations, allowing developers to identify areas needing improvement [source: overview](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "2. **Model Adaptation and Improvement**: Incorporating user feedback allows models to be fine-tuned over time. This might involve retraining the models with new data that reflects user input, leading to better accuracy and relevance in the outputs. Such adaptive practices ensure that models remain effective as user needs evolve [source: why_evidently](https://github.com/evidentlyai/docs/tree/main/docs-main/faq/why_evidently.mdx).\n",
      "\n",
      "3. **Testing and Evaluation Optimization**: Feedback can influence the design of evaluation mechanisms, enabling the identification of metrics that reflect user satisfaction and model effectiveness. Different types of evaluations, including safety checks and regression testing, can be informed by user experiences and input [source: evaluations](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "4. **Identifying Edge Cases**: User interactions can reveal edge cases or situations that the model may not handle well, helping developers refine the model's robustness against various inputs. This is particularly important in adversarial testing and ensuring overall safety in AI applications [source: evaluations](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n",
      "\n",
      "Overall, user feedback is integral to creating responsive, effective machine learning systems that align with user expectations and real-world requirements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction(\n",
    "        agent,\n",
    "        result.new_messages_json(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b02b3315-0534-4056-bae3-a7dc7678878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    log_record = load_log_data(log_file=log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "90d658f3-1ff8-4b31-a3cf-ea88346c80c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e891fbb5fc4811a664538497b8e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "for eval_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent=evaluation_agent,\n",
    "                                      log_record=eval_record)\n",
    "    eval_results.append((eval_result, eval_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dbc883a7-0f0e-4bfa-94ef-b93bdcdb42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "53268882-735e-4aa4-8c6c-bf93a891f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for data in eval_results:\n",
    "    row = {\"log_file\": data[1][\"log_file\"],\n",
    "           \"source\":  data[1][\"source\"],\n",
    "           \"model\": data[1][\"model\"]}\n",
    "    checks = {c.check_name: c.check_pass for c in data[0].checks}\n",
    "    row.update(checks)\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b7d6b313-682a-4739-b30a-acd9dd17f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "37d93d32-26f7-49ca-bfd9-4f4b77853e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    0.80\n",
       "instructions_avoid     1.00\n",
       "answer_relevant        1.00\n",
       "answer_clear           1.00\n",
       "answer_citations       0.75\n",
       "completeness           1.00\n",
       "factual                1.00\n",
       "tool_call_search       0.95\n",
       "dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the evaluation results\n",
    "eval_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ddefe4f5-3ed3-42bc-9db4-6291e3f4a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall llm judge evaluation tells us , that the agent is quite reliable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
