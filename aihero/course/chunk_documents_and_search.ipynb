{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cf24cb3-7c14-4482-810a-d7d9dad24b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "import numpy as np\n",
    "from minsearch import Index, VectorSearch\n",
    "from openai import OpenAI, APIConnectionError, RateLimitError, APIStatusError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153fc62f-05fc-4849-92fd-62e7728cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    base_url = \"https://codeload.github.com\"\n",
    "    repo_url = f\"{base_url}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "    \n",
    "    resp = requests.get(repo_url)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    with zipfile.ZipFile(io.BytesIO(initial_bytes=resp.content)) as zf:\n",
    "        for file_info in zf.infolist():\n",
    "            filename = file_info.filename.lower()\n",
    "            if not (filename.endswith(\".md\") or filename.endswith(\".mdx\")):\n",
    "                continue\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read()\n",
    "                    post = frontmatter.loads(content)\n",
    "                    data = post.to_dict()\n",
    "                    data['filename'] = filename\n",
    "                    repository_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "        return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97a628e-76fe-4128-a1a8-77bbe13f2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_docs = read_repo_data('evidentlyai', 'docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611aeb40-632a-4335-baad-2e87d1a9f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=60),\n",
    "    stop=stop_after_attempt(max_attempt_number=5),\n",
    "    retry=retry_if_exception_type((APIConnectionError, RateLimitError, APIStatusError))\n",
    ")\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14d84d3-79ee-455c-bed2-3d565db75d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d6629c5-45ba-4e40-b89d-f773272ecb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    \"\"\"Creating intelligent chunks\"\"\"\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee252ed4-451e-45fd-9644-b4ae446502ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processing(doc):\n",
    "    \"\"\"Doc processing\"\"\"\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    chunked_docs = []\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        chunked_docs.append(section_doc)\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e218f7-17e1-49dc-952f-2a502b450916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45b70bcc93d45dcb6ed31bcd0cdedc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_results = []\n",
    "MAX_WORKERS = 8\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures_data = {executor.submit(doc_processing, doc): doc for doc in evidently_docs}\n",
    "\n",
    "    for future in tqdm(as_completed(futures_data), total=len(futures_data), desc=\"Processing\"):\n",
    "        try:\n",
    "            doc_sections = future.result()\n",
    "            doc_results.extend(doc_sections)\n",
    "        except Exception as e:\n",
    "            failed_doc = futures_data[future]\n",
    "            print(f\"\\nError processing document: {e}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0933a0c2-05f8-486c-9875-5f65c579c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Create Plant',\n",
       "  'openapi': 'POST /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/create.mdx',\n",
       "  'section': \"Sure! However, I notice that there's no specific content provided within the `<DOCUMENT>` tags. Please provide the text you would like me to structure into logical sections for a Q&A system, and I'll be happy to help!\"},\n",
       " {'title': 'Delete Plant',\n",
       "  'openapi': 'DELETE /plants/{id}',\n",
       "  'filename': 'docs-main/api-reference/endpoint/delete.mdx',\n",
       "  'section': 'It seems that you\\'ve referenced a \"provided document,\" but I can\\'t see any content in your message. If you could share the details or content from the document, I\\'d be happy to help you split it into logical sections for a Q&A system. Please provide the text you\\'d like organized.'},\n",
       " {'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx',\n",
       "  'section': '## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>'},\n",
       " {'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx',\n",
       "  'section': '## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Introduction to the Q&A System\\n\\nThis section provides an overview of what a Q&A system is, including its purpose, functionalities, and applications in various fields.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Types of Q&A Systems\\n\\nA breakdown of the various types of Q&A systems available, including rule-based, retrieval-based, and generative models, along with their pros and cons.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Key Technologies Used\\n\\nAn exploration of the technologies that power Q&A systems, such as natural language processing (NLP), machine learning, and databases.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Application Areas\\n\\nDetails on the different areas where Q&A systems are implemented, including customer support, education, and information retrieval.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Designing a Q&A System\\n\\nGuidelines on how to design an effective Q&A system, covering aspects like data acquisition, model selection, and user interface considerations.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Challenges in Q&A Systems\\n\\nAn overview of the common challenges faced in developing and maintaining Q&A systems, such as ambiguity, scalability, and user satisfaction.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Future Trends\\n\\nA discussion on the future of Q&A systems, including advancements in AI and machine learning, and how they might shape the evolution of these systems.'},\n",
       " {'title': 'Get Plants',\n",
       "  'openapi': 'GET /plants',\n",
       "  'filename': 'docs-main/api-reference/endpoint/get.mdx',\n",
       "  'section': '## Conclusion\\n\\nSummarization of key points discussed in the document and the importance of continuing development in Q&A systems to enhance user experience and accuracy.'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Viewing and Exporting Reports\\n\\nYou can view or export Reports in multiple formats.'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Pre-requisites\\n\\n* You know how to [generate Reports](/docs/library/report).'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Logging to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Viewing in Jupyter Notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': \"## Saving as HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you're working in a Python environment that doesn’t display interactive visuals.\"},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Exporting as JSON\\n\\nYou can get the results of the calculation as a JSON file. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```'},\n",
       " {'title': 'Output formats',\n",
       "  'description': 'How to export the evaluation results.',\n",
       "  'filename': 'docs-main/docs/library/output_formats.mdx',\n",
       "  'section': '## Exporting as Python Dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```'},\n",
       " {'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx',\n",
       "  'section': '## Overview of Evidently Python Library\\n\\nThe Evidently Python library is an open-source tool designed to evaluate, test, and monitor the quality of AI systems, from experimentation to production. You can use the evaluation library on its own or as part of the Monitoring Platform (self-hosted or Evidently Cloud).'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_results[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4158b282-5bda-4feb-8473-229ed5f9a746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x109a62b40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(text_fields=['title', 'description', 'filename', 'section'],\n",
    "              keyword_fields=[])\n",
    "index.fit(doc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9199cb6-7791-4e3f-8838-11e6d21bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "search_results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb7e47a-cd71-4a25-be85-5ddef187bde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx', 'section': '## Overview of Retrieval-Augmented Generation (RAG) Systems\\n\\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential.'}, {'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx', 'section': '## Creating a RAG Test Dataset\\n\\nYou can generate a ground truth RAG dataset directly from your data source.\\n\\n### Steps to Create a Dataset\\n\\n1. **Create a Project**\\n   - In the Evidently UI, start a new Project or open an existing one.\\n   - Navigate to “Datasets” in the left menu.\\n   - Click “Generate” and select the “RAG” option.\\n\\n   ![](/images/synthetic/synthetic_data_select_method.png)\\n\\n2. **Upload Your Knowledge Base**\\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\\n   - Choose the number of inputs to generate.\\n   - Decide if you want to include the context used to generate the answer.\\n\\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\n   - Simply drop the file, then:\\n     - Choose the number of inputs.\\n     - Include/exclude context as needed.\\n\\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n   <Info>\\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\\n   </Info>\\n\\n3. **Review the Test Cases**\\n   - You can preview and refine the generated dataset.\\n\\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\n   - Options available:\\n     - Use “More like this” to add variations.\\n     - Drop rows that aren’t relevant.\\n     - Manually edit questions or responses.\\n\\n4. **Save the Dataset**\\n   - Once finished, store the dataset.\\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\\n\\n   <Info>\\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\\n   </Info>'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 2. Prepare the Dataset\\n\\nYou will create a toy demo chatbot dataset with \"Questions\" and \"Answers\":\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    ...\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n```\\n\\n**Preparing your own data:** You can provide data in various structures including inputs and outputs from your LLM.\\n\\n**Collecting live data:** Tracing inputs and outputs from your LLM app allows you to download the dataset from traces.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': \"## What's Next?\\n\\nFor further reading, explore how to configure custom LLM judges or other models. Check out additional examples and tutorials available on the platform.\"}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 5. Get a Dashboard\\n\\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You\\'ll be able to observe trends and set up alerts as you log more evaluation results.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 3. Run Evaluations\\n\\nYou will evaluate the answers based on:\\n\\n- **Sentiment:** Score ranges from -1 (negative) to 1 (positive).\\n- **Text length:** Measured by character count.\\n- **Denials:** Refusals to answer, using an LLM-as-a-judge with a built-in prompt.\\n\\nSet the OpenAI key as an environment variable for LLM-as-a-judge:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\nTo run evaluations:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")\\n    ]\\n)\\n```\\n\\n**Congratulations!** You\\'ve successfully run your first evaluation.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 4. Create a Report\\n\\nTo create and run a report summarizing the evaluation results:\\n\\n```python\\nreport = Report([TextEvals()])\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\nYou can preview the report locally or save it as a JSON, Python dictionary, or HTML file.\\n\\nTo upload the report to Evidently Cloud:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 7. (Optional) Add a Custom LLM Judge\\n\\nYou can create custom criteria using built-in LLM judge templates, such as classifying questions as \"appropriate\" or \"inappropriate\":\\n\\n```python\\nappropriate_scope = BinaryClassificationPromptTemplate(\\n    criteria=\"\"\"An appropriate question is any educational query related to ...\"\"\",\\n    target_category=\"APPROPRIATE\",\\n    non_target_category=\"INAPPROPRIATE\",\\n    include_reasoning=True,\\n)\\n\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"question\", template=appropriate_scope, provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Question topic\")\\n    ]\\n)\\n```'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 6. (Optional) Add Tests\\n\\nYou can implement conditions for your evaluations, such as expecting:\\n\\n- **Sentiment** is non-negative.\\n- **Text length** is at most 150 symbols.\\n- **Denials**: None.\\n\\nImplement conditions as follows:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[lte(150, alias=\"Has_expected_length\")]),\\n        ...\\n    ]\\n)\\n```'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## Introduction to Evidently\\n\\nEvidently helps you evaluate LLM outputs automatically. The platform enables comparisons of prompts and models, along with regression or adversarial tests, providing clear and repeatable checks. This leads to faster iterations, more confident decisions, and fewer surprises in production.'}]\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8408819d-8a06-4af8-8b52-36442b9d37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80cb8dc-d196-4c23-9ba1-cd6bd7f33fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a877b9baa04adcb8800843ea589a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ae0ee9d73e43658d11a6d7076431e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71b260ca79a4080a9d38db89c4b7cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da47dc019ef143e09e861652e0369771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c690a761cee40abbd0aad09a3361064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/523 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09faa67144274937bfaf074b4e560c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114fab106c14f85bf0b64327bde8c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab3ad6f294349b099ae20158e91ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f30a8ab42a446928e7dad13a4de6acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b991725c54e3f911773af004cbba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cf23fc7b91498eb6f14d9e16384763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model for qa purpose in english\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defbae5b-e6d5-4494-b902-030f601b5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = doc_results[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d5ec6b-2db8-4475-bc1a-5a717cd88596",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_embedding = embedding_model.encode(record['section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a278b3fd-841c-4718-8ffd-24445e3862c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.encode(query)\n",
    "similarity = query_embedding.dot(record_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "636be4d6-ef7e-4133-85b9-715a6088efc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.24755448)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not the best similarity\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffb31e1f-a7d2-4714-b512-d25868a05ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1464c4e0ee454e96b9bfb31fde6e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x32713c410>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_results_embeddings = []\n",
    "for d in tqdm(doc_results):\n",
    "    v = embedding_model.encode(d['section'])\n",
    "    doc_results_embeddings.append(v)\n",
    "doc_results_embeddings = np.array(doc_results_embeddings)\n",
    "doc_vindex = VectorSearch()\n",
    "doc_vindex.fit(doc_results_embeddings, doc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52aca136-ac39-4699-8919-c3758644bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08784615, -0.01384743, -0.02574886, ..., -0.03691017,\n",
       "         0.02301694,  0.00571875],\n",
       "       [ 0.06002798,  0.02636276, -0.01116224, ..., -0.0110259 ,\n",
       "        -0.0324511 ,  0.01139372],\n",
       "       [ 0.01155654,  0.00877271, -0.05588597, ..., -0.0116422 ,\n",
       "        -0.06385411, -0.01948082],\n",
       "       ...,\n",
       "       [ 0.03450141,  0.02419186,  0.03138496, ...,  0.01626617,\n",
       "        -0.00152633,  0.01150939],\n",
       "       [ 0.03450141,  0.02419186,  0.03138496, ...,  0.01626617,\n",
       "        -0.00152633,  0.01150939],\n",
       "       [-0.01549505,  0.03854848,  0.04474124, ...,  0.03841428,\n",
       "        -0.01975037,  0.01021559]], shape=(770, 768), dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_results_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22f2a6-7a74-4e1a-824a-8a206c86a09f",
   "metadata": {},
   "source": [
    "# Lets implement hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e3bc24a-9b20-46f8-b361-9bec0df68501",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I evaluate classification model results, and ensure numerical data is not drifted?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a3a6fcf-27f3-4e51-a503-f9b99299c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results = index.search(query=query, num_results=5)\n",
    "q_v = embedding_model.encode(query)\n",
    "vector_results = doc_vindex.search(query_vector=q_v, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a69e956-006a-4cce-8f42-f89a18cf6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Use Cases for Synthetic Test Inputs\\n\\nEvidently Cloud can be utilized for multiple purposes, including:\\n\\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\\n* **Regression Testing**: Validate changes in your AI system before deployment.\\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\\n\\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library.'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Example of Generating Test Inputs\\n\\nAn illustrative example of how to generate synthetic test inputs can be found in the accompanying GIF, depicting the data generation process.\\n\\n![](/images/synthetic/datagen_travel.gif)'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### RAG Dataset\\n\\nYou can generate a Q&A dataset from the knowledge source to enhance information retrieval and response accuracy. [Learn more](https://www.evidentlyai.com/synthetic-data/rag_data).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Introduction to Evidently Cloud\\n\\nEvidently Cloud is a platform that allows users to generate synthetic test inputs and outputs for evaluating AI systems. For more details on pricing, you can check [pricing](https://www.evidentlyai.com/pricing) and for a demo, please [reach out](https://www.evidentlyai.com/get-demo).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### Adversarial Tests\\n\\nCreate inputs specifically designed to test for vulnerabilities in your AI system. This is crucial for ensuring robustness against adversarial attacks. [Learn more](https://www.evidentlyai.com/synthetic-data/adversarial_data).'},\n",
       " {'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx',\n",
       "  'section': '## Use Cases\\n\\nYou can evaluate data drift in various scenarios:\\n\\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\\n\\n<Info>\\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\\n</Info>'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Dataset Drift\\n\\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Drift Detection Methods\\n\\n### Tabular Data\\n\\nMethods applicable to tabular data include:\\n\\n- **Kolmogorov–Smirnov (K-S) Test**: Default for numerical data (if ≤ 1000 objects).\\n- **Chi-Square Test**: Default for categorical data with > 2 labels (if ≤ 1000 objects).\\n- **Population Stability Index (PSI)**: Applies to numerical and categorical data.\\n\\nA full list of methods and their default thresholds is available.\\n\\n### Text Data\\n\\nText drift detection applies to columns with raw text data. Notable methods include:\\n\\n- **Text Content Drift**: Trained classifier model distinguishing between text in “current” and “reference” datasets with different thresholds based on the number of objects.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Custom Drift Detection Method\\n\\nIf a suitable drift detection method isn’t found, you can implement a custom function. Below is a basic example:\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\\n```\\nDefine the method and parameters to create a custom statistic test for drift detection.'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "637c60f3-d165-404d-be99-7ef7de753d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e7b85aa-2b1e-4416-bf3c-e9e649306987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return doc_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        text_to_hash = result['filename'] + ' ' + result['section'][0:250]\n",
    "        encoded_string = text_to_hash.encode('utf-8')\n",
    "        hash_object = hashlib.sha256(encoded_string)\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1d4ff51-c36c-459e-9d74-cd7c1c2307fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Use Cases for Synthetic Test Inputs\\n\\nEvidently Cloud can be utilized for multiple purposes, including:\\n\\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\\n* **Regression Testing**: Validate changes in your AI system before deployment.\\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\\n\\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library.'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Example of Generating Test Inputs\\n\\nAn illustrative example of how to generate synthetic test inputs can be found in the accompanying GIF, depicting the data generation process.\\n\\n![](/images/synthetic/datagen_travel.gif)'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### RAG Dataset\\n\\nYou can generate a Q&A dataset from the knowledge source to enhance information retrieval and response accuracy. [Learn more](https://www.evidentlyai.com/synthetic-data/rag_data).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Introduction to Evidently Cloud\\n\\nEvidently Cloud is a platform that allows users to generate synthetic test inputs and outputs for evaluating AI systems. For more details on pricing, you can check [pricing](https://www.evidentlyai.com/pricing) and for a demo, please [reach out](https://www.evidentlyai.com/get-demo).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### Adversarial Tests\\n\\nCreate inputs specifically designed to test for vulnerabilities in your AI system. This is crucial for ensuring robustness against adversarial attacks. [Learn more](https://www.evidentlyai.com/synthetic-data/adversarial_data).'},\n",
       " {'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx',\n",
       "  'section': '## Use Cases\\n\\nYou can evaluate data drift in various scenarios:\\n\\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\\n\\n<Info>\\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\\n</Info>'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Dataset Drift\\n\\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Drift Detection Methods\\n\\n### Tabular Data\\n\\nMethods applicable to tabular data include:\\n\\n- **Kolmogorov–Smirnov (K-S) Test**: Default for numerical data (if ≤ 1000 objects).\\n- **Chi-Square Test**: Default for categorical data with > 2 labels (if ≤ 1000 objects).\\n- **Population Stability Index (PSI)**: Applies to numerical and categorical data.\\n\\nA full list of methods and their default thresholds is available.\\n\\n### Text Data\\n\\nText drift detection applies to columns with raw text data. Notable methods include:\\n\\n- **Text Content Drift**: Trained classifier model distinguishing between text in “current” and “reference” datasets with different thresholds based on the number of objects.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Custom Drift Detection Method\\n\\nIf a suitable drift detection method isn’t found, you can implement a custom function. Below is a basic example:\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\\n```\\nDefine the method and parameters to create a custom statistic test for drift detection.'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ec8b343-e3f5-4460-a3d7-2236db3e2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1278aee6-4f4c-4859-a157-e02b08b5c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'embeddings': doc_results_embeddings,\n",
    "    'documents': doc_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07eed7f0-36e6-4ffe-9930-785e27fa11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully pickled and saved to vector_search_data.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = \"vector_search_data.pkl\"\n",
    "try:\n",
    "    with open(f'{file_path}', 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    print(f\"Data successfully pickled and saved to {file_path}\")\n",
    "except pickle.PicklingError as e:\n",
    "    print(f\"An error occurred during pickling: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"An I/O error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
