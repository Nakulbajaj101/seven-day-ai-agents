{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf24cb3-7c14-4482-810a-d7d9dad24b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index, VectorSearch\n",
    "from openai import OpenAI, APIConnectionError, RateLimitError, APIStatusError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "import pickle\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e998bea4-398a-4c0d-8625-42684d4de6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for qa purpose in english\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "611aeb40-632a-4335-baad-2e87d1a9f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=60),\n",
    "    stop=stop_after_attempt(max_attempt_number=5),\n",
    "    retry=retry_if_exception_type((APIConnectionError, RateLimitError, APIStatusError))\n",
    ")\n",
    "def llm_standard(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=60),\n",
    "    stop=stop_after_attempt(max_attempt_number=5),\n",
    "    retry=retry_if_exception_type((APIConnectionError, RateLimitError, APIStatusError))\n",
    ")\n",
    "def llm_function_calling(system_prompt: str, query: str, text_search_tool: dict,  model='gpt-4o-mini'):\n",
    "    chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=chat_messages,\n",
    "        tools=[text_search_tool]\n",
    "    )\n",
    "\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22f2a6-7a74-4e1a-824a-8a206c86a09f",
   "metadata": {},
   "source": [
    "# Lets implement hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b27bf38-1e1c-4161-8e10-3b2af8c8a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully unpickled from vector_search_data.pkl\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"vector_search_data.pkl\"\n",
    "loaded_data = None\n",
    "try:\n",
    "    with open(FILE_PATH, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        print(f\"Data successfully unpickled from {FILE_PATH}\")\n",
    "\n",
    "    # Access the loaded data\n",
    "    loaded_embeddings = loaded_data['embeddings']\n",
    "    loaded_docs = loaded_data['documents']\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {FILE_PATH} was not found.\")\n",
    "except pickle.UnpicklingError as e:\n",
    "    print(f\"An error occurred during unpickling: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2473f13e-791b-4625-bd72-10b8743725aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch, Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844c0d96-c42a-49d6-8cd7-88de471a5778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x11fe6a990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(text_fields=['title', 'description', 'filename', 'section'],\n",
    "              keyword_fields=[])\n",
    "index.fit(loaded_docs)\n",
    "\n",
    "vs = VectorSearch()\n",
    "vs.fit(loaded_embeddings, loaded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3bc24a-9b20-46f8-b361-9bec0df68501",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I evaluate classification model results, and ensure numerical data is not drifted?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3a6fcf-27f3-4e51-a503-f9b99299c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results = index.search(query=query, num_results=5)\n",
    "q_v = embedding_model.encode(query)\n",
    "vector_results = vs.search(query_vector=q_v, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a69e956-006a-4cce-8f42-f89a18cf6e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Use Cases for Synthetic Test Inputs\\n\\nEvidently Cloud can be utilized for multiple purposes, including:\\n\\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\\n* **Regression Testing**: Validate changes in your AI system before deployment.\\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\\n\\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library.'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Example of Generating Test Inputs\\n\\nAn illustrative example of how to generate synthetic test inputs can be found in the accompanying GIF, depicting the data generation process.\\n\\n![](/images/synthetic/datagen_travel.gif)'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### RAG Dataset\\n\\nYou can generate a Q&A dataset from the knowledge source to enhance information retrieval and response accuracy. [Learn more](https://www.evidentlyai.com/synthetic-data/rag_data).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Introduction to Evidently Cloud\\n\\nEvidently Cloud is a platform that allows users to generate synthetic test inputs and outputs for evaluating AI systems. For more details on pricing, you can check [pricing](https://www.evidentlyai.com/pricing) and for a demo, please [reach out](https://www.evidentlyai.com/get-demo).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### Adversarial Tests\\n\\nCreate inputs specifically designed to test for vulnerabilities in your AI system. This is crucial for ensuring robustness against adversarial attacks. [Learn more](https://www.evidentlyai.com/synthetic-data/adversarial_data).'},\n",
       " {'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx',\n",
       "  'section': '## Use Cases\\n\\nYou can evaluate data drift in various scenarios:\\n\\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\\n\\n<Info>\\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\\n</Info>'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Dataset Drift\\n\\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Drift Detection Methods\\n\\n### Tabular Data\\n\\nMethods applicable to tabular data include:\\n\\n- **Kolmogorov–Smirnov (K-S) Test**: Default for numerical data (if ≤ 1000 objects).\\n- **Chi-Square Test**: Default for categorical data with > 2 labels (if ≤ 1000 objects).\\n- **Population Stability Index (PSI)**: Applies to numerical and categorical data.\\n\\nA full list of methods and their default thresholds is available.\\n\\n### Text Data\\n\\nText drift detection applies to columns with raw text data. Notable methods include:\\n\\n- **Text Content Drift**: Trained classifier model distinguishing between text in “current” and “reference” datasets with different thresholds based on the number of objects.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Custom Drift Detection Method\\n\\nIf a suitable drift detection method isn’t found, you can implement a custom function. Below is a basic example:\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\\n```\\nDefine the method and parameters to create a custom statistic test for drift detection.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "637c60f3-d165-404d-be99-7ef7de753d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e7b85aa-2b1e-4416-bf3c-e9e649306987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return vs.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        text_to_hash = result['filename'] + ' ' + result['section'][0:250]\n",
    "        encoded_string = text_to_hash.encode('utf-8')\n",
    "        hash_object = hashlib.sha256(encoded_string)\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d4ff51-c36c-459e-9d74-cd7c1c2307fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Use Cases for Synthetic Test Inputs\\n\\nEvidently Cloud can be utilized for multiple purposes, including:\\n\\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\\n* **Regression Testing**: Validate changes in your AI system before deployment.\\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\\n\\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library.'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Example of Generating Test Inputs\\n\\nAn illustrative example of how to generate synthetic test inputs can be found in the accompanying GIF, depicting the data generation process.\\n\\n![](/images/synthetic/datagen_travel.gif)'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### RAG Dataset\\n\\nYou can generate a Q&A dataset from the knowledge source to enhance information retrieval and response accuracy. [Learn more](https://www.evidentlyai.com/synthetic-data/rag_data).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '## Introduction to Evidently Cloud\\n\\nEvidently Cloud is a platform that allows users to generate synthetic test inputs and outputs for evaluating AI systems. For more details on pricing, you can check [pricing](https://www.evidentlyai.com/pricing) and for a demo, please [reach out](https://www.evidentlyai.com/get-demo).'},\n",
       " {'title': 'Synthetic data',\n",
       "  'description': 'Generating test cases and datasets.',\n",
       "  'filename': 'docs-main/synthetic-data/introduction.mdx',\n",
       "  'section': '### Adversarial Tests\\n\\nCreate inputs specifically designed to test for vulnerabilities in your AI system. This is crucial for ensuring robustness against adversarial attacks. [Learn more](https://www.evidentlyai.com/synthetic-data/adversarial_data).'},\n",
       " {'title': 'Data Drift',\n",
       "  'description': 'Overview of the Data Drift Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_drift.mdx',\n",
       "  'section': '## Use Cases\\n\\nYou can evaluate data drift in various scenarios:\\n\\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\\n\\n<Info>\\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\\n</Info>'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'},\n",
       " {'title': 'Data drift',\n",
       "  'description': 'How data drift detection works',\n",
       "  'filename': 'docs-main/metrics/explainer_drift.mdx',\n",
       "  'section': '## Dataset Drift\\n\\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Drift Detection Methods\\n\\n### Tabular Data\\n\\nMethods applicable to tabular data include:\\n\\n- **Kolmogorov–Smirnov (K-S) Test**: Default for numerical data (if ≤ 1000 objects).\\n- **Chi-Square Test**: Default for categorical data with > 2 labels (if ≤ 1000 objects).\\n- **Population Stability Index (PSI)**: Applies to numerical and categorical data.\\n\\nA full list of methods and their default thresholds is available.\\n\\n### Text Data\\n\\nText drift detection applies to columns with raw text data. Notable methods include:\\n\\n- **Text Content Drift**: Trained classifier model distinguishing between text in “current” and “reference” datasets with different thresholds based on the number of objects.'},\n",
       " {'title': 'Customize Data Drift',\n",
       "  'description': 'How to change data drift detection methods and conditions.',\n",
       "  'filename': 'docs-main/metrics/customize_data_drift.mdx',\n",
       "  'section': '## Custom Drift Detection Method\\n\\nIf a suitable drift detection method isn’t found, you can implement a custom function. Below is a basic example:\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\\n```\\nDefine the method and parameters to create a custom statistic test for drift detection.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07eed7f0-36e6-4ffe-9930-785e27fa11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully pickled and saved to vector_search_data.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = \"vector_search_data.pkl\"\n",
    "try:\n",
    "    with open(f'{file_path}', 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    print(f\"Data successfully pickled and saved to {file_path}\")\n",
    "except pickle.PicklingError as e:\n",
    "    print(f\"An error occurred during pickling: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"An I/O error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9973bb2-e431-4017-a708-441ef891d8bd",
   "metadata": {},
   "source": [
    "# Function calling and agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "195f1554-8fde-4222-9d89-0f45110ce006",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"hybrid_search\",\n",
    "    \"description\": \"Search the docs\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"User question to extract answer from the results\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14973f1e-4708-4fdd-9454-5ed8e1054c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for finding relevant information from the docs\n",
    "\n",
    "Use the search tool to find relevant information from the materials before answering questions.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed7c956f-ac5a-4f90-9867-5479dceb9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_function_calling(system_prompt=system_prompt,\n",
    "                     query=query,\n",
    "                     text_search_tool=text_search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb3b7081-963a-475c-b7c7-794a72d86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = hybrid_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12a6d652-d290-4bf1-9627-af8d1221ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0582194a-9990-4233-b8d2-fff856ea54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50026696-78f1-45aa-941c-06ce86b975a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b00e6122-769e-4a7b-b582-8bf1ce4a4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d783faa-1040-4c6f-b2ec-0cab024d5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str) -> List[Any]:\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        text_to_hash = result['filename'] + ' ' + result['section'][0:250]\n",
    "        encoded_string = text_to_hash.encode('utf-8')\n",
    "        hash_object = hashlib.sha256(encoded_string)\n",
    "        hex_digest = hash_object.hexdigest()\n",
    "        if hex_digest not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e80ca802-1e92-435b-bec5-8dd4ad07f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"hybrid_search\",\n",
    "    \"description\": \"Search the docs\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"User question to extract answer from the results\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c17a472-0302-4fa6-8ac1-3c11ebcc65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[hybrid_search],\n",
    "    model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c2b3871d-82a7-4db0-9756-389cb5759e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run(user_prompt=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ae9bf08-201d-4bf5-85e2-9e1d9274fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pydantic_ai.run.AgentRunResult'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "943aa227-75f5-4570-ba17-18c99679101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='How can I evaluate classification model results, and ensure numerical data is not drifted?', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 35, 295493, tzinfo=datetime.timezone.utc))], timestamp=datetime.datetime(2025, 12, 27, 3, 52, 35, 295762, tzinfo=datetime.timezone.utc), instructions='You are a helpful assistant for finding relevant information from the docs\\n\\nUse the search tool to find relevant information from the materials before answering questions.\\n\\nMake multiple searches if needed to provide comprehensive answers.', run_id='e0b84cd1-3bfc-4373-8a59-288ad84997ca'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='hybrid_search', args='{\"query\": \"evaluate classification model results\"}', tool_call_id='call_TOcbGjl3LuwyFqBFLIgZNUpI'), ToolCallPart(tool_name='hybrid_search', args='{\"query\": \"numerical data drift detection\"}', tool_call_id='call_3b0pLvq4yoNVD02GMbjf8WHj'), ToolCallPart(tool_name='hybrid_search', args='{\"query\": \"monitoring model performance\"}', tool_call_id='call_i1re2ytEvX1nYKvbT2e3cRkW')], usage=RequestUsage(input_tokens=92, output_tokens=71, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 36, 591569, tzinfo=datetime.timezone.utc), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'tool_calls', 'timestamp': datetime.datetime(2025, 12, 27, 3, 52, 35, tzinfo=TzInfo(0))}, provider_response_id='chatcmpl-CrFdzBNecwQVWXmpvxSEm9JqNk5nn', finish_reason='tool_call', run_id='e0b84cd1-3bfc-4373-8a59-288ad84997ca'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='hybrid_search', content=[{'title': 'Classification', 'description': 'Overview of the Classification Quality Preset', 'filename': 'docs-main/metrics/preset_classification.mdx', 'section': '## Overview\\n\\nThe `ClassificationPreset` allows you to evaluate and visualize the performance on classification tasks, whether binary or multi-class. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance or a different model/prompt).\\n\\n### Metrics and Visualizations\\n\\n* **Various metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\\n\\nAdditionally, if you include feature columns, the Report will show Classification Quality by column, reflecting how the system performs on different data segments.'}, {'title': 'Classification', 'description': 'Overview of the Classification Quality Preset', 'filename': 'docs-main/metrics/preset_classification.mdx', 'section': '## Test Suite Functionality\\n\\nIf you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\\n\\n### Test Generation\\n\\n* **Based on reference dataset**: If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\\n* **Based on heuristics**: If there is no reference, a dummy classification model will be created as a baseline and checks will be run against it.\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\\n</Info>'}, {'title': 'Classification', 'description': 'Overview of the Classification Quality Preset', 'filename': 'docs-main/metrics/preset_classification.mdx', 'section': '## Use Cases\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**: Compare predictive system performance across different datasets, such as during A/B testing or when experimenting with different prompt versions and configurations.\\n* **Production monitoring**: Run evaluations whenever you get true labels in production to visualize performance and decide on model updates or retraining.\\n* **Debugging**: If performance drops, use the visual Report for diagnostics, exploring model errors and underperforming segments.'}, {'title': 'Classification', 'description': 'Overview of the Classification Quality Preset', 'filename': 'docs-main/metrics/preset_classification.mdx', 'section': '## Test Suite\\n\\nTo add pass/fail classification quality Tests, auto-generated from the `ref` dataset:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset(),\\n],\\ninclude_tests=True)\\n\\nmy_eval = report.run(current, ref)\\n```'}, {'title': 'Classification', 'description': 'Overview of the Classification Quality Preset', 'filename': 'docs-main/metrics/preset_classification.mdx', 'section': '## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**: Modify auto-generated conditions, either relative to the reference or any custom conditions.\\n* **Modify Report composition**: Add additional metrics, such as column Correlations, Missing Values, or Data Drift. Adding `ValueDrift(\"target\")` can help evaluate if there is a statistical distribution shift in the model target (concept drift).\\n\\n<Info>\\n  **Creating a custom Report**: Check the documentation for creating a [custom Report](/docs/library/report) and modifying [Tests](/docs/library/tests) conditions.\\n</Info>'}, {'title': 'All Metrics', 'description': 'Reference page for all dataset-level evals.', 'filename': 'docs-main/metrics/all_metrics.mdx', 'section': '- |\\n  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |\\n  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |\\n  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |\\n  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |\\n</Accordion>\\n\\n### By Label\\n\\nUse when you have multiple classes and want to evaluate quality separately.\\n\\n| Metric                             | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |\\n|'}, {'title': 'Regression metrics', 'description': 'Open-source regression quality metrics.', 'noindex': 'true', 'filename': 'docs-main/metrics/explainer_regression.mdx', 'section': '## Metrics Output\\n\\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations.'}, {'title': 'Overview', 'description': 'All available Presets.', 'filename': 'docs-main/metrics/all_presets.mdx', 'section': '## Classification Evaluation\\n\\n### Description\\nQuality evaluation for classification tasks.\\n\\n### Link\\n[View Classification](https://example.com/metrics/preset_classification)'}, {'title': 'Classification metrics', 'description': 'Open-source classification metrics.', 'noindex': 'true', 'filename': 'docs-main/metrics/explainer_classification.mdx', 'section': '## Model Quality Summary Metrics\\n\\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \\n\\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**'}, {'title': 'All Metrics', 'description': 'Reference page for all dataset-level evals.', 'filename': 'docs-main/metrics/all_metrics.mdx', 'section': '## Classification\\n\\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\\n\\n### Additional Info\\n\\n[Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.\\n\\n### General\\n\\nUse for binary classification and aggregated results for multi-class.\\n\\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\\n|'}], tool_call_id='call_TOcbGjl3LuwyFqBFLIgZNUpI', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 36, 663541, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='hybrid_search', content=[{'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Resources\\n\\nTo enhance your understanding of different tests for various use cases, consider the following blogs:\\n\\n- [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n- [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n- [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n- [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\n- [\"My data drifted. What\\'s next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\n- [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Data Drift Detection Overview\\n\\nIn some tests and metrics, Evidently uses the default Data Drift Detection algorithm to detect distribution drift in individual columns (features, prediction, or target). This section describes how the **default** algorithm works.\\n\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\n\\n<Info>\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\n</Info>'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## How It Works\\n\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Several statistical tests and drift detection methods are applied to determine if the distribution has changed significantly, returning a \"drift detected\" or \"not detected\" result.\\n\\nThe logic for choosing the appropriate drift test is based on:\\n\\n- Column type: categorical, numerical, text data\\n- Number of observations in the reference dataset\\n- Number of unique values in the column (n_unique)\\n\\nAdditionally, you can set a rule to detect dataset-level drift based on the number of drifted columns.'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Dataset Drift\\n\\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared.'}, {'title': 'Overview', 'description': 'All available Presets.', 'filename': 'docs-main/metrics/all_presets.mdx', 'section': '## Data Drift Detection\\n\\n### Description\\nData distribution drift detection.\\n\\n### Link\\n[View Data Drift](https://example.com/metrics/preset_data_drift)'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Tabular Data Drift\\n\\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\\n\\n**For small data (<= 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\\n\\n<Info>\\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\\n</Info>\\n\\n**For larger data (> 1000 observations)**:\\n\\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\n<Info>\\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\\n</Info>\\n\\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).'}, {'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'docs-main/metrics/explainer_drift.mdx', 'section': '## Data Drift Detection Overview\\n\\nIn some tests and metrics, Evidently uses the default Data Drift Detection algorithm to detect distribution drift in individual columns (features, prediction, or target). This section describes how the **default** algorithm works.\\n\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\n\\n<Info>\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\n</Info>'}, {'title': 'Customize Data Drift', 'description': 'How to change data drift detection methods and conditions.', 'filename': 'docs-main/metrics/customize_data_drift.mdx', 'section': '## Data Drift Detection Overview\\n\\nAll Metrics and Presets that evaluate shifts in data distributions use the default Data Drift algorithm. It automatically selects the drift detection method based on the column type (text, categorical, numerical) and volume. You can override the defaults by passing a custom parameter to the chosen Metric or Preset and modify the drift detection method, thresholds, or both. Fully custom drift detection methods can also be implemented.'}, {'title': 'Customize Data Drift', 'description': 'How to change data drift detection methods and conditions.', 'filename': 'docs-main/metrics/customize_data_drift.mdx', 'section': '## Custom Drift Detection Method\\n\\nIf a suitable drift detection method isn’t found, you can implement a custom function. Below is a basic example:\\n```python\\nimport pandas as pd\\nfrom scipy.stats import anderson_ksamp\\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\\n```\\nDefine the method and parameters to create a custom statistic test for drift detection.'}], tool_call_id='call_3b0pLvq4yoNVD02GMbjf8WHj', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 36, 664330, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='hybrid_search', content=[{'title': 'Batch monitoring', 'description': 'How to run batch evaluation jobs.', 'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx', 'section': '## Overview of Batch Monitoring\\n\\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview).'}, {'title': 'Dashboard panel types [Legacy]', 'description': 'Overview of the available monitoring Panels.', 'filename': 'docs-main/docs/platform/dashboard_panel_types.mdx', 'section': '## Overview of Monitoring Panels\\n\\nThis section introduces monitoring panels in versions 0.6.0 to 0.7.1 for Cloud/Workspace v1.\\n\\nA monitoring panel is an individual plot or counter on the Monitoring Dashboard. You can customize panel types, values shown, titles, and legends. Multiple panels can be organized by tabs.'}, {'title': 'Batch monitoring', 'description': 'How to run batch evaluation jobs.', 'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx', 'section': '## Running Tests vs Reports\\n\\nStructuring your evaluations as Tests, rather than monitoring a multitude of metrics simultaneously, can help alleviate alert fatigue and streamline configuration. This method simplifies the evaluation of multiple conditions at once, such as quickly verifying that all columns in the input data fall within a defined min-max range.'}, {'title': 'Batch monitoring', 'description': 'How to run batch evaluation jobs.', 'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx', 'section': '## Workflow Steps\\n\\nThe complete workflow consists of the following steps:\\n\\n### Step 1: Configure the Metrics\\n\\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\\n\\n### Step 2: Run the Evaluations\\n\\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\\n\\n- First, perform data quality, data drift, and prediction drift checks.\\n- After receiving delayed labels, run ML quality check results.\\n\\nYou can backdate your Reports with a custom timestamp.\\n\\n### Step 3: Upload to the Platform\\n\\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\\n\\n### Step 4: Configure the Dashboard\\n\\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\\n\\n### Step 5: Configure Alerts\\n\\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts).'}, {'title': 'Batch monitoring', 'description': 'How to run batch evaluation jobs.', 'filename': 'docs-main/docs/platform/monitoring_local_batch.mdx', 'section': '## Simple Example\\n\\nTo get the dataset stats for a single batch and upload to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\\n\\n### Sample Code\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DatasetStats()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\nws.add_run(project.id, my_eval, include_data=False)\\n```'}, {'title': 'Classification metrics', 'description': 'Open-source classification metrics.', 'noindex': 'true', 'filename': 'docs-main/metrics/explainer_classification.mdx', 'section': '## Model Quality Summary Metrics\\n\\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \\n\\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**'}, {'title': 'RAG evals', 'description': 'Metrics to evaluate a RAG system.', 'filename': 'docs-main/examples/llm_rag_evals.mdx', 'section': \"## What's Next?\\n\\nConsider implementing [regression testing](/examples/LLM_regression_testing) to monitor how your RAG system's retrieval and response quality change over time.\"}, {'title': 'Introduction', 'description': 'Evidently Platform at a glance.', 'filename': 'docs-main/docs/platform/overview.mdx', 'section': '## Monitoring\\n\\nRun evaluations for live systems in either batch or real-time. Create a dashboard to track results and connect back to raw data as needed. Set alerts for violations.'}, {'title': 'Overview', 'description': 'How production AI quality monitoring works.', 'filename': 'docs-main/docs/platform/monitoring_overview.mdx', 'section': '## Batch Monitoring Jobs\\n\\n<Check>\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don’t need instant quality evaluations.\\n\\n![](/images/monitoring_flow_batch.png)\\n\\n**How it works**:\\n\\n* **Build your evaluation pipeline**: Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow, running at regular intervals or triggered by new data.\\n\\n* **Run metric calculations**: Implement the evaluation step using the Evidently Python library. Select evaluations and compute the `Reports` summarizing data, metrics, and test results.\\n\\n* **Store and visualize the results**: Store Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\\n\\n**Benefits of this approach**:\\n\\n* **Decouples log storage and monitoring metrics**: This setup does not store raw data or model predictions unless chosen. It retains aggregated data summaries and test results, protecting data privacy.\\n\\n* **Full control over the evaluation pipeline**: You decide when evaluations occur, making it great for batch ML models. You can log predictions and set up separate monitoring jobs for queries.\\n\\n* **Fits most ML evaluation scenarios**: Many evaluations, such as data drift detection, work in batches, requiring a set of new data points. Analyzing shifts in predictions or user behavior is also more meaningful at intervals.\\n\\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 5. Get a Dashboard\\n\\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You\\'ll be able to observe trends and set up alerts as you log more evaluation results.'}], tool_call_id='call_i1re2ytEvX1nYKvbT2e3cRkW', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 36, 668738, tzinfo=datetime.timezone.utc))], timestamp=datetime.datetime(2025, 12, 27, 3, 52, 36, 669037, tzinfo=datetime.timezone.utc), instructions='You are a helpful assistant for finding relevant information from the docs\\n\\nUse the search tool to find relevant information from the materials before answering questions.\\n\\nMake multiple searches if needed to provide comprehensive answers.', run_id='e0b84cd1-3bfc-4373-8a59-288ad84997ca'),\n",
       " ModelResponse(parts=[TextPart(content=\"To evaluate classification model results and detect drift in numerical data, you can follow the guidelines below:\\n\\n### **Evaluating Classification Model Results**\\n\\n1. **Use Performance Metrics**: Evaluate your model using various metrics such as:\\n   - **Accuracy**: The ratio of correctly predicted instances to the total instances.\\n   - **Precision**: The ratio of true positive predictions to the total predicted positives.\\n   - **Recall**: The ratio of true positive predictions to the total actual positives.\\n   - **F1-score**: The harmonic mean of precision and recall, useful for imbalanced datasets.\\n   - **ROC AUC**: Measures the area under the receiver operating characteristic curve, indicating model ability to distinguish between classes.\\n   - **Log Loss**: Measures the performance of a classification model where the predicted output is a probability value between 0 and 1.\\n\\n2. **Visualizations**: Generate visualizations to better understand the model's performance:\\n   - **Confusion Matrix**: Displays true vs predicted classifications to evaluate performance.\\n   - **Class Separation Quality**: Helps visualize how well classes are separated.\\n   - **Probability Distributions**: To evaluate how different classes are weighted and predicted.\\n   - **ROC Curve**: To visualize true positive rates against false positive rates.\\n\\n3. **Test Suite**: Implement a test suite to assess model performance. This includes:\\n   - **Automatic Checks**: Run checks against a reference dataset to ensure metrics stay within defined bounds.\\n   - **Custom Conditions**: Modify conditions based on domain knowledge or specific requirements.\\n\\n4. **Production Monitoring**: Regularly run evaluations when you receive true labels in production to adjust or retrain your model as needed.\\n\\n5. **Debugging**: Use the generated reports to identify underperforming segments or errors in predictions.\\n\\n#### **Reference Materials**:\\n- [Overview of Classification Quality Preset](https://example.com/metrics/preset_classification)\\n- [Model Quality Summary Metrics](https://example.com/metrics/explainer_classification)\\n\\n### **Detecting Numerical Data Drift**\\n\\n1. **Data Drift Detection Overview**: Data drift refers to changes in the statistical properties of the data over time. To detect drift:\\n   - Use statistical tests tailored for different data types: \\n     - For numerical columns with unique values > 5, use the **Kolmogorov-Smirnov test** for small datasets (<= 1000 observations) or **Wasserstein Distance** for larger datasets (> 1000).\\n     - For categorical columns or numerical columns with unique values <= 5, apply the **chi-squared test** for small data and **Jensen-Shannon divergence** for larger datasets.\\n\\n2. **Threshold Definitions**: Define appropriate thresholds to ascertain when drift is significant:\\n   - For the p-value from tests, a value <= 0.05 indicates drift.\\n   - For distance or divergence, a score >= 0.1 indicates drift.\\n\\n3. **Monitoring Customization**: You can customize drift detection methods by selecting different algorithms, adjusting thresholds, or implementing custom drift tests.\\n\\n4. **Dataset-level Drift Assessment**: Set a rule for dataset-level drift based on the individual drift status of columns (e.g., if a certain percentage of columns show drift, flag the dataset as drifted).\\n\\n#### **Reference Materials**:\\n- [How Data Drift Detection Works](https://example.com/metrics/explainer_drift)\\n- [Customize Data Drift Detection](https://example.com/metrics/customize_data_drift)\\n\\n### Conclusion\\nBy combining robust evaluation metrics and ongoing monitoring for data drift, you can maintain the integrity and performance of your classification models over time.\")], usage=RequestUsage(input_tokens=5171, output_tokens=747, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 12, 27, 3, 52, 48, 537021, tzinfo=datetime.timezone.utc), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'stop', 'timestamp': datetime.datetime(2025, 12, 27, 3, 52, 37, tzinfo=TzInfo(0))}, provider_response_id='chatcmpl-CrFe1T7zXcpNL8XUDEXAMkv1POTrd', finish_reason='stop', run_id='e0b84cd1-3bfc-4373-8a59-288ad84997ca')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
