{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "Why is it essential to evaluate machine learning models in production environments?", "timestamp": "2025-12-28T08:02:58.055358Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:02:58.055766Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "10cba1c8-ccbd-4a47-96a4-97eab2bc3f72", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluate machine learning models in production environments\"}", "tool_call_id": "call_rX61yzIwuHXeYPFSooWwLhHU", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 169, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:02:58.853303Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:02:58Z"}, "provider_response_id": "chatcmpl-Crg1qKFcjv1oYMLWsWdQLSKZRmzOD", "finish_reason": "tool_call", "run_id": "10cba1c8-ccbd-4a47-96a4-97eab2bc3f72", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Sample Models\n\nHere are some models you can call using the `HuggingFace()` descriptor.\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Built-in ML Evaluators\n\n<Tip>\n**Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\n</Tip>\n\nThere are built-in evaluators for some models. You can call them like any other descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \n])\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Imports\n\nTo use the HuggingFace models for text scoring, you need to import the necessary libraries:\n\n```python\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Custom ML Evaluators\n\n<Info>\nYou can also add any custom checks [directly as a Python function](/metrics/customize_descriptor).\n</Info>\n\nAlternatively, use the general `HuggingFace()` descriptor to call a specific named model. The model you use must return a numerical score or a category for each text in a column.\n\nFor example, to evaluate \"curiosity\" expressed in a text:\n\n```python\neval_df.add_descriptors(descriptors=[\n   HuggingFace(\"question\",\n       model=\"SamLowe/roberta-base-go_emotions\", \n       params={\"label\": \"curiosity\"},\n       alias=\"Curiosity\"\n   )\n])\n```\n\nCall the result as usual:\n\n```python\neval_df.as_dataframe()\n```"}, {"title": "Overview", "description": "Running evals on the platform.", "filename": "docs-main/docs/platform/evals_overview.mdx", "section": "## Evaluations in AI Product Development\n\nYou may need evaluations at different stages of your AI product development, including:\n\n* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs.\n* **Experiments.** Test different parameters, models, or prompts and compare outcomes.\n* **Safety and adversarial testing.** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\n* **Regression testing.** Ensure performance does not degrade after updates or fixes.\n* **Monitoring.** Track the response quality for production systems.\n\nEvidently supports all these workflows, allowing evaluations to be run locally or directly on the platform."}, {"title": "Data and ML checks", "description": "Run a simple evaluation for tabular data", "filename": "docs-main/quickstart_ml.mdx", "section": "## Overview of Evidently\n\nEvidently helps you run tests and evaluations for your production ML systems. This includes:\n- Evaluating prediction quality (e.g. classification or regression accuracy)\n- Input data quality (e.g. missing values, out-of-range features)\n- Data and prediction drift."}, {"title": "Overview", "description": "Running evals on the platform.", "filename": "docs-main/docs/platform/evals_overview.mdx", "section": "## Evaluations via API\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\nThis option is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.\n\n**How it works**:\n\n* Run Python-based evaluations on your AI outputs by generating Reports.\n* Upload results to the Evidently Platform.\n* Use the Explore feature to compare and debug results between runs.\n\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm)."}, {"title": "Data Drift", "description": "Overview of the Data Drift Preset.", "filename": "docs-main/metrics/preset_data_drift.mdx", "section": "## Use Cases\n\nYou can evaluate data drift in various scenarios:\n\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\n\n<Info>\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\n</Info>"}, {"title": "Overview", "description": "How production AI quality monitoring works.", "filename": "docs-main/docs/platform/monitoring_overview.mdx", "section": "## Batch Monitoring Jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**: Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow, running at regular intervals or triggered by new data.\n\n* **Run metric calculations**: Implement the evaluation step using the Evidently Python library. Select evaluations and compute the `Reports` summarizing data, metrics, and test results.\n\n* **Store and visualize the results**: Store Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**: This setup does not store raw data or model predictions unless chosen. It retains aggregated data summaries and test results, protecting data privacy.\n\n* **Full control over the evaluation pipeline**: You decide when evaluations occur, making it great for batch ML models. You can log predictions and set up separate monitoring jobs for queries.\n\n* **Fits most ML evaluation scenarios**: Many evaluations, such as data drift detection, work in batches, requiring a set of new data points. Analyzing shifts in predictions or user behavior is also more meaningful at intervals.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch)."}], "tool_call_id": "call_rX61yzIwuHXeYPFSooWwLhHU", "metadata": null, "timestamp": "2025-12-28T08:02:59.055016Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:02:59.055266Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "10cba1c8-ccbd-4a47-96a4-97eab2bc3f72", "metadata": null}, {"parts": [{"content": "Evaluating machine learning models in production environments is essential for several reasons:\n\n1. **Performance Monitoring**: Continuous monitoring of model performance is crucial to ensure that models maintain their accuracy and effectiveness over time. Models may degrade due to changes in input data patterns, a phenomenon known as data drift. Regular evaluations help identify these issues early [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\n2. **Quality Assurance**: Evaluation processes help in ensuring the quality of predictions. This includes checking for accuracy in classification or regression outputs and assessing the quality of input data, such as the presence of missing values or out-of-range features [source](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_ml.mdx).\n\n3. **Regression Testing**: When models are updated or modified, it is vital to conduct regression testing to verify that the model's performance has not degraded due to these changes. This testing helps ensure stability and reliability in environments where models frequently undergo iterations [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\n4. **Safety and Adversarial Testing**: Evaluations also encompass testing how models respond to edge cases and adversarial inputs. This aspect is critical to ensuring that the model behaves as expected in challenging or unforeseen scenarios [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\n5. **Decision-Making for Retraining**: Evaluating model performance can guide decisions on whether retraining is necessary. If evaluations indicate data drift, it might indicate that the model needs to be updated with new data to maintain accuracy [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_data_drift.mdx).\n\nIn conclusion, regular evaluation of machine learning models in production is vital for maintaining their performance, ensuring quality, and making informed decisions regarding model updates and retraining strategies.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1944, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 411, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:07.743956Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:02:59Z"}, "provider_response_id": "chatcmpl-Crg1rrz2937BI8ExWOaTAGxhhCYAw", "finish_reason": "stop", "run_id": "10cba1c8-ccbd-4a47-96a4-97eab2bc3f72", "metadata": null}], "source": "ai-generated"}