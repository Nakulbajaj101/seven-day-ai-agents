{"agent_name": "faq_agent", "system_prompt": ["\nYou are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.\n"], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How can I evaluate classification model results, and ensure numerical data is not drifted?", "timestamp": "2025-12-28T02:00:00.395659Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T02:00:00.395948Z", "instructions": "You are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "2e9ebe59-9f1f-4619-99eb-9dd0a2a6ae8e", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\": \"evaluate classification model results\"}", "tool_call_id": "call_zoT9MkbtO8CfnJu1CIeMHLoe", "id": null, "provider_details": null, "part_kind": "tool-call"}, {"tool_name": "hybrid_search", "args": "{\"query\": \"ensure numerical data is not drifted\"}", "tool_call_id": "call_wMgxHDRrGKr32lYTrFiFKTxQ", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 118, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 55, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T02:00:02.938241Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T02:00:01Z"}, "provider_response_id": "chatcmpl-CraMbrZ5xYku0WSwZFKmaW2tuVVba", "finish_reason": "tool_call", "run_id": "2e9ebe59-9f1f-4619-99eb-9dd0a2a6ae8e", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Overview\n\nThe `ClassificationPreset` allows you to evaluate and visualize the performance on classification tasks, whether binary or multi-class. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance or a different model/prompt).\n\n### Metrics and Visualizations\n\n* **Various metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\n\nAdditionally, if you include feature columns, the Report will show Classification Quality by column, reflecting how the system performs on different data segments."}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Test Suite Functionality\n\nIf you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds.\n\n### Test Generation\n\n* **Based on reference dataset**: If the reference dataset is provided, conditions like expected prediction accuracy will be derived from it.\n* **Based on heuristics**: If there is no reference, a dummy classification model will be created as a baseline and checks will be run against it.\n\n<Info>\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\n</Info>"}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Use Cases\n\nThese Presets are useful in various scenarios:\n\n* **Model / system comparison**: Compare predictive system performance across different datasets, such as during A/B testing or when experimenting with different prompt versions and configurations.\n* **Production monitoring**: Run evaluations whenever you get true labels in production to visualize performance and decide on model updates or retraining.\n* **Debugging**: If performance drops, use the visual Report for diagnostics, exploring model errors and underperforming segments."}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Test Suite\n\nTo add pass/fail classification quality Tests, auto-generated from the `ref` dataset:\n\n```python\nreport = Report([\n    ClassificationPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Report Customization\n\nYou can customize the Report in several ways:\n\n* **Change Test conditions**: Modify auto-generated conditions, either relative to the reference or any custom conditions.\n* **Modify Report composition**: Add additional metrics, such as column Correlations, Missing Values, or Data Drift. Adding `ValueDrift(\"target\")` can help evaluate if there is a statistical distribution shift in the model target (concept drift).\n\n<Info>\n  **Creating a custom Report**: Check the documentation for creating a [custom Report](/docs/library/report) and modifying [Tests](/docs/library/tests) conditions.\n</Info>"}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "- |\n  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |\n  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |\n  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |\n  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |\n</Accordion>\n\n### By Label\n\nUse when you have multiple classes and want to evaluate quality separately.\n\n| Metric                             | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |\n|"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "## Classification\n\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\n\n### Additional Info\n\n[Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.\n\n### General\n\nUse for binary classification and aggregated results for multi-class.\n\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\n|"}], "tool_call_id": "call_zoT9MkbtO8CfnJu1CIeMHLoe", "metadata": null, "timestamp": "2025-12-28T02:00:03.207937Z", "part_kind": "tool-return"}, {"tool_name": "hybrid_search", "content": [{"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Creating Test Datasets\n\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don\u2019t reflect your specific use case. Manually writing test cases takes time and effort."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Benefits of Synthetic Data\n\nSynthetic data is not a replacement for real data or expert-designed tests \u2014 it\u2019s a way to add variety and speed up the process. With synthetic data you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses.\n\nIt\u2019s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "## Use Cases for Synthetic Test Inputs\n\nEvidently Cloud can be utilized for multiple purposes, including:\n\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\n* **Regression Testing**: Validate changes in your AI system before deployment.\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\n\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "### Adversarial Tests\n\nCreate inputs specifically designed to test for vulnerabilities in your AI system. This is crucial for ensuring robustness against adversarial attacks. [Learn more](https://www.evidentlyai.com/synthetic-data/adversarial_data)."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Uses of Test Datasets\n\nYou can use this test dataset to:\n\n* Run **experiments** and track if changes improve or degrade system performance.\n* Run **regression testing** to ensure updates don\u2019t break what was already working.\n* **Stress-test** your system with complex or adversarial inputs to check its resilience."}, {"title": "Data drift", "description": "How data drift detection works", "filename": "docs-main/metrics/explainer_drift.mdx", "section": "## Dataset Drift\n\nWith presets like `DatasetDriftPreset()` and metrics like `DriftedColumnsCount()`, you can define a rule for dataset-level drift based on individual column drift results. For example, if 50% of features drifted, dataset drift can be declared."}, {"title": "Customize Data Drift", "description": "How to change data drift detection methods and conditions.", "filename": "docs-main/metrics/customize_data_drift.mdx", "section": "## Custom Drift Detection Method\n\nIf a suitable drift detection method isn\u2019t found, you can implement a custom function. Below is a basic example:\n```python\nimport pandas as pd\nfrom scipy.stats import anderson_ksamp\nfrom evidently import Dataset, DataDefinition, Report, ColumnType\nfrom evidently.metrics import ValueDrift, DriftedColumnsCount\nfrom evidently.legacy.calculations.stattests import register_stattest, StatTest\n```\nDefine the method and parameters to create a custom statistic test for drift detection."}, {"title": "Data drift", "description": "How data drift detection works", "filename": "docs-main/metrics/explainer_drift.mdx", "section": "## Data Drift Detection Overview\n\nIn some tests and metrics, Evidently uses the default Data Drift Detection algorithm to detect distribution drift in individual columns (features, prediction, or target). This section describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>"}, {"title": "Data drift", "description": "How data drift detection works", "filename": "docs-main/metrics/explainer_drift.mdx", "section": "## Tabular Data Drift\n\nDefault methods for tabular data are provided depending on the number of observations in the reference dataset:\n\n**For small data (<= 1000 observations)**:\n\n- Numerical columns (n_unique > 5): Utilize the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\n- Categorical columns or numerical columns with n_unique <= 5: Employ a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\n- Binary categorical features (n_unique <= 2): Apply a proportion difference test for independent samples based on Z-score.\n\n<Info>\nAll tests use a 0.95 confidence level by default. Drift score is P-value. (<= 0.05 indicates drift).\n</Info>\n\n**For larger data (> 1000 observations)**:\n\n- Numerical columns (n_unique > 5): Use [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\n- Categorical columns or numerical with n_unique <= 5: Apply [Jensen\u2013Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\n\n<Info>\nAll metrics use a threshold = 0.1 by default. Drift score is distance/divergence (>= 0.1 indicates drift).\n</Info>\n\nYou can customize this drift detection logic by selecting any available method, specifying thresholds, or passing a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift)."}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "## Data Drift\n\nUse to detect distribution drift for text and tabular data or over computed text descriptors. Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).\n\n### Additional Info\n\n[Data definition](/docs/library/data_definition). You may need to map column types, ID and timestamp.  \n[Metrics explainers](/metrics/explainer_drift). Understand how data drift works.\n\n| Metric                                | Description                                                                                                                                                                                                                                                                                                                                | Parameters                                                                                                                                                                                                                                                                             | Test Defaults                                                                                                                            |\n|"}], "tool_call_id": "call_wMgxHDRrGKr32lYTrFiFKTxQ", "metadata": null, "timestamp": "2025-12-28T02:00:03.207998Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T02:00:03.209700Z", "instructions": "You are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "2e9ebe59-9f1f-4619-99eb-9dd0a2a6ae8e", "metadata": null}, {"parts": [{"content": "To evaluate classification model results and ensure numerical data is not drifted, you can follow these comprehensive steps outlined in the Evidently AI documentation:\n\n### Evaluating Classification Model Results\n1. **Use the Classification Quality Preset**: Utilize the `ClassificationPreset` to evaluate and visualize the performance of your classification tasks. This can be done for a single dataset or by comparing it against a reference dataset.\n   \n2. **Metrics and Visualizations**:\n   - **Metrics**: You can measure various metrics including Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss.\n   - **Visualizations**: Generate visual aids like Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, and PR Curve.\n   - If you include feature columns in the report, it will also show performance by column to assess different data segments.\n\n3. **Testing and Diagnostics**:\n   - Enable tests to automatically run checks that assess if model performance metrics are within acceptable bounds.\n   - Test conditions can be derived from a reference dataset, or heuristically using a dummy classification model as a baseline for comparison.\n   - Utilize reports generated to explore model errors and underperforming segments when there's a drop in performance.\n\n4. **Customization**: You can customize the report by modifying test conditions or adding additional metrics like correlation, missing values, or data drift (for concept drift evaluation).\n\n### Ensuring Numerical Data is Not Drifted\n1. **Data Drift Detection**: You should utilize methods for detecting distribution drift in numerical columns. Evidently provides different algorithms based on the size of the dataset:\n   - For smaller datasets (<= 1000 observations), use tests like the Kolmogorov-Smirnov test for numerical columns with many unique values, and Chi-squared tests for categorical columns or numerical columns with limited unique values.\n   - For larger datasets, methods such as Wasserstein Distance and Jensen-Shannon divergence can be applied.\n\n2. **Configuration of Drift Detection**: \n   - The default methods and thresholds can be adjusted, allowing for a custom statistic test for drift detection.\n   - Metrics like `ValueDrift`, `DriftedColumnsCount`, and `DataDriftPreset` can be employed to track drift over specific individual columns or overall datasets.\n\n3. **Monitoring**: Regularly run these drift detection evaluations whenever new data is obtained in production. Continuous monitoring helps in promptly addressing potential drifts.\n\nFollowing these guidelines will help ensure that your classification models are evaluated effectively and that any numerical data drift is detected and managed appropriately.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 3204, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 521, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T02:00:17.110796Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T02:00:05Z"}, "provider_response_id": "chatcmpl-CraMfH3UeiUpg0gl1VRar7qrO3xE5", "finish_reason": "stop", "run_id": "2e9ebe59-9f1f-4619-99eb-9dd0a2a6ae8e", "metadata": null}], "source": "user"}