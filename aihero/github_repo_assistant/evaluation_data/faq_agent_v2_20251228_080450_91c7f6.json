{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "Why is regression testing necessary when monitoring machine learning models in production?", "timestamp": "2025-12-28T08:04:42.986979Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:42.987093Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "fc997f4c-dbb5-4cae-912c-715840968aa8", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"regression testing machine learning models production\"}", "tool_call_id": "call_ye2h08bqbSdoBdJHtvy2rPBs", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 169, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:44.208762Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:43Z"}, "provider_response_id": "chatcmpl-Crg3XSBy1gegQvGfvOlGhRsaT9DeH", "finish_reason": "tool_call", "run_id": "fc997f4c-dbb5-4cae-912c-715840968aa8", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Introduction\n\nIn this tutorial, you will learn how to perform regression testing for LLM (Large Language Model) outputs, comparing new and old responses to identify significant changes after system updates."}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Create a Project\n\n1. **Connect to Evidently Cloud**:\n   ```python\n   ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n   ```\n2. **Create a Project**:\n   ```python\n   project = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\n   project.description = \"My project description\"\n   project.save()\n   ```"}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Prepare the Dataset\n\n1. **Create a toy dataset**:\n   ```python\n   data = [\n       [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\n       ...\n   ]\n   columns = [\"question\", \"target_response\"]\n   ref_data = pd.DataFrame(data, columns=columns)\n   ```\n\n2. **Preview the dataset**:\n   ```python\n   pd.set_option('display.max_colwidth', None)\n   ref_data.head()\n   ```\n\n3. **Optional Data Exploration**: Check text length and sentence count distribution:\n   ```python\n   ref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data), ...)\n   ref_dataset.as_dataframe()\n   ```"}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Tutorial Scope\n\nHere's what we'll cover:\n\n- **Create a toy dataset**: Build a small Q&A dataset with answers and reference responses.\n- **Get new answers**: Simulate generating new answers.\n- **Create and run a Report with Tests**: Compare answers using LLM-as-a-judge.\n- **Build a monitoring Dashboard**: Track test results over time."}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Get New Answers\n\n1. **Generate new responses**:\n   ```python\n   data = [\n       [\"Why is the sky blue?\", ...],\n       ...\n   ]\n   columns = [\"question\", \"target_response\", \"response\"]\n   eval_data = pd.DataFrame(data, columns=columns)\n   ```"}, {"title": "Regression", "description": "Overview of the Regression Quality Preset", "filename": "docs-main/metrics/preset_regression.mdx", "section": "## Overview of RegressionPreset\n\nThe `RegressionPreset` allows you to evaluate and visualize the performance on regression tasks. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance, or a different model/prompt).\n\nThe Report includes:\n\n* **Various metrics**: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), etc.\n* **Various visualizations**: Actual vs Predicted Plot, Error Distribution, Error Normality, etc.\n\nIf you enable Tests, this will automatically run checks to assess if the model performance metrics are within bounds. Tests are auto-generated based on:\n\n* **Reference dataset**: If provided, conditions like expected prediction accuracy will be derived from it.\n* **Heuristics**: If no reference is available, a dummy regression model will be created as a baseline for checks.\n\n<Info>\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\n</Info>"}, {"title": "Regression", "description": "Overview of the Regression Quality Preset", "filename": "docs-main/metrics/preset_regression.mdx", "section": "## Use Cases for RegressionPreset\n\nThese Presets are useful in various scenarios:\n\n* **Model/System Comparison**: Compare predictive system performance across different datasets, such as during A/B testing or when experimenting with model configurations and architectures.\n* **Production Monitoring**: Run evaluations whenever you receive actual values in production to communicate and visualize performance, and decide on model updates or retraining.\n* **Debugging**: If you notice a drop in performance, use the visual Report to check error distributions and explore model errors."}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "## Regression\n\nUse to evaluate the quality of a regression model.\n\n### Additional Info\n\n[Data definition](/docs/library/data_definition). You may need to map prediction and target columns.\n\n| Metric                | Description                                                                                                                                                                                                    | Parameters                                                                                                                                                                             | Test Defaults                                                                                                                                                                                   |\n|"}, {"title": "Introduction", "description": "Evidently Platform at a glance.", "filename": "docs-main/docs/platform/overview.mdx", "section": "## Regression Testing\n\nCombine evaluations into conditional Test Suites with Pass/Fail outcomes. Set alerts for failed tests and track results over time using the built-in dashboard."}, {"title": "Regression", "description": "Overview of the Regression Quality Preset", "filename": "docs-main/metrics/preset_regression.mdx", "section": "## Test Suite for Regression Quality\n\nTo add pass/fail regression quality Tests, auto-generated from the `ref` dataset:\n\n```python\nreport = Report([\n    RegressionPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"}], "tool_call_id": "call_ye2h08bqbSdoBdJHtvy2rPBs", "metadata": null, "timestamp": "2025-12-28T08:04:44.257217Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:44.259057Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "fc997f4c-dbb5-4cae-912c-715840968aa8", "metadata": null}, {"parts": [{"content": "Regression testing is a crucial step in monitoring machine learning models in production for several reasons:\n\n1. **Performance Consistency**: When models are updated, whether through new training data or algorithm changes, regression testing ensures that the performance of the model does not degrade. This is particularly important as small changes can lead to significant differences in outcomes.\n\n2. **Change Detection**: It allows teams to check for changes in model outputs against a known reference (previous model versions or expected outputs). This helps identify unwanted changes due to updates or shifts in the input data distribution.\n\n3. **Debugging Capability**: If a model underperforms, regression testing can help identify specific areas where the model's accuracy has dropped. By comparing metrics such as Mean Absolute Error (MAE) and visualizations of errors, data scientists can diagnose issues more effectively ([Regression Quality Preset](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n\n4. **Automated Testing**: With tools like the `RegressionPreset`, regression tests can be automatically generated and assessed. This allows for continuous monitoring of model performance metrics against benchmarks and easier decision-making regarding model updates or retraining ([Overview of Regression Quality Preset](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n\n5. **Production Monitoring**: Regular evaluations using regression tests can give insights into how well the model is performing in the real world, facilitating early detection of issues and allowing for swift corrective actions when the model's predictions start to deviate from the expected ([Production Monitoring](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_regression.mdx)).\n\nIn summary, regression testing is essential for maintaining the reliability and effectiveness of machine learning models in production environments, helping ensure that updates do not introduce regressions while providing the insights necessary for informed decisions.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1521, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 390, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:50.866509Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:44Z"}, "provider_response_id": "chatcmpl-Crg3YFWkBJ6Pgt41Jydgm9KV8dXG1", "finish_reason": "stop", "run_id": "fc997f4c-dbb5-4cae-912c-715840968aa8", "metadata": null}], "source": "ai-generated"}