{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How can continuous integration/continuous deployment (CI/CD) practices affect model evaluation processes?", "timestamp": "2025-12-28T08:05:28.557962Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:05:28.558107Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "d8b78700-80af-437b-9fb2-b210d5f81e01", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"continuous integration continuous deployment CI/CD model evaluation processes\"}", "tool_call_id": "call_wAeVGZV0ffNcfWeVsKHSrN52", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 173, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 23, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:29.469643Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:28Z"}, "provider_response_id": "chatcmpl-Crg4GEKENLMdDT3hSTnQs8nLUHigd", "finish_reason": "tool_call", "run_id": "d8b78700-80af-437b-9fb2-b210d5f81e01", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 5. Get a Dashboard\n\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You'll be able to observe trends and set up alerts as you log more evaluation results."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 4. Create a Report\n\nTo create and run a report summarizing the evaluation results:\n\n```python\nreport = Report([TextEvals()])\nmy_eval = report.run(eval_dataset, None)\n```\n\nYou can preview the report locally or save it as a JSON, Python dictionary, or HTML file.\n\nTo upload the report to Evidently Cloud:\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```"}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 7. (Optional) Add a Custom LLM Judge\n\nYou can create custom criteria using built-in LLM judge templates, such as classifying questions as \"appropriate\" or \"inappropriate\":\n\n```python\nappropriate_scope = BinaryClassificationPromptTemplate(\n    criteria=\"\"\"An appropriate question is any educational query related to ...\"\"\",\n    target_category=\"APPROPRIATE\",\n    non_target_category=\"INAPPROPRIATE\",\n    include_reasoning=True,\n)\n\nllm_evals = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        LLMEval(\"question\", template=appropriate_scope, provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Question topic\")\n    ]\n)\n```"}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 3. Run Evaluations\n\nYou will evaluate the answers based on:\n\n- **Sentiment:** Score ranges from -1 (negative) to 1 (positive).\n- **Text length:** Measured by character count.\n- **Denials:** Refusals to answer, using an LLM-as-a-judge with a built-in prompt.\n\nSet the OpenAI key as an environment variable for LLM-as-a-judge:\n\n```python\n## import os\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n```\n\nTo run evaluations:\n\n```python\neval_dataset = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\"),\n        TextLength(\"answer\", alias=\"Length\"),\n        DeclineLLMEval(\"answer\", alias=\"Denials\")\n    ]\n)\n```\n\n**Congratulations!** You've successfully run your first evaluation."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## Introduction to Evidently\n\nEvidently helps you evaluate LLM outputs automatically. The platform enables comparisons of prompts and models, along with regression or adversarial tests, providing clear and repeatable checks. This leads to faster iterations, more confident decisions, and fewer surprises in production."}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Conclusion\n\nAs you integrate and test various changes, consider configuring alerts for your CI/CD workflows to monitor test failures effectively."}, {"title": "Evidently and GitHub actions", "description": "Testing LLM outputs as part of the CI/CD flow.", "filename": "docs-main/examples/github_actions.mdx", "section": "## How the Integration Works\n\n- Define a test dataset of inputs (e.g., test prompts with or without reference answers). This dataset can be stored as a file or saved in Evidently Cloud callable by Dataset ID.\n- Run your LLM system or agent against those inputs inside Continuous Integration (CI).\n- Evidently automatically evaluates the outputs using the user-specified config, which defines the Evidently descriptors, tests, and report composition. This includes methods like:\n  - LLM judges (e.g., tone, helpfulness, correctness)\n  - Custom Python functions\n  - Dataset-level metrics like classification quality\n- If any test fails, the CI job fails.\n- A detailed test report is generated with pass/fail status and metrics."}, {"title": "Overview", "description": "Running evals on the platform.", "filename": "docs-main/docs/platform/evals_overview.mdx", "section": "## Evaluations via API\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\nThis option is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.\n\n**How it works**:\n\n* Run Python-based evaluations on your AI outputs by generating Reports.\n* Upload results to the Evidently Platform.\n* Use the Explore feature to compare and debug results between runs.\n\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm)."}, {"title": "Why Evidently?", "description": "Why choose Evidently.", "filename": "docs-main/faq/why_evidently.mdx", "section": "## Modular Architecture\n\nEvidently is built to adapt to your needs without lock-ins or complex setups. It's modular and component-based, enabling you to start small:\n\n- Start with local ad hoc checks.\n- Share results by adding a UI to track evaluations over time.\n- Choose to upload raw data or only evaluation results.\n- Add monitoring as you prepare to move to production workflows.\n\nEvidently supports **Presets** and **reasonable defaults**, allowing you to run evaluations with minimal setup, including auto-generated test conditions for assertions. It integrates with existing tools and lets you export metrics, reports, and datasets easily."}, {"title": "Overview", "description": "How production AI quality monitoring works.", "filename": "docs-main/docs/platform/monitoring_overview.mdx", "section": "## Batch Monitoring Jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**: Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow, running at regular intervals or triggered by new data.\n\n* **Run metric calculations**: Implement the evaluation step using the Evidently Python library. Select evaluations and compute the `Reports` summarizing data, metrics, and test results.\n\n* **Store and visualize the results**: Store Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**: This setup does not store raw data or model predictions unless chosen. It retains aggregated data summaries and test results, protecting data privacy.\n\n* **Full control over the evaluation pipeline**: You decide when evaluations occur, making it great for batch ML models. You can log predictions and set up separate monitoring jobs for queries.\n\n* **Fits most ML evaluation scenarios**: Many evaluations, such as data drift detection, work in batches, requiring a set of new data points. Analyzing shifts in predictions or user behavior is also more meaningful at intervals.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch)."}], "tool_call_id": "call_wAeVGZV0ffNcfWeVsKHSrN52", "metadata": null, "timestamp": "2025-12-28T08:05:29.503393Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:29.503640Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "d8b78700-80af-437b-9fb2-b210d5f81e01", "metadata": null}, {"parts": [{"content": "Continuous Integration/Continuous Deployment (CI/CD) practices can significantly enhance model evaluation processes in several ways:\n\n1. **Automated Evaluation**: CI/CD frameworks can automate the evaluation of machine learning models as part of the regular integration and deployment cycles. This allows teams to run evaluations each time changes are made to the codebase, ensuring that model performance is continuously assessed against a fixed set of evaluation criteria. This can lead to faster feedback loops and identification of issues before they reach production.\n\n2. **Integration of Evaluation Reports**: By integrating model evaluations with CI/CD tools, teams can generate evaluation reports as part of the deployment pipeline. These reports can include detailed examination of model outputs against test datasets, metrics tracking, and any other performance indicators that are crucial for assessing model quality. This continuous monitoring helps in maintaining model performance over time and aids in decision-making regarding model updates or rollbacks [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx).\n\n3. **Custom Evaluation Criteria**: CI/CD setups allow the incorporation of custom evaluation criteria based on the specific needs of the project. For instance, teams can configure how outputs are assessed\u2014using metrics like tone, correctness, and classification quality. This level of customization leads to more relevant and accurate evaluations of model performance under deployment conditions [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/github_actions.mdx).\n\n4. **Error Tracking and Alerts**: With CI/CD, teams can set up alerts for any evaluation failures. This means that if a model fails to meet the defined criteria during evaluation, the CI job can fail, preventing inadequate models from being deployed and ensuring that only validated models make it to production [source](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_regression_testing.mdx).\n\n5. **Version Control of Model Evaluations**: CI/CD supports version control not just of the code but also of the evaluation scripts and configurations, making it easier to track changes over time. This ensures that the stakeholders can revert to previous evaluation setups whenever necessary, promoting more reliable and repeatable evaluations [source](https://github.com/evidentlyai/docs/tree/main/docs-main/faq/why_evidently.mdx).\n\n6. **Monitoring Trends**: CI/CD pipelines can include monitoring features that allow teams to observe trends in model performance over time. This can be implemented through dashboards that track key evaluation metrics, helping teams to adjust their models proactively based on real-time data and trends [source](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/monitoring_overview.mdx).\n\nIn summary, CI/CD practices facilitate more structured, automated, and effective evaluation processes, resulting in higher confidence in model quality and faster deployment cycles. These improvements are crucial for teams looking to maintain high standards in machine learning model performance.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1958, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 583, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:40.022932Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:05:29Z"}, "provider_response_id": "chatcmpl-Crg4H8dIiZDGQHIuAO7O6XYMTWi3d", "finish_reason": "stop", "run_id": "d8b78700-80af-437b-9fb2-b210d5f81e01", "metadata": null}], "source": "ai-generated"}