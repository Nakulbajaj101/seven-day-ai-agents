{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What evaluation metrics are relevant for image data?", "timestamp": "2025-12-28T03:06:51.373696Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T03:06:51.373910Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluation metrics for image data\"}", "tool_call_id": "call_nxU4hVQG38azbXDQ92ayufPJ", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 165, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 19, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T03:06:52.956466Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T03:06:52Z"}, "provider_response_id": "chatcmpl-CrbPIm2KMH1uPv8afNj3m6h24qGDR", "finish_reason": "tool_call", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Benefits of Synthetic Data\n\nSynthetic data is not a replacement for real data or expert-designed tests \u2014 it\u2019s a way to add variety and speed up the process. With synthetic data you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses.\n\nIt\u2019s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Application in Complex AI Systems\n\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "## Use Cases for Synthetic Test Inputs\n\nEvidently Cloud can be utilized for multiple purposes, including:\n\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\n* **Regression Testing**: Validate changes in your AI system before deployment.\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\n\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"filename": "docs-main/images/metrics/readme.md", "section": "## Evaluations and Metrics Overview\n\nThis section provides an introduction to evaluations and metrics used in assessing various processes or systems. It outlines the importance of these metrics in understanding performance and effectiveness."}, {"title": "Leftovers", "description": "Description of your new file.", "noindex": true, "filename": "docs-main/docs/library/leftover_content.mdx", "section": "## Metrics\n\nOverview of metrics used in data analysis, including correlation checks and data quality assessments."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}], "tool_call_id": "call_nxU4hVQG38azbXDQ92ayufPJ", "metadata": null, "timestamp": "2025-12-28T03:06:53.023347Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T03:06:53.025034Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"image data evaluation metrics\"}", "tool_call_id": "call_Bg0ajXp0VbbLOfuCOllUbufc", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 1493, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 18, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T03:06:54.416955Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T03:06:54Z"}, "provider_response_id": "chatcmpl-CrbPKuoQCyhLTbs4d0RleltEoKORc", "finish_reason": "tool_call", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Benefits of Synthetic Data\n\nSynthetic data is not a replacement for real data or expert-designed tests \u2014 it\u2019s a way to add variety and speed up the process. With synthetic data you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses.\n\nIt\u2019s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Application in Complex AI Systems\n\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "## Use Cases for Synthetic Test Inputs\n\nEvidently Cloud can be utilized for multiple purposes, including:\n\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\n* **Regression Testing**: Validate changes in your AI system before deployment.\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\n\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Leftovers", "description": "Description of your new file.", "noindex": true, "filename": "docs-main/docs/library/leftover_content.mdx", "section": "## Metrics\n\nOverview of metrics used in data analysis, including correlation checks and data quality assessments."}, {"title": "Why Evidently?", "description": "Why choose Evidently.", "filename": "docs-main/faq/why_evidently.mdx", "section": "## Built-in Evaluations\n\nEvidently prioritizes evaluations and quality testing. Many tools require you to prepare data and implement metrics from scratch, but we provide **100+ built-in evaluations** for various ML and LLM use cases. From ranking metrics to data drift algorithms, we have streamlined quality assessment. Users can also extend Evidently by adding custom metrics. \n\nEvidently Cloud features advanced testing capabilities, including synthetic data generation and adversarial testing for creating and running test scenarios."}, {"filename": "docs-main/images/metrics/readme.md", "section": "## Evaluations and Metrics Overview\n\nThis section provides an introduction to evaluations and metrics used in assessing various processes or systems. It outlines the importance of these metrics in understanding performance and effectiveness."}], "tool_call_id": "call_Bg0ajXp0VbbLOfuCOllUbufc", "metadata": null, "timestamp": "2025-12-28T03:06:54.469958Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T03:06:54.470418Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"image classification metrics\"}", "tool_call_id": "call_5UuyRU2qZeb9QZNJMaX4HX1w", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 2891, "cache_write_tokens": 0, "cache_read_tokens": 1408, "output_tokens": 17, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T03:06:56.051833Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T03:06:55Z"}, "provider_response_id": "chatcmpl-CrbPLspmF5CCrKB5eF48Vk3RTQUkF", "finish_reason": "tool_call", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Classification Quality by Feature\n\nIn this table, we show a number of plots for each feature. To expand the plots, click on the feature name.\n\n![Classification Quality by Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2)\n\nIn the tab \u201cALL\u201d, you can see the distribution of classes against the values of the feature. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target.\n\n![Classification Quality by Feature Example](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-84fc78dd31a5732be416239154f2c90515d78c42%252Fprob_class_perf_classification_quality_by_feature_example_all.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8e13fd88&sv=2)\n\nFor each class, you can see the predicted probabilities alongside the values of the feature.\n\n![Classification Quality by Feature Example Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2)\n\nIt visualizes the regions where the model makes errors of each type and reveals the low-performance segments. You can compare the distributions and see **if the errors are sensitive to the values of a given feature**."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Confusion Matrix\n\nVisualizes the classification errors and their type.\n\n![Confusion Matrix](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Quality Metrics by Class\n\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## ROC Curve\n\nROC Curve (**receiver operating characteristic curve**) shows the share of true positives and true negatives at different classification thresholds.\n\n![ROC Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-756dcacc9d462ad45745c6bb8f6dd23c5a18d9d2%252Fprob_class_perf_roc.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=caee6451&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Precision-Recall Curve\n\nThe **precision**-**recall curve** shows the trade-off between **precision** and **recall** for different classification thresholds.\n\n![Precision-Recall Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2)"}, {"title": "Leftovers", "description": "Description of your new file.", "noindex": true, "filename": "docs-main/docs/library/leftover_content.mdx", "section": "## Classification Metrics\n\nMetrics related to classification tasks.\n\n| Metric                                    | Description                                                                                                                                                               | Parameters                                                                                                                                               | Test Defaults                                                                                                      |\n|"}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "## Classification\n\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\n\n### Additional Info\n\n[Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.\n\n### General\n\nUse for binary classification and aggregated results for multi-class.\n\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\n|"}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Overview\n\nThe `ClassificationPreset` allows you to evaluate and visualize the performance on classification tasks, whether binary or multi-class. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance or a different model/prompt).\n\n### Metrics and Visualizations\n\n* **Various metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\n\nAdditionally, if you include feature columns, the Report will show Classification Quality by column, reflecting how the system performs on different data segments."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Quality Metrics by Class\n\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"}], "tool_call_id": "call_5UuyRU2qZeb9QZNJMaX4HX1w", "metadata": null, "timestamp": "2025-12-28T03:06:56.093334Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T03:06:56.093894Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}, {"parts": [{"content": "For evaluating image data, particularly in the context of classification tasks, several metrics are commonly used to measure the performance of models. Here are some of the most relevant evaluation metrics for image classification:\n\n1. **Accuracy**: The ratio of correctly predicted instances to the total instances. It provides a basic measure of performance.\n\n2. **Precision**: Indicates the accuracy of the positive predictions. It is defined as the number of true positives divided by the sum of true positives and false positives.\n\n3. **Recall (Sensitivity)**: Measures the ability to find all relevant instances, defined as the number of true positives divided by the sum of true positives and false negatives.\n\n4. **F1 Score**: The harmonic mean of precision and recall, useful for dealing with class imbalances.\n\n5. **ROC AUC (Receiver Operating Characteristic - Area Under Curve)**: This metric evaluates the trade-off between the true positive rate and false positive rate across different thresholds.\n\n6. **Confusion Matrix**: A table that summarizes the performance of a classification algorithm by showing the correct and incorrect predictions across the different classes.\n\n7. **Log Loss**: Evaluates the probability outputs of the model by measuring the uncertainty of the predictions.\n\n8. **Precision-Recall Curve**: Displays the trade-off between precision and recall at different thresholds, providing a more insightful visualization than the ROC curve, particularly for imbalanced datasets.\n\n9. **Class Representation**: Additional visualization to understand how well the model performs across different classes.\n\nThese metrics allow for a comprehensive evaluation of the classification performance on image data and can help in optimizing and tuning machine learning models for better accuracy and reliability.\n\nYou can find more detailed information about these metrics in the following document: [Classification Metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 5176, "cache_write_tokens": 0, "cache_read_tokens": 2816, "output_tokens": 374, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T03:07:04.552995Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T03:06:57Z"}, "provider_response_id": "chatcmpl-CrbPNlgD3yxN59yApz9qdgHUqCJSD", "finish_reason": "stop", "run_id": "11b081b8-4a50-4b95-a878-e4b9cef67c95", "metadata": null}], "source": "user"}