{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How can the performance of an LLM be continuously monitored over time?", "timestamp": "2025-12-28T08:04:11.958800Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:11.958915Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "60cd5989-12d5-43cf-ac6b-1bc561742cde", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"monitoring performance LLM over time\"}", "tool_call_id": "call_FNG89LjF2uENVQhrrjEwOo64", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 170, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:12.774913Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:12Z"}, "provider_response_id": "chatcmpl-Crg3232EpS77Bzy9fybU9U3Ddr55z", "finish_reason": "tool_call", "run_id": "60cd5989-12d5-43cf-ac6b-1bc561742cde", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Overview of Batch Monitoring\n\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Workflow Steps\n\nThe complete workflow consists of the following steps:\n\n### Step 1: Configure the Metrics\n\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\n\n### Step 2: Run the Evaluations\n\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\n\n- First, perform data quality, data drift, and prediction drift checks.\n- After receiving delayed labels, run ML quality check results.\n\nYou can backdate your Reports with a custom timestamp.\n\n### Step 3: Upload to the Platform\n\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\n\n### Step 4: Configure the Dashboard\n\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### Step 5: Configure Alerts\n\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Running Tests vs Reports\n\nStructuring your evaluations as Tests, rather than monitoring a multitude of metrics simultaneously, can help alleviate alert fatigue and streamline configuration. This method simplifies the evaluation of multiple conditions at once, such as quickly verifying that all columns in the input data fall within a defined min-max range."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Simple Example\n\nTo get the dataset stats for a single batch and upload to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\n\n### Sample Code\n\n```python\neval_data = Dataset.from_pandas(\n    pd.DataFrame(source_df),\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    DatasetStats()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=False)\n```"}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Tutorial Scope\n\nHere's what we'll cover:\n\n- **Create a toy dataset**: Build a small Q&A dataset with answers and reference responses.\n- **Get new answers**: Simulate generating new answers.\n- **Create and run a Report with Tests**: Compare answers using LLM-as-a-judge.\n- **Build a monitoring Dashboard**: Track test results over time."}, {"title": "RAG evals", "description": "Metrics to evaluate a RAG system.", "filename": "docs-main/examples/llm_rag_evals.mdx", "section": "## What's Next?\n\nConsider implementing [regression testing](/examples/LLM_regression_testing) to monitor how your RAG system's retrieval and response quality change over time."}, {"title": "Text Evals", "description": "Overview of the Text Evals Preset.", "filename": "docs-main/metrics/preset_text_evals.mdx", "section": "## Use Cases\n\nYou can use this Preset in various scenarios:\n\n* **LLM experiments.** Visually explore evaluation results while experimenting on prompts, model versions, etc., and compare different runs.\n\n* **LLM observability.** Run evaluations on production data and track resulting statistics over time."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 5. Get a Dashboard\n\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You'll be able to observe trends and set up alerts as you log more evaluation results."}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Tutorial Scope\n\nHere's what we'll cover:\n\n- **Create a toy dataset**: Build a small Q&A dataset with answers and reference responses.\n- **Get new answers**: Simulate generating new answers.\n- **Create and run a Report with Tests**: Compare answers using LLM-as-a-judge.\n- **Build a monitoring Dashboard**: Track test results over time."}, {"title": "Overview", "description": "How production AI quality monitoring works.", "filename": "docs-main/docs/platform/monitoring_overview.mdx", "section": "## Tracing with Scheduled Evals\n\n<Check>\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\n</Check>\n\n**Best for**: LLM-powered applications\n\n![](/images/monitoring_flow_tracing.png)\n\n**How it works:**\n\n* **Instrument your app**: Use the `Tracely` library to capture all relevant data, including inputs, outputs, and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\n\n* **Store raw data**: Evidently Platform stores all raw data, providing a complete record of activity.\n\n* **Schedule evaluations**: Set up evaluations to run automatically at specified times, generating Reports or running Tests directly. Manual evaluation is also possible anytime.\n\n**Benefits of this approach**:\n\n* **Solves the data capture**: Collect complex traces and all production data in one place for easier management and analysis.\n\n* **Easy to re-run evals**: Raw traces stored on the platform allow for easy re-running of evaluations or adding new metrics as needed.\n\n* **No-code**: After trace instrumentation setup, all management can be done through the UI.\n\n**Next step:** check the [Tracing Quickstart](/quickstart_tracing)."}], "tool_call_id": "call_FNG89LjF2uENVQhrrjEwOo64", "metadata": null, "timestamp": "2025-12-28T08:04:12.811976Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:12.812221Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "60cd5989-12d5-43cf-ac6b-1bc561742cde", "metadata": null}, {"parts": [{"content": "To continuously monitor the performance of a large language model (LLM) over time, the following structured approach can be implemented:\n\n1. **Batch Monitoring with Evaluation API**:\n   - Utilize the core evaluation API to set up batch monitoring for your LLM. This involves defining metrics and conditions for evaluating your model's performance. The process begins by configuring an Evidently Report with optional tests to specify what evaluations are needed ([Batch monitoring](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx)).\n\n2. **Workflow Steps**:\n   - **Configure Metrics**: Define the metrics and tests to evaluate the LLM.\n   - **Run Evaluations**: Execute evaluations at scheduled intervals using tools like Apache Airflow. This can include checks for data quality, drift, and model predictions.\n   - **Upload Results**: Decide whether to store raw predictions or just upload metric summaries; this ensures that relevant data is readily available for analysis.\n   - **Setup Dashboards**: Create a dashboard that tracks results over time, allowing for a visual representation of the performance metrics.\n   - **Configure Alerts**: Set alerts for significant changes or failures in performance metrics to take timely corrective action.\n\n3. **Regression Testing**:\n   - Implement regression testing where new outputs from the model are compared against reference outputs. This involves creating a dataset of expected outputs, retrieving new outputs, and running tests to evaluate the performance over time ([LLM regression testing](https://github.com/evidentlyai/docs/tree/main/docs/examples/llm_regression_testing.mdx)).\n\n4. **Scheduled Evaluations**:\n   - Set up scheduled evaluations that automatically run at specified times. This automated approach allows for continuous monitoring without manual intervention, ensuring consistent oversight of the model's performance ([Overview](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_overview.mdx)).\n\n5. **Using Monitoring Dashboards**:\n   - Utilize dashboards to visualize and track the results over time, providing insights into performance trends. This helps in identifying anomalies or declines in output quality ([LLM Evaluation](https://github.com/evidentlyai/docs/tree/main/docs-main/quickstart_llm.mdx)).\n\nIn conclusion, continuous monitoring of LLM performance involves setting up automated evaluations, regression testing, and visual dashboards to effectively track and respond to changes in model behavior over time. Each component contributes to a comprehensive monitoring ecosystem designed to maintain and enhance model performance.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1646, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 504, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:21.990625Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:13Z"}, "provider_response_id": "chatcmpl-Crg33ONXQky92ifHDjrsH614fWogl", "finish_reason": "stop", "run_id": "60cd5989-12d5-43cf-ac6b-1bc561742cde", "metadata": null}], "source": "ai-generated"}