{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How can benchmarking against previous model versions assist in evaluation and monitoring?", "timestamp": "2025-12-28T08:05:48.625885Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:05:48.626026Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "eead9df9-3566-4da4-8706-b92cc2e914d3", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"benchmarking against previous model versions evaluation monitoring\"}", "tool_call_id": "call_GYCv90VsrDsTglJemUuhBXUn", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 169, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 22, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:49.338372Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:48Z"}, "provider_response_id": "chatcmpl-Crg4a99BGebJWlGIZ80wD4U77gPaw", "finish_reason": "tool_call", "run_id": "eead9df9-3566-4da4-8706-b92cc2e914d3", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Overview of Batch Monitoring\n\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Running Tests vs Reports\n\nStructuring your evaluations as Tests, rather than monitoring a multitude of metrics simultaneously, can help alleviate alert fatigue and streamline configuration. This method simplifies the evaluation of multiple conditions at once, such as quickly verifying that all columns in the input data fall within a defined min-max range."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Workflow Steps\n\nThe complete workflow consists of the following steps:\n\n### Step 1: Configure the Metrics\n\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\n\n### Step 2: Run the Evaluations\n\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\n\n- First, perform data quality, data drift, and prediction drift checks.\n- After receiving delayed labels, run ML quality check results.\n\nYou can backdate your Reports with a custom timestamp.\n\n### Step 3: Upload to the Platform\n\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\n\n### Step 4: Configure the Dashboard\n\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### Step 5: Configure Alerts\n\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Simple Example\n\nTo get the dataset stats for a single batch and upload to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\n\n### Sample Code\n\n```python\neval_data = Dataset.from_pandas(\n    pd.DataFrame(source_df),\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    DatasetStats()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=False)\n```"}, {"title": "Dashboard panel types [Legacy]", "description": "Overview of the available monitoring Panels.", "filename": "docs-main/docs/platform/dashboard_panel_types.mdx", "section": "## Overview of Monitoring Panels\n\nThis section introduces monitoring panels in versions 0.6.0 to 0.7.1 for Cloud/Workspace v1.\n\nA monitoring panel is an individual plot or counter on the Monitoring Dashboard. You can customize panel types, values shown, titles, and legends. Multiple panels can be organized by tabs."}, {"title": "Explore view", "description": "Reviewing the evaluation results on the Platform.", "filename": "docs-main/docs/platform/evals_explore.mdx", "section": "## Comparing Evaluations\n\nTo analyze multiple evaluation results side by side, select reports from the Report list (for example, those generated using different LLMs) and click the **\"Compare\"** button. This feature helps you identify differences in performance, quality, or behavior across different model versions or configurations.\n\nYou can also click on \"duplicate snapshot\" while viewing a specific Report to maintain the current Metric in view and select another Report for comparison."}, {"title": "Text Evals", "description": "Overview of the Text Evals Preset.", "filename": "docs-main/metrics/preset_text_evals.mdx", "section": "## Use Cases\n\nYou can use this Preset in various scenarios:\n\n* **LLM experiments.** Visually explore evaluation results while experimenting on prompts, model versions, etc., and compare different runs.\n\n* **LLM observability.** Run evaluations on production data and track resulting statistics over time."}, {"title": "Recommendations", "description": "Overview of the Recommender Systems Preset", "filename": "docs-main/metrics/preset_recsys.mdx", "section": "## Use Case\n\nThese Presets are useful in various scenarios:\n\n- **Experimental evaluations** as you iterate on building your recommender system.\n- **Side-by-side comparison** for two different models or periods.\n- **Production monitoring** checks after you acquire ground truth labels.\n- **Debugging**. If you notice a drop in performance, use the visual Report to understand changes."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 5. Get a Dashboard\n\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You'll be able to observe trends and set up alerts as you log more evaluation results."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}], "tool_call_id": "call_GYCv90VsrDsTglJemUuhBXUn", "metadata": null, "timestamp": "2025-12-28T08:05:49.371849Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:49.372129Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "eead9df9-3566-4da4-8706-b92cc2e914d3", "metadata": null}, {"parts": [{"content": "Benchmarking against previous model versions is critical for effective evaluation and monitoring of machine learning models. This process allows practitioners to:\n\n1. **Identify Performance Trends**: By comparing the performance metrics of different model versions, it becomes feasible to track improvements or regressions over time. This is essential to ensure that updates to the model are indeed beneficial.\n\n2. **Understand Model Behavior**: Benchmarking provides insights into how changes in the model impact its performance on various tasks or datasets. It can help in identifying which specific changes (e.g., feature adjustments or algorithm updates) lead to performance variations.\n\n3. **Facilitate Quality Assurance**: Regularly comparing against established benchmarks serves as a quality control mechanism. If a new model version performs significantly worse than its predecessors, it raises flags for further investigation.\n\n4. **Support Explainability and Transparency**: With the ability to visualize differences between model versions, stakeholders can understand why certain models perform better. This transparency can be vital when justifying model choices in production environments.\n\n5. **Optimize Parameters and Strategies**: Benchmarking can highlight which configurations yield the best results, allowing for more informed decisions in hyperparameter tuning and model selection strategies.\n\nFor practical implementation, using tools for batch monitoring and evaluation can create comprehensive reports that compare the performance of various models side by side. For instance, utilizing features from evaluation libraries can help streamline this process, track results over time, and set up alerts based on performance metrics.\n\nFor more detailed information on model evaluation and monitoring, you can check the relevant resources in the documentation:\n- [Batch Monitoring Overview](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx)\n- [Comparing Evaluations](https://github.com/evidentlyai/docs/tree/main/docs/platform/evals_explore.mdx)", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1416, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 366, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:56.407822Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:05:49Z"}, "provider_response_id": "chatcmpl-Crg4bS8NL7Zujx4ucXmCQA3xlIui9", "finish_reason": "stop", "run_id": "eead9df9-3566-4da4-8706-b92cc2e914d3", "metadata": null}], "source": "ai-generated"}