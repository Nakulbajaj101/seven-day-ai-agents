{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What specific metrics can be used to evaluate the accuracy of image classification models?", "timestamp": "2025-12-28T08:04:35.924781Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:35.925058Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "416bad55-573c-4466-8864-fe5e54db6e37", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"metrics evaluate accuracy image classification models\"}", "tool_call_id": "call_qqvnvdoU0BEOAXn2n8yB7xj6", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 171, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 20, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:36.688170Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:36Z"}, "provider_response_id": "chatcmpl-Crg3Q4CC7MWNnNSeDNGYDVSPT7rUj", "finish_reason": "tool_call", "run_id": "416bad55-573c-4466-8864-fe5e54db6e37", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Classification Quality by Feature\n\nIn this table, we show a number of plots for each feature. To expand the plots, click on the feature name.\n\n![Classification Quality by Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2)\n\nIn the tab \u201cALL\u201d, you can see the distribution of classes against the values of the feature. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target.\n\n![Classification Quality by Feature Example](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-84fc78dd31a5732be416239154f2c90515d78c42%252Fprob_class_perf_classification_quality_by_feature_example_all.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8e13fd88&sv=2)\n\nFor each class, you can see the predicted probabilities alongside the values of the feature.\n\n![Classification Quality by Feature Example Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2)\n\nIt visualizes the regions where the model makes errors of each type and reveals the low-performance segments. You can compare the distributions and see **if the errors are sensitive to the values of a given feature**."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Confusion Matrix\n\nVisualizes the classification errors and their type.\n\n![Confusion Matrix](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Quality Metrics by Class\n\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Class Separation Quality\n\nA scatter plot of the predicted probabilities shows correct and incorrect predictions for each class.\n\nIt serves as a representation of both model accuracy and the quality of its calibration. It also helps visually **choose the best probability threshold for each class.**\n\n![Class Separation Quality](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e16ca5618f050c77396f80958fecebf330c24c72%252Fprob_class_perf_class_separation_quality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b3b31454&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates several standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). For each quality metric, Evidently also shows one standard deviation of its value to estimate the stability of the performance. \n\nTo support model performance analysis, Evidently generates interactive visualizations, helping to analyze where the model makes mistakes and to ideate on improvements."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Mean Error per Group\n\nA summary of model quality metrics for specific segments of predictions\u2014those with the highest negative and positive errors\u2014is provided here. The metrics include mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE)."}], "tool_call_id": "call_qqvnvdoU0BEOAXn2n8yB7xj6", "metadata": null, "timestamp": "2025-12-28T08:04:36.725252Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:36.725506Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "416bad55-573c-4466-8864-fe5e54db6e37", "metadata": null}, {"parts": [{"content": "To evaluate the accuracy of image classification models, several key metrics can be employed:\n\n1. **Accuracy**: This is the ratio of correctly predicted instances to the total instances. It provides a quick overview of the model's performance.\n\n2. **Precision**: Precision measures the ratio of true positive predictions to the total positive predictions made by the model. It reflects the quality of the positive class predictions.\n\n3. **Recall (Sensitivity)**: Recall is the ratio of true positive predictions to the total actual positives. It indicates how well the model identifies positive instances.\n\n4. **F1-score**: The F1-score is the harmonic mean of precision and recall. It provides a balance between the two metrics, especially in cases of class imbalance.\n\n5. **ROC AUC (Receiver Operating Characteristic Area Under the Curve)**: This metric evaluates the model's ability to distinguish between classes and plots the trade-off between true positive rate and false positive rate.\n\n6. **Log Loss (Cross-Entropy Loss)**: Log Loss provides a measure of how close the predicted probabilities are to the actual classes, penalizing incorrect predictions based on their confidence.\n\n7. **Confusion Matrix**: This visual tool helps understand the performance of a model by depicting true positives, false positives, true negatives, and false negatives.\n\nThese metrics can help to analyze different aspects of the model's performance and identify areas for improvement. They are essential for ensuring that the model is not only accurate but also reliable and effective in making predictions across potentially skewed classes or distributions [Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2137, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 336, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:42.980642Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:36Z"}, "provider_response_id": "chatcmpl-Crg3QUdJwr5pwhQ51vHP0lxmlKcir", "finish_reason": "stop", "run_id": "416bad55-573c-4466-8864-fe5e54db6e37", "metadata": null}], "source": "ai-generated"}