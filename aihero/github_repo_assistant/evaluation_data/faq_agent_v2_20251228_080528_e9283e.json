{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What are the implications of using biased training datasets on the evaluation of machine learning models?", "timestamp": "2025-12-28T08:05:19.856216Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:05:19.856352Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "8668c98b-04be-421b-9b1a-00c3b513e8e0", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"biased training datasets implications machine learning model evaluation\"}", "tool_call_id": "call_XCsjQHGFEBz0iv0QOy45IEHa", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 173, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 22, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:20.792225Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:20Z"}, "provider_response_id": "chatcmpl-Crg48RBX94oF6wWmXq6sszfz4g33b", "finish_reason": "tool_call", "run_id": "8668c98b-04be-421b-9b1a-00c3b513e8e0", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Work with datasets", "description": "How to create, upload and manage Datasets.", "filename": "docs-main/docs/platform/datasets_workflow.mdx", "section": "## Include the Dataset in Reports\n\nYou can include Datasets when you upload Reports to the platform. This way, after running an evaluation locally, you simultaneously upload:\n\n- the Report with the evaluation result,\n- the Dataset it was generated for, with new added scores if applicable.\n\nBy default, you upload only the Report. To include the Dataset, use the `include_data` parameter:\n\n```python\nws.add_run(project.id, data_report, include_data=True)\n```\n\nCheck the docs on [running evals via API](/docs/platform/evals_api) for additional details."}, {"title": "Work with datasets", "description": "How to create, upload and manage Datasets.", "filename": "docs-main/docs/platform/datasets_workflow.mdx", "section": "## Download the Dataset\n\nYou can pull the Dataset stored or generated on the platform to your local environment. For example, call the evaluation or tracing dataset to use in a CI/CD testing script.\n\nUse the `load_dataset` method:\n\n```python\neval_dataset = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\") \n\n# to create as pandas dataframe\ndf = eval_dataset.as_dataframe()\n```"}, {"title": "Work with datasets", "description": "How to create, upload and manage Datasets.", "filename": "docs-main/docs/platform/datasets_workflow.mdx", "section": "## Connecting to Evidently Cloud or Local Workspace\n\nYou must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage)."}, {"title": "Work with datasets", "description": "How to create, upload and manage Datasets.", "filename": "docs-main/docs/platform/datasets_workflow.mdx", "section": "## Upload a Dataset\n\n### Uploading via Python\n\nPrepare your dataset as an Evidently Dataset with the corresponding data definition. To upload a Dataset to the specified Project in workspace `ws`, use the `add_dataset` method:\n\n```python\neval_data = Dataset.from_pandas(\n    source_df,\n    data_definition=DataDefinition()\n)\nws.add_dataset(\n    dataset = eval_data, \n    name = \"dataset_name\",\n    project_id = project.id, \n    description = \"Optional description\"\n)\n```\n\nYou must always specify the dataset `name` that you will see in the UI. The description is optional.\n\n### Uploading via UI\n\nTo upload any existing dataset as a CSV file, click on \"Add dataset\". When you upload the Dataset, you must also add a [**data definition**](/docs/library/data_definition). This lets Evidently understand the role of specific columns and prepare your Dataset for future evaluations.\n\n**Note:** Read the [Data Definition docs](../library/data-definition) for guidance on creating an Evidently Dataset."}, {"title": "Overview", "description": "Introduction to Datasets.", "filename": "docs-main/docs/platform/datasets_overview.mdx", "section": "## When Do You Need Datasets?\n\nHere are common use cases for datasets in Evidently:\n\n- **Organize evaluation datasets**: Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in the UI and access them programmatically for CI/CD checks.\n- **Debug evaluation results**: After you run an evaluation, view the dataset to identify and debug specific failures. For example, you can sort all text outputs by added scores.\n- **Store ML inference logs or LLM traces**: Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it."}, {"title": "Overview", "description": "Introduction to Tracing.", "filename": "docs-main/docs/platform/tracing_overview.mdx", "section": "## Running Evaluations on Tracing Datasets\n\nOnce tracing data is captured, you can also run evaluations (evals) on the tracing datasets for further insights."}, {"title": "Data Drift", "description": "Overview of the Data Drift Preset.", "filename": "docs-main/metrics/preset_data_drift.mdx", "section": "## Use Cases\n\nYou can evaluate data drift in various scenarios:\n\n* **Monitoring ML Model Performance**: When true labels or actuals are unavailable, monitoring feature drift and prediction drift helps check if the model operates in a familiar environment.\n* **Debugging ML Model Quality Decay**: Evaluate data drift to explore changes in feature patterns if a drop in model quality is observed.\n* **Understanding Model Drift in Offline Environment**: Explore historical data drift to understand past changes and define optimal detection and retraining strategies.\n* **Deciding on Model Retraining**: Verify if retraining is necessary. If no data drift is detected, the environment is stable.\n\n<Info>\n  For a conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift).\n</Info>"}, {"title": "Overview", "description": "Introduction to Datasets.", "filename": "docs-main/docs/platform/datasets_overview.mdx", "section": "## When Do You Need Datasets?\n\nHere are common use cases for datasets in Evidently:\n\n- **Organize evaluation datasets**: Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in the UI and access them programmatically for CI/CD checks.\n- **Debug evaluation results**: After you run an evaluation, view the dataset to identify and debug specific failures. For example, you can sort all text outputs by added scores.\n- **Store ML inference logs or LLM traces**: Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it."}, {"title": "Introduction", "description": "Core concepts and components of the Evidently Python library.", "filename": "docs-main/docs/library/overview.mdx", "section": "## Core Evaluation Concepts\n\n### Dataset Preparation\n\nTo run an evaluation, you first need to prepare the data as a pandas DataFrame, which can include any combination of numerical, categorical, text, and metadata columns. Once prepared, you create an Evidently `Dataset` object to process the data correctly."}, {"title": "Data definition", "description": "How to map the input data.", "filename": "docs-main/docs/library/data_definition.mdx", "section": "## Evaluations Overview\n\nTo run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\n\n- **Column types** (e.g., categorical, numerical, text).\n- **Column roles** (e.g., id, prediction, target).\n\nThis structure allows Evidently to process the data correctly, ensuring specific columns required for evaluations are present."}], "tool_call_id": "call_XCsjQHGFEBz0iv0QOy45IEHa", "metadata": null, "timestamp": "2025-12-28T08:05:20.858070Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:20.858309Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "8668c98b-04be-421b-9b1a-00c3b513e8e0", "metadata": null}, {"parts": [{"content": "Using biased training datasets can have several significant implications for the evaluation of machine learning models:\n\n1. **Performance Degradation**: Models trained on biased datasets may exhibit poor performance on unseen data, especially if that data reflects a more balanced distribution of features or classes. When evaluated, the model's accuracy and reliability can be significantly lower than expected, leading to misguided conclusions about its effectiveness.\n\n2. **Misleading Evaluation Metrics**: Bias in training data can skew evaluation metrics (like precision, recall, and F1 score). For instance, if a certain class is under-represented during training, a model may predict heavily toward the majority class, thus artificially inflating its evaluation metrics when tested on a similar biased dataset.\n\n3. **Unequal Treatment of Groups**: Bias in datasets can lead models to perform poorly for under-represented or marginalized groups, resulting in ethical and legal implications. Evaluations may mask these issues if they do not reflect the performance across diverse populations.\n\n4. **Reinforcement of Pre-existing Biases**: Machine learning models are vulnerable to perpetuating biases present in training data. If these biases are not recognized and managed during the evaluation phase, they can contribute to systemic issues, particularly in sensitive applications like hiring, law enforcement, and healthcare.\n\n5. **Need for Comprehensive Testing**: Relying solely on biased datasets for evaluation limits understanding of model robustness. Evaluations should involve diverse datasets to ensure models can generalize well across various scenarios and groups.\n\n6. **Challenges in Data Drift and Model Monitoring**: Bias can also complicate ongoing performance monitoring and data drift evaluations. If the training and evaluation datasets continue to convey biased information, it can hinder the accurate detection of whether a model is still functioning effectively in real-world scenarios ([docs-main/docs/platform/datasets_overview.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/datasets_overview.mdx)).\n\nIn conclusion, the implications of using biased training datasets require careful consideration to ensure machine learning models are fair, reliable, and effectively evaluated. Addressing bias in datasets is crucial for ethical AI deployment and maintaining model integrity.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1668, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 433, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:28.553307Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:05:21Z"}, "provider_response_id": "chatcmpl-Crg494SDP92ESUVCylV2JJAFQqhzV", "finish_reason": "stop", "run_id": "8668c98b-04be-421b-9b1a-00c3b513e8e0", "metadata": null}], "source": "ai-generated"}