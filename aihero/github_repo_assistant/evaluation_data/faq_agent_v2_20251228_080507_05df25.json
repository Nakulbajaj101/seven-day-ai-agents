{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What steps should be taken to establish a custom metric for evaluating a specific model's performance?", "timestamp": "2025-12-28T08:04:58.760386Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:58.760515Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "5c45e85b-5ea8-4f47-9122-cb36778c5dad", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"custom metric model performance evaluation steps\"}", "tool_call_id": "call_OtisEBhC2lBXGaQprGzn7ux4", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 174, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 20, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:59.572710Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:58Z"}, "provider_response_id": "chatcmpl-Crg3muTvrykrQlWYbyJ2luTjJEmKD", "finish_reason": "tool_call", "run_id": "5c45e85b-5ea8-4f47-9122-cb36778c5dad", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Creating a Custom Metric\n\nCreating a custom Metric requires:\n\n- **(Required)**: Implementing the Metric **calculation method**.\n- **(Optional)**: Defining the **default Test conditions** that apply when running Tests for this Metric.\n- **(Optional)**: Creating a **custom visualization** for the Metric using Plotly.\n\nAfter implementation, the Metric can be used in Reports, viewed in the Evidently Cloud or a self-hosted UI, and visualized over time on the Dashboard."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Custom Metrics and Tests Overview\n\nYou can build fully custom Metrics and Tests to handle any column- or dataset-level evaluations, enabling the implementation of business metrics, weighted scores, etc."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Example Usage\n\nOnce your custom Metric is implemented, it can be referenced in a Report.\n\n### Sample Data Creation\n\n```python\ndata = {\n    \"Item\": [f\"Item_{i}\" for i in range(1, 11)],\n    \"Quantity\": np.random.randint(1, 50, size=10),\n    \"Sales\": np.random.uniform(100, 5000, size=10).round(2),\n}\n\ndf = pd.DataFrame(data)\n\ndataset = Dataset.from_pandas(\n    pd.DataFrame(df),\n    data_definition=DataDefinition()\n)\n```\n\n### Adding to Report\n\n```python\nreport = Report([\n    MyMaxMetric(column=\"Sales\")\n])\nmy_eval = report.run(dataset, None)\nmy_eval\n```"}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Contributing to the Library\n\n**Note**: If you would like a Metric added to the core library, share your idea or feature request by [opening a GitHub issue](https://github.com/evidentlyai/evidently/issues)."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Workflow Steps\n\nThe complete workflow consists of the following steps:\n\n### Step 1: Configure the Metrics\n\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\n\n### Step 2: Run the Evaluations\n\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\n\n- First, perform data quality, data drift, and prediction drift checks.\n- After receiving delayed labels, run ML quality check results.\n\nYou can backdate your Reports with a custom timestamp.\n\n### Step 3: Upload to the Platform\n\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\n\n### Step 4: Configure the Dashboard\n\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### Step 5: Configure Alerts\n\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts)."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Custom Metrics and Tests Overview\n\nYou can build fully custom Metrics and Tests to handle any column- or dataset-level evaluations, enabling the implementation of business metrics, weighted scores, etc."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}], "tool_call_id": "call_OtisEBhC2lBXGaQprGzn7ux4", "metadata": null, "timestamp": "2025-12-28T08:04:59.610632Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:59.610874Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "5c45e85b-5ea8-4f47-9122-cb36778c5dad", "metadata": null}, {"parts": [{"content": "To establish a custom metric for evaluating a specific model's performance, you can follow these steps based on the documentation from Evidently AI:\n\n1. **Implement the Metric Calculation Method**: This is a required step where you define how the custom metric will be calculated. This involves coding the logic that determines the performance based on your criteria.\n\n2. **Define Default Test Conditions (Optional)**: You can set up default parameters for the tests that will be applied when using the metric. This step allows you to personalize the evaluations further.\n\n3. **Create a Custom Visualization (Optional)**: If needed, you can use visualization tools like Plotly to create graphs that represent the metric visually, assisting in better interpretation of the results.\n\n4. **Test Your Custom Metric**: After implementing your metric, it\u2019s important to run it against a dataset to validate its performance. This can involve creating a sample dataset to ensure that the metric behaves as expected.\n\n5. **Reference the Metric in Reports**: Post validation, you can include your custom metric in reports for monitoring and tracking model performance over time.\n\n6. **Monitor Over Time**: Utilize dashboards to continuously evaluate the performance of the model, enabling adjustments based on the custom metrics you've defined.\n\nThese steps facilitate the creation and implementation of metrics tailored to specific evaluation needs, which can include business metrics or other weighted scores.\n\nFor more detailed information on customizing metrics, refer to the documentation on creating a [Custom Metric](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_metric.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1521, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 314, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:07.351218Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:59Z"}, "provider_response_id": "chatcmpl-Crg3nFEEC9LK3GhKRx15CO4FkEOU6", "finish_reason": "stop", "run_id": "5c45e85b-5ea8-4f47-9122-cb36778c5dad", "metadata": null}], "source": "ai-generated"}