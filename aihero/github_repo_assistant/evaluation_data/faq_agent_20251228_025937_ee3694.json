{"agent_name": "faq_agent", "system_prompt": ["\nYou are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.\n"], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What evaluation metrics are relevant for image data?", "timestamp": "2025-12-28T02:59:25.689027Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T02:59:25.689240Z", "instructions": "You are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluation metrics for image data\"}", "tool_call_id": "call_ToizT2FxIdhyaM1ttJ55uKyJ", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 110, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 19, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T02:59:27.400461Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T02:59:26Z"}, "provider_response_id": "chatcmpl-CrbI6ILWBQmSmeFljdmwHseSLmAqS", "finish_reason": "tool_call", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Benefits of Synthetic Data\n\nSynthetic data is not a replacement for real data or expert-designed tests \u2014 it\u2019s a way to add variety and speed up the process. With synthetic data you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses.\n\nIt\u2019s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Application in Complex AI Systems\n\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "## Use Cases for Synthetic Test Inputs\n\nEvidently Cloud can be utilized for multiple purposes, including:\n\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\n* **Regression Testing**: Validate changes in your AI system before deployment.\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\n\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"filename": "docs-main/images/metrics/readme.md", "section": "## Evaluations and Metrics Overview\n\nThis section provides an introduction to evaluations and metrics used in assessing various processes or systems. It outlines the importance of these metrics in understanding performance and effectiveness."}, {"title": "Leftovers", "description": "Description of your new file.", "noindex": true, "filename": "docs-main/docs/library/leftover_content.mdx", "section": "## Metrics\n\nOverview of metrics used in data analysis, including correlation checks and data quality assessments."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}], "tool_call_id": "call_ToizT2FxIdhyaM1ttJ55uKyJ", "metadata": null, "timestamp": "2025-12-28T02:59:27.469223Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T02:59:27.469539Z", "instructions": "You are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"image classification evaluation metrics\"}", "tool_call_id": "call_ts11A2QNPpDqcF0vFpkP0RFF", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 1438, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 18, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T02:59:29.184986Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T02:59:28Z"}, "provider_response_id": "chatcmpl-CrbI8ZXVf1qS9sZjPJ6gepYh31BdA", "finish_reason": "tool_call", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Classification Quality by Feature\n\nIn this table, we show a number of plots for each feature. To expand the plots, click on the feature name.\n\n![Classification Quality by Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2)\n\nIn the tab \u201cALL\u201d, you can see the distribution of classes against the values of the feature. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target.\n\n![Classification Quality by Feature Example](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-84fc78dd31a5732be416239154f2c90515d78c42%252Fprob_class_perf_classification_quality_by_feature_example_all.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8e13fd88&sv=2)\n\nFor each class, you can see the predicted probabilities alongside the values of the feature.\n\n![Classification Quality by Feature Example Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2)\n\nIt visualizes the regions where the model makes errors of each type and reveals the low-performance segments. You can compare the distributions and see **if the errors are sensitive to the values of a given feature**."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Confusion Matrix\n\nVisualizes the classification errors and their type.\n\n![Confusion Matrix](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Quality Metrics by Class\n\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## ROC Curve\n\nROC Curve (**receiver operating characteristic curve**) shows the share of true positives and true negatives at different classification thresholds.\n\n![ROC Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-756dcacc9d462ad45745c6bb8f6dd23c5a18d9d2%252Fprob_class_perf_roc.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=caee6451&sv=2)"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Precision-Recall Curve\n\nThe **precision**-**recall curve** shows the trade-off between **precision** and **recall** for different classification thresholds.\n\n![Precision-Recall Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2)"}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Leftovers", "description": "Description of your new file.", "noindex": true, "filename": "docs-main/docs/library/leftover_content.mdx", "section": "## Classification Metrics\n\nMetrics related to classification tasks.\n\n| Metric                                    | Description                                                                                                                                                               | Parameters                                                                                                                                               | Test Defaults                                                                                                      |\n|"}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "## Classification\n\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\n\n### Additional Info\n\n[Data definition](/docs/library/data_definition). You may need to map prediction, target columns and classification type.\n\n### General\n\nUse for binary classification and aggregated results for multi-class.\n\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                                                                       | Test Defaults                                                                                                                                                                        |\n|"}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Overview\n\nThe `ClassificationPreset` allows you to evaluate and visualize the performance on classification tasks, whether binary or multi-class. You can run this Report either for a single dataset or compare it against a reference dataset (such as past performance or a different model/prompt).\n\n### Metrics and Visualizations\n\n* **Various metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\n* **Various visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\n\nAdditionally, if you include feature columns, the Report will show Classification Quality by column, reflecting how the system performs on different data segments."}], "tool_call_id": "call_ts11A2QNPpDqcF0vFpkP0RFF", "metadata": null, "timestamp": "2025-12-28T02:59:29.246233Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T02:59:29.246556Z", "instructions": "You are a helpful assistant to read and understand technical docs for evidently ai. \n\nUse the search tool to find relevant information from the data before answering questions.\n\nIf you can find specific information through search, use it to provide accurate answers.\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}, {"parts": [{"content": "When evaluating image data, particularly in the context of classification tasks, several key evaluation metrics are commonly used to assess the performance of models. Here are the relevant metrics:\n\n1. **Accuracy**: Measures the overall correctness of the model across all classes.\n   \n2. **Precision**: Indicates the proportion of true positive results in relation to the total predicted positives. It helps understand how many of the predicted classes were actually correct.\n\n3. **Recall (Sensitivity)**: Measures the proportion of true positives identified by the model compared to the actual positives. It shows how well the model can identify actual instances of a class.\n\n4. **F1 Score**: The harmonic mean of precision and recall, providing a single metric that balances both concerns, which is particularly useful in scenarios where class distribution is uneven.\n\n5. **ROC Curve**: The Receiver Operating Characteristic curve visualizes the true positive rate against the false positive rate at various threshold settings, aiding in understanding model performance across different discrimination thresholds.\n\n6. **AUC (Area Under the Curve)**: The area under the ROC curve which summarizes the performance of the model across all thresholds. A value closer to 1 indicates better model performance.\n\n7. **Log Loss**: Measures the uncertainty of the model's predictions, penalizing false classifications, and is used to evaluate probabilistic outputs.\n\n8. **Confusion Matrix**: A visual representation that allows you to see the true positive, false positive, true negative, and false negative counts for each class, helping to diagnose specific areas where the model performs poorly.\n\n9. **Precision-Recall Curve**: Similar to the ROC curve, this visualization helps in understanding the trade-off between precision and recall for various thresholds. \n\n10. **Quality Metrics by Class**: These metrics show individual class performance, which is especially important in multi-class problems where some classes may be harder to classify than others.\n\nThese metrics provide a comprehensive view of an image classification model's performance and can help guide improvements and adjustments.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 3555, "cache_write_tokens": 0, "cache_read_tokens": 1408, "output_tokens": 403, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T02:59:37.149937Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T02:59:30Z"}, "provider_response_id": "chatcmpl-CrbIAar7i10ls9PUYsq2aENrKPJqP", "finish_reason": "stop", "run_id": "4536f51d-c013-445f-97ae-b82abf80c7bf", "metadata": null}], "source": "user"}