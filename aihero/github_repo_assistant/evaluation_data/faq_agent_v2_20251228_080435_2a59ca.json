{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How can visualizations enhance the understanding of model performance during evaluations?", "timestamp": "2025-12-28T08:04:28.342412Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:28.342517Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "ab37ebd0-9177-4c58-8517-bef45589049a", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"visualizations enhance understanding of model performance evaluations\"}", "tool_call_id": "call_24ULaoAtgh436T046tKFne8T", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 169, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 22, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:29.256869Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:28Z"}, "provider_response_id": "chatcmpl-Crg3Is7ZLnv8ZPeNK3Pq2gZpXtc7Q", "finish_reason": "tool_call", "run_id": "ab37ebd0-9177-4c58-8517-bef45589049a", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## Evaluations in the Evidently Library\n\nEvaluations are a core feature of the Evidently library. It offers both a catalog of 100+ evals and a framework to easily configure yours. Before exploring, make sure to know the core workflow: try an example for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml)."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## Popular Links\n\n### LLM Judges\n\nThis section covers how to create a custom LLM judge, offering insights and guidelines for those looking to extend their language model evaluations."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Descriptors\n\nThis section provides access to text and LLM evaluations. You can explore various descriptors that are available within the library."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "### Data Drift\n\nHere, you will learn how to customize data drift detection. This is essential for maintaining the relevance and accuracy of models as their underlying data may change over time."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Predicted vs Actual per Group\n\nPrediction plots visualize the regions where the model underestimates or overestimates the target function, segmented into \"overestimation\" and \"underestimation\" groups."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"filename": "docs-main/images/metrics/readme.md", "section": "## Visual Representations\n\nThis section is dedicated to discussing images and visual aids used in evaluations and metrics documentation. It emphasizes the role of graphics in interpreting data."}], "tool_call_id": "call_24ULaoAtgh436T046tKFne8T", "metadata": null, "timestamp": "2025-12-28T08:04:29.289075Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:29.289423Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "ab37ebd0-9177-4c58-8517-bef45589049a", "metadata": null}, {"parts": [{"content": "Visualizations play a crucial role in enhancing the understanding of model performance during evaluations in several ways:\n\n1. **Interactive Analysis**: Visualizations can help users interactively explore model evaluation metrics. For instance, they allow stakeholders to see where a model makes mistakes and can suggest areas for improvement. This is particularly useful for analyzing classification metrics such as accuracy, precision, and recall, where interactive visualizations from frameworks like Evidently enable a deeper dive into model performance [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).\n\n2. **Visualizing Predictions vs Actual Outcomes**: In regression evaluations, visualizations help display the predicted versus actual outcomes. By plotting these predictions, one can easily identify regions where the model underestimates or overestimates values, assisting in pinpointing specific areas that may need model adjustments [source](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_regression.mdx).\n\n3. **Clarity with Metrics**: Visual aids in the metrics documentation clarify complex data and performance indicators. Such representations can facilitate understanding, making it easier for stakeholders to grasp the evaluation results at a glance [source](https://github.com/evidentlyai/docs/tree/main/docs-main/images/metrics/readme.md).\n\nOverall, by employing effective visualizations, one can significantly improve comprehension and communication regarding model performance, leading to informed decision-making and more effective model tuning.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1003, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 296, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:35.918476Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:29Z"}, "provider_response_id": "chatcmpl-Crg3JWXNOMGQKZOPvcDdmFXNMEXUM", "finish_reason": "stop", "run_id": "ab37ebd0-9177-4c58-8517-bef45589049a", "metadata": null}], "source": "ai-generated"}