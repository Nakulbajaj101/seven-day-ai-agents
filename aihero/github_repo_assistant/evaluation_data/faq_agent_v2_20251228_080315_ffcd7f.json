{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What challenges can arise during the monitoring of large language models (LLMs)?", "timestamp": "2025-12-28T08:03:07.746265Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:03:07.746482Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "713dd8db-dee3-489b-a8e2-bbdf5af9afd6", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"challenges monitoring large language models\"}", "tool_call_id": "call_SMxIHmSu23tSutOdyy0ZZXLj", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 171, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 20, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:08.460325Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:03:07Z"}, "provider_response_id": "chatcmpl-Crg1zbHUemd9avJVgyvE9FSASXP1J", "finish_reason": "tool_call", "run_id": "713dd8db-dee3-489b-a8e2-bbdf5af9afd6", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Sample Models\n\nHere are some models you can call using the `HuggingFace()` descriptor.\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Imports\n\nTo use the HuggingFace models for text scoring, you need to import the necessary libraries:\n\n```python\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Built-in ML Evaluators\n\n<Tip>\n**Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\n</Tip>\n\nThere are built-in evaluators for some models. You can call them like any other descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \n])\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "|\n| **Emotion classification** <br /> <ul><li> Scores texts by 28 emotions. </li><li> Returns the predicted probability for the chosen emotion label. </li><li> Scale: 0 to 1. </li><li>[HuggingFace Model](https://huggingface.co/SamLowe/roberta-base-go_emotions) </li></ul>                                                                                                                                                                                                                                              | `HuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")`                                        | **Required**:<ul><li> `params={\"label\":\"label\"}`</li></ul>**Available labels**:<ul><li>admiration</li><li>amusement</li><li>anger</li><li>annoyance</li><li>approval</li><li>caring</li><li>confusion</li><li>curiosity</li><li>desire</li><li>disappointment</li><li>disapproval</li><li>disgust</li><li>embarrassment</li><li>excitement</li><li>fear</li><li>gratitude</li><li>grief</li><li>joy</li><li>love</li><li>nervousness</li><li>optimism</li><li>pride</li><li>realization</li><li>relief</li><li>remorse</li><li>sadness</li><li>surprise</li><li>neutral</li></ul>**Optional**:<ul><li>`alias=\"name\"`</li></ul> |\n| **Zero-shot classification** <br /><ul><li>A natural language inference model. </li><li>Use it for zero-shot classification by user-provided topics.</li><li> List candidate topics as `labels`. You can provide one or several topics. </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Returns a label. </li><li>[HuggingFace Model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)</li></ul> | `HuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels\": [\"science\", \"physics\"], \"threshold\":0.5}, alias=\"Topic\")` | **Required**: <ul><li>`params={\"labels\": [\"label\"]}`</li></ul> **Optional**:<ul><li> `params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| **GPT-2 text detection** <br /><ul><li> Predicts if a text is Real or Fake (generated by a GPT-2 model). </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Note that it is not usable as a detector for more advanced models like ChatGPT.  </li><li> Returns a label. </li><li> [HuggingFace Model](https://huggingface.co/openai-community/roberta-base-openai-detector)  </li></ul>                                             | `HuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")`                                        | **Optional**:<ul><li>`params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n\nThis list is not exhaustive, and the Descriptor may support other models published on Hugging Face. The implemented interface generally works for models that:\n\n* Output a single number (e.g., predicted score for a label) or a label, **not** an array of values.\n* Can process raw text input directly.\n* Name labels using\u00a0`label`\u00a0or\u00a0`labels`\u00a0fields.\n* Use methods named\u00a0`predict`\u00a0or\u00a0`predict_proba`\u00a0for scoring.\n\nHowever, since each model is implemented differently, we cannot provide a complete list of models with a compatible interface. We suggest testing the implementation on your own using trial and error. If you discover useful models, feel free to share them with the community in Discord. You can also open an issue on GitHub to request support for a specific model."}, {"title": "LLM regression testing", "description": "How to run regression testing for LLM outputs.", "filename": "docs-main/examples/llm_regression_testing.mdx", "section": "## Introduction\n\nIn this tutorial, you will learn how to perform regression testing for LLM (Large Language Model) outputs, comparing new and old responses to identify significant changes after system updates."}, {"title": "LLM evaluations", "description": "Run different LLM evaluation methods.", "noindex": "true", "filename": "docs-main/examples/llm_evals.mdx", "section": "## LLM Evaluation Methods\n\nThis section provides an overview of various methods for evaluating Large Language Models (LLMs). It breaks down into three parts:\n\n- **Part 1**: Anatomy of a Single Evaluation  \n  Covers the basic LLM evaluation API and the setup required for evaluation.\n\n- **Part 2**: Reference-Based Evaluation  \n  Discusses methods like exact match, semantic similarity, BERTScore, and LLM judge.\n\n- **Part 3**: Reference-Free Evaluation  \n  Examines techniques such as text statistics, regex, machine learning models, LLM judges, and session-level evaluators.\n\n**Resources**:  \n- [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb)\n\n**Videos**:  \n- [Video 1](https://www.youtube.com/watch?v=6JGRdMGbNCI&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=3)  \n- [Video 2](https://www.youtube.com/watch?v=yD20c-KAImE&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=4)  \n- [Video 3](https://www.youtube.com/watch?v=-zoIqOpt2DA&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=5)"}, {"title": "RAG evals", "description": "Metrics to evaluate a RAG system.", "filename": "docs-main/examples/llm_rag_evals.mdx", "section": "## What's Next?\n\nConsider implementing [regression testing](/examples/LLM_regression_testing) to monitor how your RAG system's retrieval and response quality change over time."}, {"title": "Overview", "description": "How production AI quality monitoring works.", "filename": "docs-main/docs/platform/monitoring_overview.mdx", "section": "## Batch Monitoring Jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**: Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow, running at regular intervals or triggered by new data.\n\n* **Run metric calculations**: Implement the evaluation step using the Evidently Python library. Select evaluations and compute the `Reports` summarizing data, metrics, and test results.\n\n* **Store and visualize the results**: Store Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**: This setup does not store raw data or model predictions unless chosen. It retains aggregated data summaries and test results, protecting data privacy.\n\n* **Full control over the evaluation pipeline**: You decide when evaluations occur, making it great for batch ML models. You can log predictions and set up separate monitoring jobs for queries.\n\n* **Fits most ML evaluation scenarios**: Many evaluations, such as data drift detection, work in batches, requiring a set of new data points. Analyzing shifts in predictions or user behavior is also more meaningful at intervals.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch)."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## Popular Links\n\n### LLM Judges\n\nThis section covers how to create a custom LLM judge, offering insights and guidelines for those looking to extend their language model evaluations."}], "tool_call_id": "call_SMxIHmSu23tSutOdyy0ZZXLj", "metadata": null, "timestamp": "2025-12-28T08:03:08.497283Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:03:08.497535Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "713dd8db-dee3-489b-a8e2-bbdf5af9afd6", "metadata": null}, {"parts": [{"content": "Monitoring large language models (LLMs) can present several challenges, including:\n\n1. **Dynamic Outputs**: LLMs often generate outputs that can vary significantly with slight changes in input or model parameters. This variability can complicate evaluation and monitoring efforts, especially in ensuring that the model maintains consistency over time.\n\n2. **Evaluation Complexity**: Different evaluation methods such as reference-based (e.g., exact match, semantic similarity) and reference-free evaluations (e.g., analyzing text statistics, using regex) add complexity. Selecting the appropriate metrics to capture the model\u2019s performance accurately can be challenging, as the best method may depend on the specific use case and objectives.\n\n3. **Regulatory and Ethical Concerns**: Monitoring LLMs for bias and ensuring fairness in their outputs is crucial. These models may unintentionally propagate biases present in the training data, necessitating systematic checks and balances over their output.\n\n4. **System Updates**: Any updates or changes to the model can lead to unexpected changes in output behavior. Regression testing is often necessary to identify significant changes after an update, which can require substantial resources and expertise.\n\n5. **Long-Term Tracking**: Implementing a consistent evaluation pipeline to track model performance over time can be complicated. It requires scheduling regular evaluations and possibly integrating these efforts with larger machine learning workflows.\n\n6. **Data Privacy**: When monitoring outputs, especially in production environments, ensuring user privacy while evaluating model performance poses a significant challenge. Aggregated data approaches may help, but they also require careful planning and implementation.\n\nFor a successful monitoring strategy, it\u2019s important to establish clear evaluation goals, choose appropriate metrics, and set up robust evaluation pipelines to assess model performance consistently. These efforts can also benefit from community-supported frameworks and tools that streamline the process [LLM Evaluation Methods](https://github.com/evidentlyai/docs/tree/main/docs-main/examples/llm_evals.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2756, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 385, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:15.223686Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:03:08Z"}, "provider_response_id": "chatcmpl-Crg20S2fhSS8uTZsdqAPiEF1PYYqa", "finish_reason": "stop", "run_id": "713dd8db-dee3-489b-a8e2-bbdf5af9afd6", "metadata": null}], "source": "ai-generated"}