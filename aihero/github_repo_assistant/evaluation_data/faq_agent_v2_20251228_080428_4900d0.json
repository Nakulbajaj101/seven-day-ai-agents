{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What is the role of confusion matrices in evaluating machine learning models?", "timestamp": "2025-12-28T08:04:21.996905Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:21.997136Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "abbf01b9-69aa-494f-99a0-18cf7eaf9881", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"confusion matrix role in evaluating machine learning models\"}", "tool_call_id": "call_fMmI06EByrgyMh4IXmcXmBEg", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 169, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 23, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:22.910484Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:22Z"}, "provider_response_id": "chatcmpl-Crg3Cl5U2lZAE8L7ay8zPMcZhzB3d", "finish_reason": "tool_call", "run_id": "abbf01b9-69aa-494f-99a0-18cf7eaf9881", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Sample Models\n\nHere are some models you can call using the `HuggingFace()` descriptor.\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Built-in ML Evaluators\n\n<Tip>\n**Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\n</Tip>\n\nThere are built-in evaluators for some models. You can call them like any other descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \n])\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Imports\n\nTo use the HuggingFace models for text scoring, you need to import the necessary libraries:\n\n```python\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "|\n| **Emotion classification** <br /> <ul><li> Scores texts by 28 emotions. </li><li> Returns the predicted probability for the chosen emotion label. </li><li> Scale: 0 to 1. </li><li>[HuggingFace Model](https://huggingface.co/SamLowe/roberta-base-go_emotions) </li></ul>                                                                                                                                                                                                                                              | `HuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")`                                        | **Required**:<ul><li> `params={\"label\":\"label\"}`</li></ul>**Available labels**:<ul><li>admiration</li><li>amusement</li><li>anger</li><li>annoyance</li><li>approval</li><li>caring</li><li>confusion</li><li>curiosity</li><li>desire</li><li>disappointment</li><li>disapproval</li><li>disgust</li><li>embarrassment</li><li>excitement</li><li>fear</li><li>gratitude</li><li>grief</li><li>joy</li><li>love</li><li>nervousness</li><li>optimism</li><li>pride</li><li>realization</li><li>relief</li><li>remorse</li><li>sadness</li><li>surprise</li><li>neutral</li></ul>**Optional**:<ul><li>`alias=\"name\"`</li></ul> |\n| **Zero-shot classification** <br /><ul><li>A natural language inference model. </li><li>Use it for zero-shot classification by user-provided topics.</li><li> List candidate topics as `labels`. You can provide one or several topics. </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Returns a label. </li><li>[HuggingFace Model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)</li></ul> | `HuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels\": [\"science\", \"physics\"], \"threshold\":0.5}, alias=\"Topic\")` | **Required**: <ul><li>`params={\"labels\": [\"label\"]}`</li></ul> **Optional**:<ul><li> `params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| **GPT-2 text detection** <br /><ul><li> Predicts if a text is Real or Fake (generated by a GPT-2 model). </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Note that it is not usable as a detector for more advanced models like ChatGPT.  </li><li> Returns a label. </li><li> [HuggingFace Model](https://huggingface.co/openai-community/roberta-base-openai-detector)  </li></ul>                                             | `HuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")`                                        | **Optional**:<ul><li>`params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n\nThis list is not exhaustive, and the Descriptor may support other models published on Hugging Face. The implemented interface generally works for models that:\n\n* Output a single number (e.g., predicted score for a label) or a label, **not** an array of values.\n* Can process raw text input directly.\n* Name labels using\u00a0`label`\u00a0or\u00a0`labels`\u00a0fields.\n* Use methods named\u00a0`predict`\u00a0or\u00a0`predict_proba`\u00a0for scoring.\n\nHowever, since each model is implemented differently, we cannot provide a complete list of models with a compatible interface. We suggest testing the implementation on your own using trial and error. If you discover useful models, feel free to share them with the community in Discord. You can also open an issue on GitHub to request support for a specific model."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Confusion Matrix\n\nVisualizes the classification errors and their type.\n\n![Confusion Matrix](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2)"}, {"title": "Data and ML checks", "description": "Run a simple evaluation for tabular data", "filename": "docs-main/quickstart_ml.mdx", "section": "## Overview of Evidently\n\nEvidently helps you run tests and evaluations for your production ML systems. This includes:\n- Evaluating prediction quality (e.g. classification or regression accuracy)\n- Input data quality (e.g. missing values, out-of-range features)\n- Data and prediction drift."}, {"title": "Data definition", "description": "How to map the input data.", "filename": "docs-main/docs/library/data_definition.mdx", "section": "## Evaluations Overview\n\nTo run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\n\n- **Column types** (e.g., categorical, numerical, text).\n- **Column roles** (e.g., id, prediction, target).\n\nThis structure allows Evidently to process the data correctly, ensuring specific columns required for evaluations are present."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates several standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). For each quality metric, Evidently also shows one standard deviation of its value to estimate the stability of the performance. \n\nTo support model performance analysis, Evidently generates interactive visualizations, helping to analyze where the model makes mistakes and to ideate on improvements."}], "tool_call_id": "call_fMmI06EByrgyMh4IXmcXmBEg", "metadata": null, "timestamp": "2025-12-28T08:04:22.950237Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:22.950493Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "abbf01b9-69aa-494f-99a0-18cf7eaf9881", "metadata": null}, {"parts": [{"content": "Confusion matrices play an important role in evaluating machine learning models, particularly in classification tasks. They provide a visualization of the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions. Below are the key aspects and benefits of using confusion matrices:\n\n1. **Error Visualization**: A confusion matrix enables the visualization of classification errors and their types. This helps in understanding not just the overall accuracy of the model but also the specific areas where it performs poorly. For instance, it reveals whether a model is better at identifying one class over another.\n\n2. **Multiple Metrics Derivation**: From a confusion matrix, various evaluation metrics can be easily derived, such as accuracy, precision, recall, and F1 score. This helps in assessing different aspects of model performance beyond accuracy alone. \n\n3. **Class Imbalance Understanding**: In cases of class imbalance, where the number of instances in various classes differs significantly, a confusion matrix helps clarify how the model performs across these classes, allowing for better assessment and adjustment.\n\n4. **Model Comparison**: By comparing the confusion matrices of different models, one can identify which model has better performance characteristics and make more informed decisions based on those insights.\n\nOverall, confusion matrices provide critical insights that assist in model diagnostics, allowing practitioners to fine-tune models for better performance.\n\nFor additional information, you can refer to the following source: [Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2355, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 313, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:28.340328Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:23Z"}, "provider_response_id": "chatcmpl-Crg3DcWhZdzzdrDfuEk631LJZuAuL", "finish_reason": "stop", "run_id": "abbf01b9-69aa-494f-99a0-18cf7eaf9881", "metadata": null}], "source": "ai-generated"}