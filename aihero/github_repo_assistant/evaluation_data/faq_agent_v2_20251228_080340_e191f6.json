{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What are the best practices for setting up an evaluation pipeline for text data models?", "timestamp": "2025-12-28T08:03:27.001440Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:03:27.001591Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "b0c73d14-fbd9-490a-9b0b-664e0e75ef30", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"best practices evaluation pipeline text data models\"}", "tool_call_id": "call_8Vt4BQM5RqYAGdzXAzT1fYew", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 172, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:28.739532Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:03:28Z"}, "provider_response_id": "chatcmpl-Crg2KsHlPyC0iQXcFGag2e07ubcO8", "finish_reason": "tool_call", "run_id": "b0c73d14-fbd9-490a-9b0b-664e0e75ef30", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Creating Test Datasets\n\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don\u2019t reflect your specific use case. Manually writing test cases takes time and effort."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "## Use Cases for Synthetic Test Inputs\n\nEvidently Cloud can be utilized for multiple purposes, including:\n\n* **Experiments**: Create test data to see how your LLM application handles various inputs.\n* **Regression Testing**: Validate changes in your AI system before deployment.\n* **Adversarial Testing**: Assess how your system manages tricky or unexpected inputs.\n\nOnce the data is generated, you can evaluate the results using the Evidently Cloud interface or the Evidently Python library."}, {"title": "Synthetic data", "description": "Generating test cases and datasets.", "filename": "docs-main/synthetic-data/introduction.mdx", "section": "### RAG Dataset\n\nYou can generate a Q&A dataset from the knowledge source to enhance information retrieval and response accuracy. [Learn more](https://www.evidentlyai.com/synthetic-data/rag_data)."}, {"title": "Overview", "description": "Introduction to Datasets.", "filename": "docs-main/docs/platform/datasets_overview.mdx", "section": "## When Do You Need Datasets?\n\nHere are common use cases for datasets in Evidently:\n\n- **Organize evaluation datasets**: Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in the UI and access them programmatically for CI/CD checks.\n- **Debug evaluation results**: After you run an evaluation, view the dataset to identify and debug specific failures. For example, you can sort all text outputs by added scores.\n- **Store ML inference logs or LLM traces**: Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it."}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "Text Evals", "description": "Overview of the Text Evals Preset.", "filename": "docs-main/metrics/preset_text_evals.mdx", "section": "## Overview of TextEvals\n\nThe `TextEvals` is a utility Preset that summarizes results of all **descriptors** (output-level text evaluations) computed on your dataset. It allows visual exploration of distributions and captures relevant statistics, varying based on descriptor type. If you pass two datasets, you'll receive a side-by-side comparison.\n\n<Info>\n  **How text and LLM evaluations work.** Read about [Descriptors](/docs/library/descriptors), or try a [Quickstart](/quickstart_llm).\n</Info>\n\n![](/images/metrics/preset_text_evals-min.gif)"}, {"title": "No code evals", "description": "How to evaluate your data in a no-code interface.", "filename": "docs-main/docs/platform/evals_no_code.mdx", "section": "## Configuring the Evaluation\n\nYou must choose which column to evaluate and how. You can choose from the following methods:\n\n- **Model-based**: Use built-in machine learning models, like sentiment analysis.\n- **Regular expressions**: Check for specific words or patterns.\n- **Text stats**: Measure stats like the number of symbols or sentences.\n- **LLM-based**: Use external LLMs to evaluate your text data.\n\nSelect specific checks one by one. Each evaluation result is called a **Descriptor**. No matter the method, you\u2019ll get a label or score for every evaluated text.\n\n<Note>\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\n</Note>\n\n### Descriptor Examples\n\nHere are a few examples of Descriptors and how to configure them:\n\n#### Words Presence\n\n**Include Words**: This Descriptor checks for listed words and returns \"True\" or \"False.\"\n\nSet up these parameters:\n\n- Add a list of words.\n- Choose whether to check for \u201cany\u201d or \u201call\u201d of the words present.\n- Set the **lemmatize** parameter to check for inflected and variant words automatically.\n- Give your check a name so you can easily find it in your results.\n\n#### Semantic Similarity\n\n**Semantic Similarity**: This descriptor converts texts to embeddings and calculates Cosine Similarity between your evaluated column and another column. It scores from 0 to 1 (0: completely different, 0.5: unrelated, 1: identical).\n\n#### LLM as a Judge\n\n**Custom LLM evaluator**: If you've added your token, use LLM-based evaluations (built-in or custom) to send your texts to LLMs for grading or scoring.\n\nFor example, you can configure:\n\n- **Criteria**: Define custom criteria in free text.\n- **Target/Non-target Category**: Labels you want to use.\n- **Uncertain Category**: How the model should respond when it can\u2019t decide.\n- **Reasoning**: Include explanation (Recommended).\n- **Category and/or Score**: Decide what the LLM should respond with.\n- **Visualize as**: Select what to display in the Report."}, {"title": "Configure LLM Judges", "description": "How to run prompt-based evaluators for custom criteria.", "filename": "docs-main/metrics/customize_llm_judge.mdx", "section": "## LLM-based Descriptors\n\nLLM-based descriptors use an external LLM for evaluation. You can:\n\n- Use built-in evaluators (with pre-written prompts), or\n- Configure custom criteria using templates.\n\n**Pre-requisites**:\n\n- You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}], "tool_call_id": "call_8Vt4BQM5RqYAGdzXAzT1fYew", "metadata": null, "timestamp": "2025-12-28T08:03:28.775601Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:03:28.775863Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "b0c73d14-fbd9-490a-9b0b-664e0e75ef30", "metadata": null}, {"parts": [{"content": "Setting up an evaluation pipeline for text data models involves several best practices to ensure effective assessments and improvements. Here are some key strategies:\n\n1. **Define the Evaluation Objectives**: Clearly specify what aspects of your model you want to evaluate, such as accuracy, consistency, and robustness. This includes identifying the evaluation criteria relevant to your specific use case.\n\n2. **Create Diverse Test Datasets**: Utilize a mix of real, synthetic, and historical datasets to ensure comprehensive coverage of potential inputs. Synthetic datasets are particularly useful for generating edge cases and addressing any situational deficits in real data. They can be created using templates or algorithms to simulate various scenarios that the model may encounter in practice ([docs-main/synthetic-data/why_synthetic.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/synthetic-data/why_synthetic.mdx)).\n\n3. **Utilize Evaluation Metrics**: Implement various metrics tailored to text evaluation. These can include machine learning model-based metrics like accuracy and precision, as well as natural language processing (NLP)-specific measures such as BLEU scores, ROUGE, or custom scoring methods based on semantic similarity ([docs-main/metrics/customize_hf_descriptor.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_hf_descriptor.mdx)). \n\n4. **Incorporate Human Feedback**: In addition to automated evaluation, consider integrating human judgments to provide insights into model performance, particularly in cases where subjective criteria are important.\n\n5. **Debugging and Continuous Improvement**: Post-evaluation, analyze the results to identify failures or weaknesses in the model. Use insights from the evaluation to iteratively improve the model by refining data inputs, training techniques, or feature engineering ([docs-main/docs/platform/datasets_overview.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/datasets_overview.mdx)).\n\n6. **Implement Evaluation Pipelines**: Automated evaluation pipelines streamline the process of executing and reporting evaluations. Ensure that the pipeline can collect logs, run checks, and generate reports efficiently, possibly using frameworks that allow for CI/CD integrations.\n\n7. **Visualize Results**: Utilize visualization tools to explore the evaluation results, which help in understanding distributions and identifying trends in performance metrics ([docs-main/metrics/preset_text_evals.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/preset_text_evals.mdx)). \n\n8. **Integration of LLMs**: Consider using Language Models (LLMs) as evaluators in your pipeline to provide nuanced insights into text performance, which can facilitate richer evaluations through customized prompts and criteria ([docs-main/metrics/customize_llm_judge.mdx](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/customize_llm_judge.mdx)).\n\nBy following these best practices, you can establish a robust evaluation pipeline that not only assesses model performance effectively but also equips you with insights for continuous refinement and enhancement of your text data models.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2219, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 619, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:40.003489Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:03:29Z"}, "provider_response_id": "chatcmpl-Crg2Lj2R4EBHNsWAf2EquDdHIjrx2", "finish_reason": "stop", "run_id": "b0c73d14-fbd9-490a-9b0b-664e0e75ef30", "metadata": null}], "source": "ai-generated"}