{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What role does user feedback play in the ongoing evaluation of machine learning models?", "timestamp": "2025-12-28T08:05:56.413765Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:05:56.414135Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "3a34ab61-88d2-410f-a9d1-928b93e8b74f", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"user feedback machine learning models evaluation\"}", "tool_call_id": "call_YzgGUKOxS1jZC0Kc8dS67TII", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 171, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 20, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:57.427570Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:56Z"}, "provider_response_id": "chatcmpl-Crg4iQ2WYxWX1anpGAVp37KTovntG", "finish_reason": "tool_call", "run_id": "3a34ab61-88d2-410f-a9d1-928b93e8b74f", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Sample Models\n\nHere are some models you can call using the `HuggingFace()` descriptor.\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Imports\n\nTo use the HuggingFace models for text scoring, you need to import the necessary libraries:\n\n```python\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Built-in ML Evaluators\n\n<Tip>\n**Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\n</Tip>\n\nThere are built-in evaluators for some models. You can call them like any other descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \n])\n```"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "|\n| **Emotion classification** <br /> <ul><li> Scores texts by 28 emotions. </li><li> Returns the predicted probability for the chosen emotion label. </li><li> Scale: 0 to 1. </li><li>[HuggingFace Model](https://huggingface.co/SamLowe/roberta-base-go_emotions) </li></ul>                                                                                                                                                                                                                                              | `HuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")`                                        | **Required**:<ul><li> `params={\"label\":\"label\"}`</li></ul>**Available labels**:<ul><li>admiration</li><li>amusement</li><li>anger</li><li>annoyance</li><li>approval</li><li>caring</li><li>confusion</li><li>curiosity</li><li>desire</li><li>disappointment</li><li>disapproval</li><li>disgust</li><li>embarrassment</li><li>excitement</li><li>fear</li><li>gratitude</li><li>grief</li><li>joy</li><li>love</li><li>nervousness</li><li>optimism</li><li>pride</li><li>realization</li><li>relief</li><li>remorse</li><li>sadness</li><li>surprise</li><li>neutral</li></ul>**Optional**:<ul><li>`alias=\"name\"`</li></ul> |\n| **Zero-shot classification** <br /><ul><li>A natural language inference model. </li><li>Use it for zero-shot classification by user-provided topics.</li><li> List candidate topics as `labels`. You can provide one or several topics. </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Returns a label. </li><li>[HuggingFace Model](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli)</li></ul> | `HuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels\": [\"science\", \"physics\"], \"threshold\":0.5}, alias=\"Topic\")` | **Required**: <ul><li>`params={\"labels\": [\"label\"]}`</li></ul> **Optional**:<ul><li> `params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| **GPT-2 text detection** <br /><ul><li> Predicts if a text is Real or Fake (generated by a GPT-2 model). </li><li> You can set a classification threshold: if the predicted probability is below, an \"unknown\" label will be assigned. </li><li> Note that it is not usable as a detector for more advanced models like ChatGPT.  </li><li> Returns a label. </li><li> [HuggingFace Model](https://huggingface.co/openai-community/roberta-base-openai-detector)  </li></ul>                                             | `HuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")`                                        | **Optional**:<ul><li>`params={\"score_threshold\": 0.7}` (default: 0.5)</li><li> `alias=\"name\"`</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n\nThis list is not exhaustive, and the Descriptor may support other models published on Hugging Face. The implemented interface generally works for models that:\n\n* Output a single number (e.g., predicted score for a label) or a label, **not** an array of values.\n* Can process raw text input directly.\n* Name labels using\u00a0`label`\u00a0or\u00a0`labels`\u00a0fields.\n* Use methods named\u00a0`predict`\u00a0or\u00a0`predict_proba`\u00a0for scoring.\n\nHowever, since each model is implemented differently, we cannot provide a complete list of models with a compatible interface. We suggest testing the implementation on your own using trial and error. If you discover useful models, feel free to share them with the community in Discord. You can also open an issue on GitHub to request support for a specific model."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Why Evidently?", "description": "Why choose Evidently.", "filename": "docs-main/faq/why_evidently.mdx", "section": "## Built-in Evaluations\n\nEvidently prioritizes evaluations and quality testing. Many tools require you to prepare data and implement metrics from scratch, but we provide **100+ built-in evaluations** for various ML and LLM use cases. From ranking metrics to data drift algorithms, we have streamlined quality assessment. Users can also extend Evidently by adding custom metrics. \n\nEvidently Cloud features advanced testing capabilities, including synthetic data generation and adversarial testing for creating and running test scenarios."}, {"title": "Overview", "description": "Running evals on the platform.", "filename": "docs-main/docs/platform/evals_overview.mdx", "section": "## Evaluations in AI Product Development\n\nYou may need evaluations at different stages of your AI product development, including:\n\n* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs.\n* **Experiments.** Test different parameters, models, or prompts and compare outcomes.\n* **Safety and adversarial testing.** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\n* **Regression testing.** Ensure performance does not degrade after updates or fixes.\n* **Monitoring.** Track the response quality for production systems.\n\nEvidently supports all these workflows, allowing evaluations to be run locally or directly on the platform."}, {"title": "Data and ML checks", "description": "Run a simple evaluation for tabular data", "filename": "docs-main/quickstart_ml.mdx", "section": "## Overview of Evidently\n\nEvidently helps you run tests and evaluations for your production ML systems. This includes:\n- Evaluating prediction quality (e.g. classification or regression accuracy)\n- Input data quality (e.g. missing values, out-of-range features)\n- Data and prediction drift."}], "tool_call_id": "call_YzgGUKOxS1jZC0Kc8dS67TII", "metadata": null, "timestamp": "2025-12-28T08:05:57.460615Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:57.460915Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "3a34ab61-88d2-410f-a9d1-928b93e8b74f", "metadata": null}, {"parts": [{"content": "User feedback plays a crucial role in the ongoing evaluation of machine learning models in several ways:\n\n1. **Performance Monitoring**: User feedback helps track the quality of model predictions, particularly in production environments. Continuous feedback indicates how well the model performs in real-world situations, allowing developers to identify areas needing improvement [source: overview](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\n2. **Model Adaptation and Improvement**: Incorporating user feedback allows models to be fine-tuned over time. This might involve retraining the models with new data that reflects user input, leading to better accuracy and relevance in the outputs. Such adaptive practices ensure that models remain effective as user needs evolve [source: why_evidently](https://github.com/evidentlyai/docs/tree/main/docs-main/faq/why_evidently.mdx).\n\n3. **Testing and Evaluation Optimization**: Feedback can influence the design of evaluation mechanisms, enabling the identification of metrics that reflect user satisfaction and model effectiveness. Different types of evaluations, including safety checks and regression testing, can be informed by user experiences and input [source: evaluations](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\n4. **Identifying Edge Cases**: User interactions can reveal edge cases or situations that the model may not handle well, helping developers refine the model's robustness against various inputs. This is particularly important in adversarial testing and ensuring overall safety in AI applications [source: evaluations](https://github.com/evidentlyai/docs/tree/main/docs-main/docs/platform/evals_overview.mdx).\n\nOverall, user feedback is integral to creating responsive, effective machine learning systems that align with user expectations and real-world requirements.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2268, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 360, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:06:07.040780Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:05:57Z"}, "provider_response_id": "chatcmpl-Crg4jS65RO9sYvunpm1mR2FSduYMz", "finish_reason": "stop", "run_id": "3a34ab61-88d2-410f-a9d1-928b93e8b74f", "metadata": null}], "source": "ai-generated"}