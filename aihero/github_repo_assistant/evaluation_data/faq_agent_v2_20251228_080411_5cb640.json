{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "What factors should be considered when determining the evaluation metrics for a specific machine learning model?", "timestamp": "2025-12-28T08:04:01.099953Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:01.100153Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "f6338529-aed1-4180-ac05-fbc13f2334a9", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluation metrics factors machine learning model\"}", "tool_call_id": "call_XAxeNfbyJa8ZIaW3aumZt0fS", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 173, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 20, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:01.931450Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:01Z"}, "provider_response_id": "chatcmpl-Crg2r3HONyIZLk1KrisXWqlkJglsK", "finish_reason": "tool_call", "run_id": "f6338529-aed1-4180-ac05-fbc13f2334a9", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates several standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). For each quality metric, Evidently also shows one standard deviation of its value to estimate the stability of the performance. \n\nTo support model performance analysis, Evidently generates interactive visualizations, helping to analyze where the model makes mistakes and to ideate on improvements."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Mean Error per Group\n\nA summary of model quality metrics for specific segments of predictions\u2014those with the highest negative and positive errors\u2014is provided here. The metrics include mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE)."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Quality Metrics by Class\n\nShows the model quality metrics for the individual classes. In the case of multi-class problems, it will also include ROC AUC.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates several standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). For each quality metric, Evidently also shows one standard deviation of its value to estimate the stability of the performance. \n\nTo support model performance analysis, Evidently generates interactive visualizations, helping to analyze where the model makes mistakes and to ideate on improvements."}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "- |\n  | **ClassificationDummyQuality()** | <ul><li>Small Preset summarizing quality of a dummy model.</li><li>Metric result: all Metrics</li></ul> | N/A        | N/A           |\n  | **DummyPrecision()**             | <ul><li>Calculates precision for a dummy model.</li><li>Metric result: `value`.</li></ul>               | N/A        | N/A           |\n  | **DummyRecall()**                | <ul><li>Calculates recall for a dummy model.</li><li>Metric result: `value`.</li></ul>                  | N/A        | N/A           |\n  | **DummyF1()**                    | <ul><li>Calculates F1 Score for a dummy model.</li><li>Metric result: `value`.</li></ul>                | N/A        | N/A           |\n</Accordion>\n\n### By Label\n\nUse when you have multiple classes and want to evaluate quality separately.\n\n| Metric                             | Description                                                                                                                  | Parameters                                                                                                        | Test Defaults                                                                                                                                         |   |\n|"}], "tool_call_id": "call_XAxeNfbyJa8ZIaW3aumZt0fS", "metadata": null, "timestamp": "2025-12-28T08:04:01.962149Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:01.962406Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "f6338529-aed1-4180-ac05-fbc13f2334a9", "metadata": null}, {"parts": [{"content": "When determining the evaluation metrics for a specific machine learning model, several important factors should be considered:\n\n1. **Type of Problem**: The choice of metrics can vary significantly between classification and regression problems. For classification, metrics such as accuracy, precision, recall, F1-score, and ROC AUC are crucial. For regression problems, metrics like Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) are typically used.\n\n2. **Model Goal**: Understanding what you are trying to achieve with the model is vital. For example, if the priority is to minimize false positives, precision may be more critical. Conversely, if minimizing false negatives is essential, recall should be emphasized.\n\n3. **Class Imbalance**: In classification tasks where classes are imbalanced, accuracy may not be a sufficient measure. Metrics such as F1-score or area under the ROC curve (AUC-ROC) can provide a more comprehensive evaluation.\n\n4. **Stakeholder Requirements**: Consider what metrics stakeholders are most interested in. Different stakeholders may prioritize different outcomes, which affects which metrics should be reported.\n\n5. **Robustness of Metrics**: It\u2019s important to consider the stability and interpretability of the metrics you choose. Metrics should effectively convey the model's performance and provide actionable insights, such as identifying model weaknesses or sources of error.\n\n6. **Visualization Support**: Utilizing metrics that can be complemented with visualizations to analyze model performance can provide deeper insights into where the model is making mistakes and how it can be improved.\n\nEvidently calculates standard evaluation metrics and provides interactive visualizations to assist in analyzing model performance. For classification, common metrics include accuracy, precision, recall, F1-score, and ROC AUC. For regression, standard metrics include Mean Error, MAE, and MAPE, along with insights into stability through the standard deviation of these metrics ([Classification metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx), [Regression metrics](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_regression.mdx)). \n\nOverall, these factors help ensure that the evaluation reflects the model's performance appropriately for the specific context of its use.", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 1442, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 462, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:11.955663Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:02Z"}, "provider_response_id": "chatcmpl-Crg2sy8EMxhj0WC3cspXrZrQwI7qv", "finish_reason": "stop", "run_id": "f6338529-aed1-4180-ac05-fbc13f2334a9", "metadata": null}], "source": "ai-generated"}