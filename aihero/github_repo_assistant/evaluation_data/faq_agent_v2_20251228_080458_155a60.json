{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "How does the concept of precision-recall differ in the context of model evaluation?", "timestamp": "2025-12-28T08:04:50.869305Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:04:50.869400Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "bf7c972c-197e-47e0-b3be-3cf5ac8e4ada", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"precision recall model evaluation\"}", "tool_call_id": "call_3TPwfxkHtgQebUQTufx7WwIy", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 172, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 18, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:51.582288Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:04:51Z"}, "provider_response_id": "chatcmpl-Crg3famfAz2E8CB5g1tK7xpS7DMHH", "finish_reason": "tool_call", "run_id": "bf7c972c-197e-47e0-b3be-3cf5ac8e4ada", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 5. Get a Dashboard\n\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You'll be able to observe trends and set up alerts as you log more evaluation results."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 4. Create a Report\n\nTo create and run a report summarizing the evaluation results:\n\n```python\nreport = Report([TextEvals()])\nmy_eval = report.run(eval_dataset, None)\n```\n\nYou can preview the report locally or save it as a JSON, Python dictionary, or HTML file.\n\nTo upload the report to Evidently Cloud:\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```"}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 7. (Optional) Add a Custom LLM Judge\n\nYou can create custom criteria using built-in LLM judge templates, such as classifying questions as \"appropriate\" or \"inappropriate\":\n\n```python\nappropriate_scope = BinaryClassificationPromptTemplate(\n    criteria=\"\"\"An appropriate question is any educational query related to ...\"\"\",\n    target_category=\"APPROPRIATE\",\n    non_target_category=\"INAPPROPRIATE\",\n    include_reasoning=True,\n)\n\nllm_evals = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        LLMEval(\"question\", template=appropriate_scope, provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Question topic\")\n    ]\n)\n```"}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 3. Run Evaluations\n\nYou will evaluate the answers based on:\n\n- **Sentiment:** Score ranges from -1 (negative) to 1 (positive).\n- **Text length:** Measured by character count.\n- **Denials:** Refusals to answer, using an LLM-as-a-judge with a built-in prompt.\n\nSet the OpenAI key as an environment variable for LLM-as-a-judge:\n\n```python\n## import os\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n```\n\nTo run evaluations:\n\n```python\neval_dataset = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\"),\n        TextLength(\"answer\", alias=\"Length\"),\n        DeclineLLMEval(\"answer\", alias=\"Denials\")\n    ]\n)\n```\n\n**Congratulations!** You've successfully run your first evaluation."}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 1. Set Up Your Environment\n\n### 1.1. Set Up Evidently Cloud\n\n<CloudSignup />\n\n### 1.2. Installation and Imports\n\nTo install the Evidently Python library:\n\n```python\n!pip install evidently\n```\n\nComponents needed to run the evaluations:\n\n```python\nimport pandas as pd\nfrom evidently import Dataset, DataDefinition, Report\nfrom evidently.presets import TextEvals\nfrom evidently.tests import lte, gte, eq\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\n```\n\nComponents needed to connect with Evidently Cloud:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n```\n\n### 1.3. Create a Project\n\n<CreateProject />"}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Precision-Recall Curve\n\nThe **precision**-**recall curve** shows the trade-off between **precision** and **recall** for different classification thresholds.\n\n![Precision-Recall Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2)"}, {"title": "Ranking and RecSys metrics", "description": "Open-source metrics for ranking and recommendations.", "filename": "docs-main/metrics/explainer_recsys.mdx", "section": "## Mean Average Recall (MAR)\n\n**Evidently Metric**: `MAR`.\n\nMAR at K assesses the ability of a recommendation system to retrieve all relevant items within the top-K results.\n\n### Implementation\n* **Compute the average recall at K by user**.\n\n$$\\text{AR@K} = \\frac{1}{N} \\sum_{k=1}^{K} Recall(k) \\times rel(k)$$\n\n* **Compute mean average recall at K**.\n\n$$\\text{MAR@K} = \\frac{1}{U} \\sum_{u=1}^{U} \\text{AR@K}_u$$\n\n### Range\n0 to 1.\n\n### Interpretation\nHigher MAR at K values indicate a better ability to retrieve relevant items."}, {"title": "All Metrics", "description": "Reference page for all dataset-level evals.", "filename": "docs-main/metrics/all_metrics.mdx", "section": "|\n| **RecallTopK()**                    | <ul><li>Calculates Recall at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if recall > 0.</li><li>**With reference**: Fails if Recall differs by >10%.</li></ul>                   |\n| **FBetaTopK()**                     | <ul><li>Calculates F-beta score at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                             | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if F-beta > 0.</li><li>**With reference**: Fails if F-beta differs by >10%.</li></ul>                   |\n| **PrecisionTopK()**                 | <ul><li>Calculates Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Precision > 0.</li><li>**With reference**: Fails if Precision differs by >10%.</li></ul>             |\n| **MAP()**                           | <ul><li>Calculates Mean Average Precision at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                   | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MAP > 0.</li><li>**With reference**: Fails if MAP differs by >10%.</li></ul>                         |\n| **NDCG()**                          | <ul><li>Calculates Normalized Discounted Cumulative Gain at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                    | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if NDCG > 0.</li><li>**With reference**: Fails if NDCG differs by >10%.</li></ul>                       |\n| **MRR()**                           | <ul><li>Calculates Mean Reciprocal Rank at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                     | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if MRR > 0.</li><li>**With reference**: Fails if MRR differs by >10%.</li></ul>                         |\n| **HitRate()**                       | <ul><li>Calculates Hit Rate at the top K retrieved items.</li><li>Metric result: `value`.</li></ul>                                                                                 | **Required**: <ul><li>`k`</li></ul> **Optional**: <ul><li>`no_feedback_users`</li><li>`min_rel_score`</li><li>[Test conditions](/docs/library/tests)</li></ul> | <ul><li>**No reference**: Tests if Hit Rate > 0.</li><li>**With reference**: Fails if Hit Rate differs by >10%.</li></ul>               |"}, {"title": "Ranking and RecSys metrics", "description": "Open-source metrics for ranking and recommendations.", "filename": "docs-main/metrics/explainer_recsys.mdx", "section": "## Recall\n\n**Evidently Metric**: `RecallTopK`.\n\nRecall at K reflects the ability of the recommender or ranking system to retrieve all relevant items within the top K results.\n\n### Implementation\n* **Compute recall at K by user**: Measure the share of all relevant items in the dataset appearing in the top K results.\n\n$$\\text{Recall at } K = \\frac{\\text{Number of relevant items in } K}{\\text{Total number of relevant items}}$$\n\n* **Compute overall recall**: Average the results across all users (queries).\n\n### Range\n0 to 1.\n\n### Interpretation\nA higher recall at K indicates that the model retrieves a higher proportion of relevant items."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Precision-Recall Table\n\nThe table shows possible **outcomes for different classification thresholds** and **prediction coverage**. If you have two datasets, the table is generated for both.\n\n![Precision-Recall Table](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-23088b547ed1b7126b1584e67acd2bf42715582b%252Fprob_class_perf_pr_table_current.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2b13494f&sv=2)\n\nEach line in the table defines a case when only *top-X%* predictions are considered, with a 5% step. It shows the absolute number of predictions *(Count)* and the probability threshold *(Prob)* that correspond to this combination. The table then shows the quality metrics for a given combination. It includes *Precision*, *Recall*, the share of *True Positives (TP)*, and *False Positives (FP)*.\n\nThis helps explore the quality of the model if you choose to act only on some of the predictions."}], "tool_call_id": "call_3TPwfxkHtgQebUQTufx7WwIy", "metadata": null, "timestamp": "2025-12-28T08:04:51.622323Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:04:51.622754Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "bf7c972c-197e-47e0-b3be-3cf5ac8e4ada", "metadata": null}, {"parts": [{"content": "In the context of model evaluation, **precision** and **recall** are two important metrics that measure the effectiveness of a classification model, particularly in binary classification tasks.\n\n### Precision\nPrecision refers to the proportion of true positive predictions to the total predicted positives (i.e., the sum of true positives and false positives). High precision indicates that when the model predicts a positive class, it is usually correct. It is mathematically defined as:\n\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\n\n### Recall\nRecall, on the other hand, measures the proportion of true positive predictions relative to the total actual positives (i.e., the sum of true positives and false negatives). High recall indicates that the model successfully identifies a large portion of the actual positive cases. Its formula is:\n\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\n### Trade-off Between Precision and Recall\nThese two metrics often have an inverse relationship, forming a trade-off when adjusting the classification thresholds. If a model is tuned to maximize precision, it may miss a number of actual positive cases, thus lowering recall, and vice versa. This relationship is often visualized using a **Precision-Recall Curve**, which illustrates how the model's precision and recall change at different thresholds.\n\nThe choice between optimizing for precision or recall depends on the specific context of the application:\n- **Precision** is critical in scenarios where false positives are costly or undesirable.\n- **Recall** is more important in situations where failing to identify a positive case (false negative) is more detrimental.\n\nIn summary, both metrics are vital for assessing model performance, and understanding their balance is key for developing effective classification models. For further detailed examples and visual representations, refer to the documentation [here](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/explainer_classification.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 3131, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 415, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:04:58.754517Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:04:51Z"}, "provider_response_id": "chatcmpl-Crg3fNccdQrVVPH6f0c9U5qZqqHpR", "finish_reason": "stop", "run_id": "bf7c972c-197e-47e0-b3be-3cf5ac8e4ada", "metadata": null}], "source": "ai-generated"}