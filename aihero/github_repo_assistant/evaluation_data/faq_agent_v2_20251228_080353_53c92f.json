{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "In what ways can evaluation metrics differ between image and structured data models?", "timestamp": "2025-12-28T08:03:40.006240Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:03:40.006475Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluation metrics differences image structured data models\"}", "tool_call_id": "call_T3L2bHqdIPjjl2gQUqAGeICy", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 170, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:41.645343Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:03:40Z"}, "provider_response_id": "chatcmpl-Crg2WedBnGcOB51QKxmzj9aJFTN7m", "finish_reason": "tool_call", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Sample Models\n\nHere are some models you can call using the `HuggingFace()` descriptor.\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Example use                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "Use HuggingFace models", "description": "How to use models from HuggingFace as evaluators.", "filename": "docs-main/metrics/customize_hf_descriptor.mdx", "section": "## Introduction to Scoring Text with ML Models\n\nYou can score your text by downloading and using ML models from HuggingFace. This lets you apply any criteria from the source model, e.g., classify texts by emotion. There are:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models you select.\n\n**Pre-requisites**:\n* You know how to use [descriptors](/docs/library/descriptors) to evaluate text data."}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Introduction to Test Data in AI\n\nWhen working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\n\n* Just the inputs, or\n* Both inputs and expected outputs (ground truth)."}, {"title": "Regression metrics", "description": "Open-source regression quality metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_regression.mdx", "section": "## Metrics Output\n\nThis section likely provides additional insights and details regarding the overall metrics output from the model evaluations."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Why Evidently?", "description": "Why choose Evidently.", "filename": "docs-main/faq/why_evidently.mdx", "section": "## Built-in Evaluations\n\nEvidently prioritizes evaluations and quality testing. Many tools require you to prepare data and implement metrics from scratch, but we provide **100+ built-in evaluations** for various ML and LLM use cases. From ranking metrics to data drift algorithms, we have streamlined quality assessment. Users can also extend Evidently by adding custom metrics. \n\nEvidently Cloud features advanced testing capabilities, including synthetic data generation and adversarial testing for creating and running test scenarios."}, {"title": "Explore view", "description": "Reviewing the evaluation results on the Platform.", "filename": "docs-main/docs/platform/evals_explore.mdx", "section": "## Comparing Evaluations\n\nTo analyze multiple evaluation results side by side, select reports from the Report list (for example, those generated using different LLMs) and click the **\"Compare\"** button. This feature helps you identify differences in performance, quality, or behavior across different model versions or configurations.\n\nYou can also click on \"duplicate snapshot\" while viewing a specific Report to maintain the current Metric in view and select another Report for comparison."}], "tool_call_id": "call_T3L2bHqdIPjjl2gQUqAGeICy", "metadata": null, "timestamp": "2025-12-28T08:03:41.681931Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:03:41.682185Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"evaluation metrics for image data structured data\"}", "tool_call_id": "call_6F1dbJfMKoAB4Tz0rFHanrHp", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 1615, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 21, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:42.564271Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:03:41Z"}, "provider_response_id": "chatcmpl-Crg2XxLP5cIGKNmaI9lfM2u0JjZ4c", "finish_reason": "tool_call", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Benefits of Synthetic Data\n\nSynthetic data is not a replacement for real data or expert-designed tests \u2014 it\u2019s a way to add variety and speed up the process. With synthetic data you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses.\n\nIt\u2019s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Introduction to Test Data in AI\n\nWhen working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\n\n* Just the inputs, or\n* Both inputs and expected outputs (ground truth)."}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Creating a RAG Test Dataset\n\nYou can generate a ground truth RAG dataset directly from your data source.\n\n### Steps to Create a Dataset\n\n1. **Create a Project**\n   - In the Evidently UI, start a new Project or open an existing one.\n   - Navigate to \u201cDatasets\u201d in the left menu.\n   - Click \u201cGenerate\u201d and select the \u201cRAG\u201d option.\n\n   ![](/images/synthetic/synthetic_data_select_method.png)\n\n2. **Upload Your Knowledge Base**\n   - Select a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs.\n   - Choose the number of inputs to generate.\n   - Decide if you want to include the context used to generate the answer.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\n\n   - Simply drop the file, then:\n     - Choose the number of inputs.\n     - Include/exclude context as needed.\n\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\n\n   The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\n\n   <Info>\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\n   </Info>\n\n3. **Review the Test Cases**\n   - You can preview and refine the generated dataset.\n\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\n\n   - Options available:\n     - Use \u201cMore like this\u201d to add variations.\n     - Drop rows that aren\u2019t relevant.\n     - Manually edit questions or responses.\n\n4. **Save the Dataset**\n   - Once finished, store the dataset.\n   - You can download it as a CSV file or access it via the Python API using the dataset ID for evaluation.\n\n   <Info>\n     **Dataset API.** Learn how to work with [Evidently datasets](/docs/platform/datasets_overview).\n   </Info>"}, {"title": "RAG evaluation dataset", "description": "Synthetic data for RAG.", "filename": "docs-main/synthetic-data/rag_data.mdx", "section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\n\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, a test dataset reflecting what the system *should* know is essential."}, {"title": "Why synthetic data?", "description": "When do you need synthetic data in LLM evaluations.", "filename": "docs-main/synthetic-data/why_synthetic.mdx", "section": "## Application in Complex AI Systems\n\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios."}, {"title": "Custom Metric", "description": "How to create a custom dataset or column-level Metric.", "filename": "docs-main/metrics/customize_metric.mdx", "section": "## Ways to Customize Evaluations\n\nThere are methods to customize your evaluations without needing to create Metrics from scratch:\n\n- Add a [custom text descriptor](https://metrics/customize_descriptor) for row-level evaluations.\n- Utilize a built-in template to create a custom [LLM-based evaluator](https://metrics/customize_llm_judge).\n- Implement a [custom data drift](https://metrics/customize_data_drift) detection method by reusing existing renders."}, {"title": "Evaluations", "description": "Available metrics, tests and how to customize them.", "mode": "wide", "filename": "docs-main/metrics/introduction.mdx", "section": "## All Metrics\n\nThis section includes all data and ML evaluations. It allows users to analyze metrics relevant to their models and datasets."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Classification Evaluation\n\n### Description\nQuality evaluation for classification tasks.\n\n### Link\n[View Classification](https://example.com/metrics/preset_classification)"}, {"title": "Why Evidently?", "description": "Why choose Evidently.", "filename": "docs-main/faq/why_evidently.mdx", "section": "## Built-in Evaluations\n\nEvidently prioritizes evaluations and quality testing. Many tools require you to prepare data and implement metrics from scratch, but we provide **100+ built-in evaluations** for various ML and LLM use cases. From ranking metrics to data drift algorithms, we have streamlined quality assessment. Users can also extend Evidently by adding custom metrics. \n\nEvidently Cloud features advanced testing capabilities, including synthetic data generation and adversarial testing for creating and running test scenarios."}, {"title": "Overview", "description": "All available Presets.", "filename": "docs-main/metrics/all_presets.mdx", "section": "## Important Note on Presets\n\nNote that Presets apply on the **dataset level**. If you're looking at row-level evaluations (e.g., scoring relevance, correctness, etc. for LLM outputs and RAG), it's best to explore [built-in descriptors](/metrics/all_descriptors)."}], "tool_call_id": "call_6F1dbJfMKoAB4Tz0rFHanrHp", "metadata": null, "timestamp": "2025-12-28T08:03:42.581785Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:03:42.582023Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}, {"parts": [{"content": "Evaluation metrics can differ significantly between models processing image data and those handling structured data due to the inherent characteristics and requirements of the two types of data. Here are some key ways in which these metrics can differ:\n\n1. **Nature of Data**:\n   - **Image Data**: Metrics often focus on aspects pertinent to image characteristics such as accuracy, precision, recall, F1 score, Intersection over Union (IoU) for segmentation tasks, and Mean Average Precision (mAP) for object detection tasks. These metrics evaluate how well a model performs concerning visual features and spatial arrangements.\n   - **Structured Data**: Metrics for structured data might include accuracy, ROC AUC (Area Under the Receiver Operating Characteristic Curve), precision, recall, F1 score, and regression metrics like RMSE (Root Mean Square Error) or MAE (Mean Absolute Error). These evaluations are often based on numerical values or categorical classes and look more at the correctness of data representation and classification.\n\n2. **Complexity of Interpretation**:\n   - **Image Data**: Evaluating image models involves subjective assessments of visual outputs in many cases. For example, visual quality evaluations could incorporate human judgments about whether the generated images meet certain aesthetic criteria.\n   - **Structured Data**: Evaluations are more straightforward as they typically involve comparing predicted vs. actual values. The interpretation of output is often quantitative.\n\n3. **Error Analysis**:\n   - **Image Models**: Error analysis in image classification can entail examining specific images to understand misclassifications, often requiring visual inspection to determine why a model may have failed.\n   - **Structured Models**: Error analysis tends to be more statistical, focusing on classification reports, confusion matrices, and metrics that quantify the model's errors, making the faults easier to quantify and address.\n\n4. **Use of Synthetic Data**:\n   - While both domains can utilize synthetic data for testing and training, the methods and implications may vary. In image data evaluation, synthetic data generation can help create various scenarios for the model to experience diverse conditions (e.g., different lighting, orientations). For structured data, synthetic datasets might focus on generating edge cases or augmenting datasets to explore decision boundaries without direct visual interpretation.\n\n5. **Temporal Considerations**:\n   - Models working with structured data may often involve temporal sequences (like time series data), requiring evaluation metrics that account for the time aspect (e.g., forecasting accuracy, seasonality adjustments). In contrast, image data typically doesn't involve time-related metrics unless evaluating frames in a video.\n\nFor a detailed examination of these metrics across various ML applications, especially relating to their specific characteristics, you can refer to the [Evidently documentation](https://github.com/evidentlyai/docs/tree/main/docs-main/metrics/introduction.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 3025, "cache_write_tokens": 0, "cache_read_tokens": 1536, "output_tokens": 559, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:03:53.831248Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:03:42Z"}, "provider_response_id": "chatcmpl-Crg2YTLfv9Fm1y9uHM8Fs7ehs8fys", "finish_reason": "stop", "run_id": "1ec2593d-3504-42a4-b0c3-1ab940b0afe2", "metadata": null}], "source": "ai-generated"}