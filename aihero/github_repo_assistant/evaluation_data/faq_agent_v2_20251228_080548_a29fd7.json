{"agent_name": "faq_agent_v2", "system_prompt": ["You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance."], "provider": "openai", "model": "gpt-4o-mini", "tools": ["hybrid_search"], "messages": [{"parts": [{"content": "Why is feature importance analysis a critical part of model monitoring?", "timestamp": "2025-12-28T08:05:40.029042Z", "part_kind": "user-prompt"}], "timestamp": "2025-12-28T08:05:40.029154Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"feature importance analysis model monitoring\"}", "tool_call_id": "call_WOLFyGGqOknEsJJPGBfjbnVp", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 168, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 19, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:40.839690Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:40Z"}, "provider_response_id": "chatcmpl-Crg4SOeCMF1tY1WWGo9scT3x7xfvA", "finish_reason": "tool_call", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Overview of Batch Monitoring\n\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Running Tests vs Reports\n\nStructuring your evaluations as Tests, rather than monitoring a multitude of metrics simultaneously, can help alleviate alert fatigue and streamline configuration. This method simplifies the evaluation of multiple conditions at once, such as quickly verifying that all columns in the input data fall within a defined min-max range."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Workflow Steps\n\nThe complete workflow consists of the following steps:\n\n### Step 1: Configure the Metrics\n\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\n\n### Step 2: Run the Evaluations\n\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\n\n- First, perform data quality, data drift, and prediction drift checks.\n- After receiving delayed labels, run ML quality check results.\n\nYou can backdate your Reports with a custom timestamp.\n\n### Step 3: Upload to the Platform\n\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\n\n### Step 4: Configure the Dashboard\n\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### Step 5: Configure Alerts\n\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Simple Example\n\nTo get the dataset stats for a single batch and upload to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\n\n### Sample Code\n\n```python\neval_data = Dataset.from_pandas(\n    pd.DataFrame(source_df),\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    DatasetStats()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=False)\n```"}, {"title": "Open-source vs. Cloud", "description": "Deployment options and feature overview.", "filename": "docs-main/faq/oss_vs_cloud.mdx", "section": "## OSS vs. Cloud / Enterprise Platform Features\n\nPlatform editions differ in features, level of support, and maintenance costs.\n\n### Feature Availability\n\n| Category             | Feature                   | Open-source | Cloud and Enterprise |\n|"}, {"title": "LLM Evaluation", "description": "Evaluate text outputs in under 5 minutes", "filename": "docs-main/quickstart_llm.mdx", "section": "## 5. Get a Dashboard\n\nTo track evaluations over time, enter the \"Dashboard,\" and add a new \"Columns\" tab. You'll be able to observe trends and set up alerts as you log more evaluation results."}, {"title": "Alerts", "description": "How to set up alerts.", "filename": "docs-main/docs/platform/alerts.mdx", "section": "### Custom Conditions\n\nYou can also set alerts on individual Metric values. For example, you can generate alerts when the share of drifting features is above a certain threshold. \n\nTo do this, click on the plus sign below the \u201cAdd new Metric alert\u201d and follow the prompts to set an alert condition."}, {"title": "Recommendations", "description": "Overview of the Recommender Systems Preset", "filename": "docs-main/metrics/preset_recsys.mdx", "section": "## Use Case\n\nThese Presets are useful in various scenarios:\n\n- **Experimental evaluations** as you iterate on building your recommender system.\n- **Side-by-side comparison** for two different models or periods.\n- **Production monitoring** checks after you acquire ground truth labels.\n- **Debugging**. If you notice a drop in performance, use the visual Report to understand changes."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Overview", "description": "How production AI quality monitoring works.", "filename": "docs-main/docs/platform/monitoring_overview.mdx", "section": "## AI Observability Overview\n\nAI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This provides an up-to-date view of your system behavior and helps spot and fix issues.\n\nEvidently offers several ways to set up monitoring."}], "tool_call_id": "call_WOLFyGGqOknEsJJPGBfjbnVp", "metadata": null, "timestamp": "2025-12-28T08:05:40.877214Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:40.877463Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "args": "{\"query\":\"importance of feature importance analysis in model monitoring\"}", "tool_call_id": "call_trPOYTCxUiv6AX0p2N7qQ2mf", "id": null, "provider_details": null, "part_kind": "tool-call"}], "usage": {"input_tokens": 1390, "cache_write_tokens": 0, "cache_read_tokens": 0, "output_tokens": 22, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:41.865541Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "tool_calls", "timestamp": "2025-12-28T08:05:41Z"}, "provider_response_id": "chatcmpl-Crg4TTdjf0w4jwz0RoSZ5JYNJmAR0", "finish_reason": "tool_call", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}, {"parts": [{"tool_name": "hybrid_search", "content": [{"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Overview of Batch Monitoring\n\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Running Tests vs Reports\n\nStructuring your evaluations as Tests, rather than monitoring a multitude of metrics simultaneously, can help alleviate alert fatigue and streamline configuration. This method simplifies the evaluation of multiple conditions at once, such as quickly verifying that all columns in the input data fall within a defined min-max range."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Workflow Steps\n\nThe complete workflow consists of the following steps:\n\n### Step 1: Configure the Metrics\n\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to specify the evaluations.\n\n### Step 2: Run the Evaluations\n\nYou must independently execute Reports on a chosen cadence. Consider using tools like Airflow. Reports can be sent from different steps in your pipeline, such as:\n\n- First, perform data quality, data drift, and prediction drift checks.\n- After receiving delayed labels, run ML quality check results.\n\nYou can backdate your Reports with a custom timestamp.\n\n### Step 3: Upload to the Platform\n\nDecide whether to store raw inferences or only upload metric summaries. Refer to [How to upload / delete results](/docs/platform/evals_api) for more information.\n\n### Step 4: Configure the Dashboard\n\nSet up a Dashboard to track results over time using pre-built Tabs or customized monitoring Panels. Check the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### Step 5: Configure Alerts\n\nSet up alerts on Metric values or Test failures. For detailed instructions, see the section on [Alerts](/docs/platform/alerts)."}, {"title": "Batch monitoring", "description": "How to run batch evaluation jobs.", "filename": "docs-main/docs/platform/monitoring_local_batch.mdx", "section": "## Simple Example\n\nTo get the dataset stats for a single batch and upload to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) or local workspace and [create a Project](/docs/platform/projects_manage).\n\n### Sample Code\n\n```python\neval_data = Dataset.from_pandas(\n    pd.DataFrame(source_df),\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    DatasetStats()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=False)\n```"}, {"title": "Dashboard panel types [Legacy]", "description": "Overview of the available monitoring Panels.", "filename": "docs-main/docs/platform/dashboard_panel_types.mdx", "section": "## Overview of Monitoring Panels\n\nThis section introduces monitoring panels in versions 0.6.0 to 0.7.1 for Cloud/Workspace v1.\n\nA monitoring panel is an individual plot or counter on the Monitoring Dashboard. You can customize panel types, values shown, titles, and legends. Multiple panels can be organized by tabs."}, {"title": "Classification", "description": "Overview of the Classification Quality Preset", "filename": "docs-main/metrics/preset_classification.mdx", "section": "## Use Cases\n\nThese Presets are useful in various scenarios:\n\n* **Model / system comparison**: Compare predictive system performance across different datasets, such as during A/B testing or when experimenting with different prompt versions and configurations.\n* **Production monitoring**: Run evaluations whenever you get true labels in production to visualize performance and decide on model updates or retraining.\n* **Debugging**: If performance drops, use the visual Report for diagnostics, exploring model errors and underperforming segments."}, {"title": "Regression", "description": "Overview of the Regression Quality Preset", "filename": "docs-main/metrics/preset_regression.mdx", "section": "## Use Cases for RegressionPreset\n\nThese Presets are useful in various scenarios:\n\n* **Model/System Comparison**: Compare predictive system performance across different datasets, such as during A/B testing or when experimenting with model configurations and architectures.\n* **Production Monitoring**: Run evaluations whenever you receive actual values in production to communicate and visualize performance, and decide on model updates or retraining.\n* **Debugging**: If you notice a drop in performance, use the visual Report to check error distributions and explore model errors."}, {"title": "Recommendations", "description": "Overview of the Recommender Systems Preset", "filename": "docs-main/metrics/preset_recsys.mdx", "section": "## Use Case\n\nThese Presets are useful in various scenarios:\n\n- **Experimental evaluations** as you iterate on building your recommender system.\n- **Side-by-side comparison** for two different models or periods.\n- **Production monitoring** checks after you acquire ground truth labels.\n- **Debugging**. If you notice a drop in performance, use the visual Report to understand changes."}, {"title": "Classification metrics", "description": "Open-source classification metrics.", "noindex": "true", "filename": "docs-main/metrics/explainer_classification.mdx", "section": "## Model Quality Summary Metrics\n\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. \n\n**To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.**"}, {"title": "Data Summary", "description": "Overview of the Data Summary Preset.", "filename": "docs-main/metrics/preset_data_summary.mdx", "section": "## Use Cases for DataSummaryPreset\n\nThe `DataSummaryPreset` can be beneficial in various scenarios:\n\n* **Exploratory data analysis**: Use the visual report to understand your dataset during different stages, such as model training or debugging.\n* **Dataset comparison**: Compare datasets to comprehend differences, like between training and test datasets.\n* **Data quality tests in production**: Enable Tests to ensure data quality before generating predictions.\n* **Data profiling in production**: Utilize this preset for ongoing monitoring to capture production data characteristics for future analysis."}], "tool_call_id": "call_trPOYTCxUiv6AX0p2N7qQ2mf", "metadata": null, "timestamp": "2025-12-28T08:05:41.885284Z", "part_kind": "tool-return"}], "timestamp": "2025-12-28T08:05:41.885518Z", "instructions": "You are a helpful assistant for a course.  \n\nUse the search tool to find relevant information from the document materials before answering questions.  \n\nIf you can find specific information through search, use it to provide accurate answers.\n\nAlways include references by citing the filename of the source material you used.  \nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/evidentlyai/docs/tree/main\"\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\n\nIf the search doesn't return relevant results, let the user know and provide general guidance.", "kind": "request", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}, {"parts": [{"content": "Feature importance analysis is a critical component of model monitoring due to several key reasons:\n\n1. **Understanding Model Behavior**: It allows practitioners to comprehend which features significantly impact model predictions. By analyzing feature importance, teams can identify whether the model relies on the expected features or if it is influenced by unexpected ones. This understanding can guide decisions during model deployment and adjustments.\n\n2. **Monitoring for Drift**: Feature importance helps in monitoring changes in data distributions over time. If the importance of certain features significantly shifts, it can indicate potential data drift, where the relationship between input features and target outcomes changes. Recognizing this early can assist in maintaining model accuracy and reliability.\n\n3. **Debugging and Diagnosing Issues**: When model performance degrades, feature importance analysis can help pinpoint which features are contributing to errors or biases in predictions. It aids in diagnosing issues more effectively by revealing if the model is overfitting or relying too heavily on certain features.\n\n4. **Guiding Feature Selection and Engineering**: Understanding which features contribute most to model predictions can inform decisions about feature selection during future iterations of model development. This can lead to better-performing models by focusing on the most impactful variables and potentially reducing complexity by removing less important features.\n\n5. **Improving Model Transparency**: In an era where model transparency is increasingly demanded, providing insights into feature importance can enhance trust in AI systems. Stakeholders and end-users are more likely to accept models when they understand why certain decisions are made.\n\nOverall, by incorporating feature importance analysis into model monitoring practices, organizations can make informed decisions, enhance model performance, and ensure robustness against unforeseen changes in data.\n\nFor more information on monitoring practices, you can refer to the documentation: [Overview of Batch Monitoring](https://github.com/evidentlyai/docs/tree/main/docs/platform/monitoring_local_batch.mdx).", "id": null, "provider_details": null, "part_kind": "text"}], "usage": {"input_tokens": 2799, "cache_write_tokens": 0, "cache_read_tokens": 1280, "output_tokens": 370, "input_audio_tokens": 0, "cache_audio_read_tokens": 0, "output_audio_tokens": 0, "details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}}, "model_name": "gpt-4o-mini-2024-07-18", "timestamp": "2025-12-28T08:05:48.623361Z", "kind": "response", "provider_name": "openai", "provider_url": "https://api.openai.com/v1/", "provider_details": {"finish_reason": "stop", "timestamp": "2025-12-28T08:05:42Z"}, "provider_response_id": "chatcmpl-Crg4Uko91H28Vw0XCS5VjuIkPxOux", "finish_reason": "stop", "run_id": "8c6f6e98-cd30-460f-8167-c637c6cd887c", "metadata": null}], "source": "ai-generated"}